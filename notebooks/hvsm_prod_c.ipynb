{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01730023c034613881230865cb14390",
   "metadata": {},
   "source": [
    "# HVSM Notebook: hvsm_prod_c.ipynb\n",
    "\n",
    "- Runs with: slurm_scripts/hvsm_job_c.sh\n",
    "- Purpose: GPU models (cuML) with CPU TF-IDF, rules, and prevalence match.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe44baf9f843659d6dd052c5c35e7a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (papermill)\n",
    "DATA_DIR = \"data\"\n",
    "TRAIN_CSV = \"data/train.csv\"\n",
    "VAL_CSV = \"data/val.csv\"\n",
    "TEST_CSV = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2cf675",
   "metadata": {},
   "source": [
    "# HVSM \u2014 GPU TF\u2013IDF + LR/NB (Polars + cuML) with Binary Rules and Prevalence Match\n\nInputs: `data/train.csv`, `data/val.csv`, `data/test.csv` in `data/`. `data/test.csv` must have `id` and no `label`. Output: `outputs/submission_hvsm_prod_c.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d71cc",
   "metadata": {},
   "source": [
    "## Imports and GPU guardrails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468392e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, string, warnings, json, hashlib\n",
    "import gc\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer as SkTfidfVectorizer,\n",
    ")\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cupyx.scipy.sparse as cpx_sparse\n",
    "    import cuml\n",
    "    from cuml.linear_model import LogisticRegression\n",
    "    from cuml.naive_bayes import MultinomialNB\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"cuML + CUDA (cupy/cudf) required for GPU-first run.\"\n",
    "    ) from e\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except Exception:\n",
    "    TextBlob = None\n",
    "    warnings.warn(\"TextBlob missing; sentiment features set to zeros.\")\n",
    "np.set_printoptions(linewidth=79)\n",
    "cuml.set_global_output_type(\"cupy\")\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d366fbf",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42190c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    tfidf_max_features: int = 5000\n",
    "    tfidf_chunk_size: int = 5000\n",
    "    proba_chunk_size: int = 20000\n",
    "    tfidf_ngram_max: int = 7\n",
    "    use_char_ngrams: bool = False\n",
    "    char_tfidf_max_features: int = 5000\n",
    "    min_df: int = 2\n",
    "    kfolds: int = 3\n",
    "    lr_iter: int = 8\n",
    "    nb_iter: int = 6\n",
    "    plot_level: str = \"full\"\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44654139",
   "metadata": {},
   "source": [
    "## Plotting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc() -> None:\n",
    "    gc.collect()\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        try:\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "_STEP_STARTS = {}\n",
    "\n",
    "\n",
    "def log_step(msg: str) -> None:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{ts}] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "def log_step_start(name: str) -> None:\n",
    "    _STEP_STARTS[name] = time.perf_counter()\n",
    "    log_step(f\"START: {name}\")\n",
    "\n",
    "\n",
    "def log_step_end(name: str) -> None:\n",
    "    start = _STEP_STARTS.pop(name, None)\n",
    "    if start is None:\n",
    "        log_step(f\"END: {name}\")\n",
    "    else:\n",
    "        elapsed = time.perf_counter() - start\n",
    "        log_step(f\"END: {name} (elapsed {elapsed:.1f}s)\")\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        try:\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def predict_proba_chunks(model, X, chunk_size: int = 50000):\n",
    "    n = X.shape[0]\n",
    "    out = cp.empty(n, dtype=cp.float32)\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        out[start:end] = model.predict_proba(X[start:end])[:, 1]\n",
    "        _gc()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _to_numpy(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    if hasattr(x, \"get\"):\n",
    "        return x.get()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def f1_score_np(y_true, y_pred) -> float:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    return float(2 * precision * recall / (precision + recall + 1e-12))\n",
    "\n",
    "\n",
    "def confusion_matrix_np(y_true, y_pred) -> np.ndarray:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "\n",
    "def classification_report_np(y_true, y_pred) -> str:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "\n",
    "    def _prf(label):\n",
    "        tp = int(((y_true == label) & (y_pred == label)).sum())\n",
    "        fp = int(((y_true != label) & (y_pred == label)).sum())\n",
    "        fn = int(((y_true == label) & (y_pred != label)).sum())\n",
    "        precision = tp / (tp + fp + 1e-12)\n",
    "        recall = tp / (tp + fn + 1e-12)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "        support = int((y_true == label).sum())\n",
    "        return precision, recall, f1, support\n",
    "\n",
    "    p0, r0, f0, s0 = _prf(0)\n",
    "    p1, r1, f1, s1 = _prf(1)\n",
    "    acc = float((y_true == y_pred).mean())\n",
    "    macro_p = (p0 + p1) / 2\n",
    "    macro_r = (r0 + r1) / 2\n",
    "    macro_f = (f0 + f1) / 2\n",
    "    total = s0 + s1\n",
    "    w_p = (p0 * s0 + p1 * s1) / max(total, 1)\n",
    "    w_r = (r0 * s0 + r1 * s1) / max(total, 1)\n",
    "    w_f = (f0 * s0 + f1 * s1) / max(total, 1)\n",
    "    lines = [\n",
    "        \"              precision    recall  f1-score   support\",\n",
    "        f\"           0       {p0:0.3f}      {r0:0.3f}      {f0:0.3f}      {s0:5d}\",\n",
    "        f\"           1       {p1:0.3f}      {r1:0.3f}      {f1:0.3f}      {s1:5d}\",\n",
    "        \"\",\n",
    "        f\"    accuracy                           {acc:0.3f}      {total:5d}\",\n",
    "        f\"   macro avg       {macro_p:0.3f}      {macro_r:0.3f}      {macro_f:0.3f}      {total:5d}\",\n",
    "        f\"weighted avg       {w_p:0.3f}      {w_r:0.3f}      {w_f:0.3f}      {total:5d}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def roc_curve_np(y_true, y_score):\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_score = _to_numpy(y_score).astype(float)\n",
    "    order = np.argsort(-y_score)\n",
    "    y_true = y_true[order]\n",
    "    y_score = y_score[order]\n",
    "    tps = np.cumsum(y_true == 1)\n",
    "    fps = np.cumsum(y_true == 0)\n",
    "    tpr = tps / max(tps[-1], 1)\n",
    "    fpr = fps / max(fps[-1], 1)\n",
    "    thresholds = y_score\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "\n",
    "def precision_recall_curve_np(y_true, y_score):\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_score = _to_numpy(y_score).astype(float)\n",
    "    order = np.argsort(-y_score)\n",
    "    y_true = y_true[order]\n",
    "    y_score = y_score[order]\n",
    "    tps = np.cumsum(y_true == 1)\n",
    "    fps = np.cumsum(y_true == 0)\n",
    "    precision = tps / np.maximum(tps + fps, 1)\n",
    "    recall = tps / max(tps[-1], 1)\n",
    "    return precision, recall, y_score\n",
    "\n",
    "\n",
    "def roc_auc_score_np(y_true, y_score) -> float:\n",
    "    fpr, tpr, _ = roc_curve_np(y_true, y_score)\n",
    "    return float(np.trapz(tpr, fpr))\n",
    "\n",
    "\n",
    "def _tight() -> None:\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def qq_plot(residuals: np.ndarray, title: str) -> None:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(title)\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def residual_plot(y_true: np.ndarray, y_prob: np.ndarray, title: str) -> None:\n",
    "    resid = y_true - y_prob\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(y_prob, resid, s=8)\n",
    "    plt.axhline(0.0, linestyle=\"--\")\n",
    "    plt.xlabel(\"p(y=1)\")\n",
    "    plt.ylabel(\"residual\")\n",
    "    plt.title(title)\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def violin_by_label(\n",
    "    df: pl.DataFrame, label_col: str, feat_col: str, title: str\n",
    ") -> None:\n",
    "    y = df.select(label_col).to_numpy().ravel()\n",
    "    x = df.select(feat_col).to_numpy().ravel()\n",
    "    if sns is None:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.boxplot([x[y == 0], x[y == 1]], labels=[\"0\", \"1\"])\n",
    "        plt.title(title)\n",
    "        _tight()\n",
    "        plt.show()\n",
    "        return\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.violinplot(x=y, y=x)\n",
    "    plt.title(title)\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_pr(y_true: np.ndarray, y_prob: np.ndarray, title: str) -> None:\n",
    "    fpr, tpr, _ = roc_curve_np(y_true, y_prob)\n",
    "    prec, rec, _ = precision_recall_curve_np(y_true, y_prob)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax[0].plot(fpr, tpr)\n",
    "    ax[0].set_title(f\"ROC AUC={roc_auc_score_np(y_true, y_prob):.3f}\")\n",
    "    ax[0].set_xlabel(\"FPR\")\n",
    "    ax[0].set_ylabel(\"TPR\")\n",
    "    ax[1].plot(rec, prec)\n",
    "    ax[1].set_title(\"Precision\u2013Recall\")\n",
    "    ax[1].set_xlabel(\"Recall\")\n",
    "    ax[1].set_ylabel(\"Precision\")\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion(y_true: np.ndarray, y_hat: np.ndarray, title: str) -> None:\n",
    "    cm = confusion_matrix_np(y_true, y_hat)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "    plt.xlabel(\"Pred\")\n",
    "    plt.ylabel(\"True\")\n",
    "    _tight()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca448e3f",
   "metadata": {},
   "source": [
    "## Processing and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Processing and features\")\n",
    "\n",
    "\n",
    "def _ttr(text: str) -> float:\n",
    "    toks = re.findall(r\"\\S+\", text.lower())\n",
    "    return float(len(set(toks)) / len(toks)) if toks else 0.0\n",
    "\n",
    "\n",
    "def _sentiment(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if TextBlob is None:\n",
    "        return df.with_columns(\n",
    "            [\n",
    "                pl.lit(0.0).alias(\"sentiment_polarity\"),\n",
    "                pl.lit(0.0).alias(\"sentiment_subjectivity\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _polarity(x: str) -> float:\n",
    "        return float(TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    def _subjectivity(x: str) -> float:\n",
    "        return float(TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_polarity, return_dtype=pl.Float64)\n",
    "            .alias(\"sentiment_polarity\"),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_subjectivity, return_dtype=pl.Float64)\n",
    "            .alias(\"sentiment_subjectivity\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def process_text_file(filename: str) -> pl.DataFrame:\n",
    "    df = pl.read_csv(os.path.join(filename))\n",
    "    assert \"text\" in df.columns\n",
    "    df = df.with_columns(pl.col(\"text\").cast(pl.Utf8))\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"text\").str.len_chars().alias(\"text_length\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"\\S+\").alias(\"word_count\"),\n",
    "            pl.col(\"text\")\n",
    "            .str.count_matches(r\"[.!?]+\")\n",
    "            .alias(\"sentence_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"[^\\w\\s]\").alias(\"punct_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"\\d\").alias(\"digit_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"[A-Z]\").alias(\"upper_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"!\").alias(\"bangs\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"\\?\").alias(\"questions\"),\n",
    "        ]\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"sentence_count\") == 0)\n",
    "            .then(1)\n",
    "            .otherwise(pl.col(\"sentence_count\"))\n",
    "            .alias(\"sentence_count\"),\n",
    "            pl.when(pl.col(\"text_length\") == 0)\n",
    "            .then(1)\n",
    "            .otherwise(pl.col(\"text_length\"))\n",
    "            .alias(\"text_length_safe\"),\n",
    "        ]\n",
    "    )\n",
    "    avg_sentence_expr = pl.col(\"word_count\") / pl.col(\"sentence_count\")\n",
    "    punct_expr = pl.col(\"punct_count\") / pl.col(\"text_length_safe\")\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(avg_sentence_expr > 100)\n",
    "            .then(100)\n",
    "            .otherwise(avg_sentence_expr)\n",
    "            .alias(\"avg_sentence_length\"),\n",
    "            pl.when(punct_expr > 0.3)\n",
    "            .then(0.3)\n",
    "            .otherwise(punct_expr)\n",
    "            .alias(\"punct_ratio\"),\n",
    "            (pl.col(\"digit_count\") / pl.col(\"text_length_safe\")).alias(\n",
    "                \"digit_ratio\"\n",
    "            ),\n",
    "            (pl.col(\"upper_count\") / pl.col(\"text_length_safe\")).alias(\n",
    "                \"upper_ratio\"\n",
    "            ),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_ttr, return_dtype=pl.Float64)\n",
    "            .alias(\"ttr\"),\n",
    "        ]\n",
    "    )\n",
    "    df = df.drop([\"digit_count\", \"upper_count\", \"text_length_safe\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "log_step_end(\"Processing and features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b3d62",
   "metadata": {},
   "source": [
    "## Binary features and 2^3 sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17276e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Binary features and 2^3 sweep\")\n",
    "\n",
    "\n",
    "def ends_with_letter(text: str) -> int:\n",
    "    s = text.rstrip()\n",
    "    return int(len(s) > 0 and s[-1] in string.ascii_letters)\n",
    "\n",
    "\n",
    "def has_5gram_repetition(text: str) -> int:\n",
    "    toks = re.findall(r\"\\S+\", text)\n",
    "    if len(toks) < 10:\n",
    "        return 0\n",
    "    seen = {}\n",
    "    w = 5\n",
    "    for i in range(len(toks) - w + 1):\n",
    "        key = tuple(toks[i : i + w])\n",
    "        if key in seen:\n",
    "            return 1\n",
    "        seen[key] = 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "COMMON_SMALL = set(\n",
    "    [\n",
    "        \"the\",\n",
    "        \"be\",\n",
    "        \"to\",\n",
    "        \"of\",\n",
    "        \"and\",\n",
    "        \"a\",\n",
    "        \"in\",\n",
    "        \"that\",\n",
    "        \"have\",\n",
    "        \"i\",\n",
    "        \"it\",\n",
    "        \"for\",\n",
    "        \"not\",\n",
    "        \"on\",\n",
    "        \"with\",\n",
    "        \"he\",\n",
    "        \"as\",\n",
    "        \"you\",\n",
    "        \"do\",\n",
    "        \"at\",\n",
    "        \"this\",\n",
    "        \"but\",\n",
    "        \"his\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def max_uncommon_binary(\n",
    "    text: str, thr_rep: int = 3, thr_count: int = 5\n",
    ") -> int:\n",
    "    toks = [t.lower() for t in re.findall(r\"\\w+\", text)]\n",
    "    if not toks:\n",
    "        return 0\n",
    "    freqs = {}\n",
    "    uncommon = 0\n",
    "    for t in toks:\n",
    "        if t not in COMMON_SMALL:\n",
    "            uncommon += 1\n",
    "            freqs[t] = freqs.get(t, 0) + 1\n",
    "    if uncommon < thr_count:\n",
    "        return 0\n",
    "    return int(any(v >= thr_rep for v in freqs.values()))\n",
    "\n",
    "\n",
    "def add_binary_feats(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(ends_with_letter, return_dtype=pl.Int64)\n",
    "            .alias(\"ends_with_letter\"),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(has_5gram_repetition, return_dtype=pl.Int64)\n",
    "            .alias(\"has_5gram_repetition\"),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(max_uncommon_binary, return_dtype=pl.Int64)\n",
    "            .alias(\"max_uncommon_binary\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def sweep_binary_subsets(y_true: np.ndarray, fe_df: pl.DataFrame):\n",
    "    cols = [\"ends_with_letter\", \"has_5gram_repetition\", \"max_uncommon_binary\"]\n",
    "    best_f1, best_key = -1.0, \"none\"\n",
    "    for mask in range(1, 1 << len(cols)):\n",
    "        sel = [cols[i] for i in range(len(cols)) if (mask >> i) & 1]\n",
    "        rule = (\n",
    "            fe_df.select(\n",
    "                pl.any_horizontal([pl.col(c) for c in sel]).alias(\"rule\")\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .ravel()\n",
    "            .astype(int)\n",
    "        )\n",
    "        f1 = f1_score_np(y_true, rule)\n",
    "        key = \"|\".join(sel)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_key = f1, key\n",
    "    return best_key, float(best_f1)\n",
    "\n",
    "\n",
    "log_step_end(\"Binary features and 2^3 sweep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cc586",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Load data\")\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / DATA_DIR).exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / DATA_DIR).exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "\n",
    "\n",
    "def resolve_path(path_str: str) -> str:\n",
    "    p = Path(path_str)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    if p.parent == Path(\".\"):\n",
    "        data_dir = Path(DATA_DIR)\n",
    "        if not data_dir.is_absolute():\n",
    "            data_dir = PROJECT_ROOT / data_dir\n",
    "        candidate = data_dir / p.name\n",
    "        if candidate.exists():\n",
    "            return str(candidate)\n",
    "    return str((PROJECT_ROOT / p).resolve())\n",
    "\n",
    "\n",
    "# Reassemble chunked CSVs if needed\n",
    "def ensure_chunked_csv(path: Path) -> None:\n",
    "    if path.exists():\n",
    "        return\n",
    "    parts = sorted(path.parent.glob(path.name + \".part*\"))\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(f\"Missing {path} and no chunk files found.\")\n",
    "    tmp_path = path.with_suffix(path.suffix + \".tmp\")\n",
    "    if tmp_path.exists():\n",
    "        tmp_path.unlink()\n",
    "    hasher = hashlib.sha256()\n",
    "    with tmp_path.open(\"wb\") as out:\n",
    "        for part in parts:\n",
    "            with part.open(\"rb\") as f:\n",
    "                while True:\n",
    "                    chunk = f.read(1024 * 1024)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    out.write(chunk)\n",
    "                    hasher.update(chunk)\n",
    "    sha_path = path.with_suffix(path.suffix + \".sha256\")\n",
    "    if sha_path.exists():\n",
    "        expected = sha_path.read_text().split()[0]\n",
    "        actual = hasher.hexdigest()\n",
    "        if expected != actual:\n",
    "            tmp_path.unlink(missing_ok=True)\n",
    "            raise ValueError(\n",
    "                f\"SHA256 mismatch for {path}: expected {expected} got {actual}\"\n",
    "            )\n",
    "    tmp_path.replace(path)\n",
    "    log_step(f\"Reassembled {path} from {len(parts)} chunks.\")\n",
    "\n",
    "\n",
    "train_path = Path(resolve_path(TRAIN_CSV))\n",
    "val_path = Path(resolve_path(VAL_CSV))\n",
    "test_path = Path(resolve_path(TEST_CSV))\n",
    "ensure_chunked_csv(train_path)\n",
    "ensure_chunked_csv(val_path)\n",
    "ensure_chunked_csv(test_path)\n",
    "\n",
    "train = process_text_file(str(train_path))\n",
    "val = process_text_file(str(val_path))\n",
    "test = process_text_file(str(test_path))\n",
    "assert \"label\" in train.columns and \"label\" in val.columns\n",
    "assert \"label\" not in test.columns and \"id\" in test.columns\n",
    "print(\"Rows:\", train.height, val.height, test.height)\n",
    "log_step_end(\"Load data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542007f6",
   "metadata": {},
   "source": [
    "## Sentiment + binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Sentiment + binaries\")\n",
    "train = _sentiment(train)\n",
    "val = _sentiment(val)\n",
    "test = _sentiment(test)\n",
    "train = add_binary_feats(train)\n",
    "val = add_binary_feats(val)\n",
    "test = add_binary_feats(test)\n",
    "rk, rf1 = sweep_binary_subsets(val[\"label\"].to_numpy().astype(int), val)\n",
    "print(f\"Best binary subset (val): {rk} | F1={rf1:.4f}\")\n",
    "log_step_end(\"Sentiment + binaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb7f0d",
   "metadata": {},
   "source": [
    "## Numeric + TF\u2013IDF design (GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Numeric + TF\u2013IDF design (GPU)\")\n",
    "num_cols = [\n",
    "    \"text_length\",\n",
    "    \"word_count\",\n",
    "    \"ttr\",\n",
    "    \"sentence_count\",\n",
    "    \"avg_sentence_length\",\n",
    "    \"punct_ratio\",\n",
    "    \"sentiment_polarity\",\n",
    "    \"sentiment_subjectivity\",\n",
    "    \"digit_ratio\",\n",
    "    \"upper_ratio\",\n",
    "    \"bangs\",\n",
    "    \"questions\",\n",
    "    \"ends_with_letter\",\n",
    "    \"has_5gram_repetition\",\n",
    "    \"max_uncommon_binary\",\n",
    "]\n",
    "Xtr_num = cpx_sparse.csr_matrix(\n",
    "    cp.asarray(train.select(num_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "Xva_num = cpx_sparse.csr_matrix(\n",
    "    cp.asarray(val.select(num_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "Xte_num = cpx_sparse.csr_matrix(\n",
    "    cp.asarray(test.select(num_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "train_text = train[\"text\"].to_list()\n",
    "val_text = val[\"text\"].to_list()\n",
    "test_text = test[\"text\"].to_list()\n",
    "vec_word = SkTfidfVectorizer(\n",
    "    ngram_range=(1, CFG.tfidf_ngram_max),\n",
    "    max_features=CFG.tfidf_max_features,\n",
    "    min_df=CFG.min_df,\n",
    "    stop_words=\"english\",\n",
    "    dtype=np.float32,\n",
    ")\n",
    "vec_word.fit(train_text)\n",
    "_gc()\n",
    "\n",
    "\n",
    "def _transform_in_chunks_cpu(texts, chunk_size: int):\n",
    "    n = len(texts)\n",
    "    chunks = []\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        X_chunk = vec_word.transform(texts[start:end])\n",
    "        chunks.append(X_chunk)\n",
    "        _gc()\n",
    "    if not chunks:\n",
    "        return sp.csr_matrix((0, 0))\n",
    "    if len(chunks) == 1:\n",
    "        return chunks[0].tocsr()\n",
    "    return sp.vstack(chunks).tocsr()\n",
    "\n",
    "\n",
    "Xtr_w_cpu = _transform_in_chunks_cpu(train_text, CFG.tfidf_chunk_size)\n",
    "Xva_w_cpu = _transform_in_chunks_cpu(val_text, CFG.tfidf_chunk_size)\n",
    "Xte_w_cpu = _transform_in_chunks_cpu(test_text, CFG.tfidf_chunk_size)\n",
    "\n",
    "del train_text, val_text, test_text\n",
    "_gc()\n",
    "\n",
    "Xtr_w = cpx_sparse.csr_matrix(Xtr_w_cpu)\n",
    "Xva_w = cpx_sparse.csr_matrix(Xva_w_cpu)\n",
    "Xte_w = cpx_sparse.csr_matrix(Xte_w_cpu)\n",
    "\n",
    "del Xtr_w_cpu, Xva_w_cpu, Xte_w_cpu\n",
    "_gc()\n",
    "\n",
    "X_train = cpx_sparse.hstack([Xtr_num, Xtr_w]).tocsr()\n",
    "X_val = cpx_sparse.hstack([Xva_num, Xva_w]).tocsr()\n",
    "X_test = cpx_sparse.hstack([Xte_num, Xte_w]).tocsr()\n",
    "\n",
    "del Xtr_num, Xva_num, Xte_num, Xtr_w, Xva_w, Xte_w\n",
    "_gc()\n",
    "\n",
    "y_train = cp.asarray(train[\"label\"].to_numpy()).astype(cp.int32)\n",
    "y_val = cp.asarray(val[\"label\"].to_numpy()).astype(cp.int32)\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "_gc()\n",
    "log_step_end(\"Numeric + TF\u2013IDF design (GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb16023",
   "metadata": {},
   "source": [
    "## Tuning and calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Tuning and calibration\")\n",
    "\n",
    "\n",
    "def _lr_space():\n",
    "    return {\"C\": [0.5, 1.0, 2.0, 4.0]}\n",
    "\n",
    "\n",
    "def _nb_space():\n",
    "    return {\"alpha\": [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
    "\n",
    "\n",
    "def _search_signature(name, space, n_iter, random_state, X_shape):\n",
    "    payload = {\n",
    "        \"name\": name,\n",
    "        \"space\": space,\n",
    "        \"n_iter\": n_iter,\n",
    "        \"random_state\": random_state,\n",
    "        \"X_shape\": list(X_shape),\n",
    "    }\n",
    "    raw = json.dumps(payload, sort_keys=True, default=str)\n",
    "    return hashlib.md5(raw.encode()).hexdigest()[:10]\n",
    "\n",
    "\n",
    "def _load_results(results_path: Path):\n",
    "    rows = []\n",
    "    if not results_path.exists():\n",
    "        return rows\n",
    "    with results_path.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _to_jsonable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _to_jsonable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [_to_jsonable(v) for v in obj]\n",
    "    if isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    return obj\n",
    "\n",
    "\n",
    "def tune_model(\n",
    "    name: str,\n",
    "    build_fn,\n",
    "    space,\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_va,\n",
    "    y_va,\n",
    "    n_iter: int,\n",
    "    random_state: int,\n",
    "):\n",
    "    ckpt_root = Path(PROJECT_ROOT) if \"PROJECT_ROOT\" in globals() else Path.cwd()\n",
    "    ckpt_dir = ckpt_root / \"checkpoints\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sig = _search_signature(name, space, n_iter, random_state, X_tr.shape)\n",
    "    base = f\"{name.lower()}_search_{sig}\"\n",
    "    candidates_path = ckpt_dir / f\"{base}_candidates.json\"\n",
    "    results_path = ckpt_dir / f\"{base}_results.jsonl\"\n",
    "    best_path = ckpt_dir / f\"{base}_best.json\"\n",
    "    meta_path = ckpt_dir / f\"{base}_meta.json\"\n",
    "\n",
    "    if candidates_path.exists():\n",
    "        candidates = json.loads(candidates_path.read_text())\n",
    "    else:\n",
    "        candidates = list(\n",
    "            ParameterSampler(space, n_iter=n_iter, random_state=random_state)\n",
    "        )\n",
    "        candidates_path.write_text(json.dumps(_to_jsonable(candidates), indent=2))\n",
    "\n",
    "    meta_path.write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"signature\": sig,\n",
    "                \"name\": name,\n",
    "                \"n_iter\": n_iter,\n",
    "                \"random_state\": random_state,\n",
    "                \"X_shape\": list(X_tr.shape),\n",
    "                \"n_candidates\": len(candidates),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rows = _load_results(results_path)\n",
    "    done = set()\n",
    "    best_params, best_f1 = None, -1.0\n",
    "    for row in rows:\n",
    "        idx = row.get(\"iter_idx\")\n",
    "        if idx is not None:\n",
    "            done.add(int(idx))\n",
    "        if row.get(\"status\") == \"ok\":\n",
    "            f1 = float(row.get(\"f1\", -1.0))\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_params = row.get(\"params\")\n",
    "\n",
    "    if rows:\n",
    "        log_step(\n",
    "            f\"{name} resume: {len(done)}/{len(candidates)} candidates done\"\n",
    "        )\n",
    "\n",
    "    with results_path.open(\"a\") as f:\n",
    "        for i, params in enumerate(candidates):\n",
    "            if i in done:\n",
    "                continue\n",
    "            iter_name = f\"{name} iter {i + 1}/{len(candidates)}\"\n",
    "            log_step_start(iter_name)\n",
    "            status = \"ok\"\n",
    "            f1 = None\n",
    "            error = None\n",
    "            model = build_fn(**params)\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                p = model.predict_proba(X_va)[:, 1]\n",
    "                f1 = float(_f1_from_proba(y_va, p))\n",
    "            except Exception as exc:\n",
    "                status = \"fail\"\n",
    "                error = f\"{type(exc).__name__}: {exc}\"\n",
    "            record = {\n",
    "                \"iter_idx\": i,\n",
    "                \"status\": status,\n",
    "                \"f1\": f1,\n",
    "                \"params\": _to_jsonable(params),\n",
    "                \"error\": error,\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\n",
    "\")\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "            if status == \"ok\":\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_params = _to_jsonable(params)\n",
    "                    best_path.write_text(\n",
    "                        json.dumps(\n",
    "                            {\n",
    "                                \"best_f1\": best_f1,\n",
    "                                \"best_params\": best_params,\n",
    "                            },\n",
    "                            indent=2,\n",
    "                        )\n",
    "                    )\n",
    "                log_step(f\"{iter_name} f1={f1:.4f} best={best_f1:.4f}\")\n",
    "            else:\n",
    "                log_step(f\"{iter_name} failed: {error}\")\n",
    "            log_step_end(iter_name)\n",
    "            del model\n",
    "            _gc()\n",
    "\n",
    "    print(f\"Best {name}: {best_params} | F1={best_f1:.4f}\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "class PlattCalibrator:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    def fit(self, scores, y):\n",
    "        scores = cp.asarray(scores).reshape(-1, 1)\n",
    "        self.model.fit(scores, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, scores):\n",
    "        scores = cp.asarray(scores).reshape(-1, 1)\n",
    "        return self.model.predict_proba(scores)[:, 1]\n",
    "\n",
    "\n",
    "lr_params = tune_model(\n",
    "    \"LR\",\n",
    "    lambda **p: LogisticRegression(max_iter=2000, **p),\n",
    "    _lr_space(),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    CFG.lr_iter,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "nb_params = tune_model(\n",
    "    \"NB\",\n",
    "    lambda **p: MultinomialNB(**p),\n",
    "    _nb_space(),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    CFG.nb_iter,\n",
    "    random_state=RANDOM_SEED + 17,\n",
    ")\n",
    "X_trval = cpx_sparse.vstack([X_train, X_val]).tocsr()\n",
    "y_trval = cp.concatenate([y_train, y_val])\n",
    "lr_tuned = LogisticRegression(max_iter=2000, **lr_params)\n",
    "nb_tuned = MultinomialNB(**nb_params)\n",
    "log_step_start(\"Fold 1/1 (single split)\")\n",
    "log_step_start(\"LR fit\")\n",
    "lr_tuned.fit(X_trval, y_trval)\n",
    "log_step_end(\"LR fit\")\n",
    "log_step_start(\"NB fit\")\n",
    "nb_tuned.fit(X_trval, y_trval)\n",
    "log_step_end(\"NB fit\")\n",
    "log_step_end(\"Fold 1/1 (single split)\")\n",
    "\n",
    "cal_lr = PlattCalibrator().fit(lr_tuned.predict_proba(X_val)[:, 1], y_val)\n",
    "cal_nb = PlattCalibrator().fit(nb_tuned.predict_proba(X_val)[:, 1], y_val)\n",
    "_gc()\n",
    "log_step_end(\"Tuning and calibration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c0101",
   "metadata": {},
   "source": [
    "## Ensembling, thresholding, prevalence match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b81022",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Ensembling, thresholding, prevalence match\")\n",
    "\n",
    "\n",
    "def decode_prevalence(y_prob: np.ndarray, pos_rate: float) -> np.ndarray:\n",
    "    n = len(y_prob)\n",
    "    k = int(round(pos_rate * n))\n",
    "    idx = np.argsort(-y_prob)\n",
    "    out = np.zeros(n, dtype=int)\n",
    "    out[idx[:k]] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "scores_lr = predict_proba_chunks(\n",
    "    lr_tuned, X_val, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "scores_nb = predict_proba_chunks(\n",
    "    nb_tuned, X_val, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "p_lr = _to_numpy(cal_lr.predict_proba(scores_lr))\n",
    "p_nb = _to_numpy(cal_nb.predict_proba(scores_nb))\n",
    "best_w, best_f1, best_thr = 0.5, -1.0, 0.5\n",
    "for w in np.linspace(0.0, 1.0, 21):\n",
    "    p = w * p_nb + (1.0 - w) * p_lr\n",
    "    for thr in np.arange(0.1, 0.91, 0.01):\n",
    "        f1 = f1_score_np(y_val, (p >= thr).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_w, best_f1, best_thr = float(w), float(f1), float(thr)\n",
    "print(f\"Threshold head: w={best_w:.2f} thr={best_thr:.2f} F1={best_f1:.4f}\")\n",
    "p_ens = best_w * p_nb + (1.0 - best_w) * p_lr\n",
    "val_pos_rate = float(np.mean(_to_numpy(y_val)))\n",
    "yhat_topk = decode_prevalence(p_ens, val_pos_rate)\n",
    "f1_topk = f1_score_np(y_val, yhat_topk)\n",
    "print(f\"Prevalence head: rate={val_pos_rate:.3f} F1={f1_topk:.4f}\")\n",
    "rk, rf1 = sweep_binary_subsets(_to_numpy(y_val), val)\n",
    "print(f\"Rule head (best subset {rk}) F1={rf1:.4f}\")\n",
    "heads = [(\"threshold\", best_f1), (\"prevalence\", f1_topk), (\"rule\", rf1)]\n",
    "heads.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Head ranking:\", heads)\n",
    "_gc()\n",
    "log_step_end(\"Ensembling, thresholding, prevalence match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d5fd4",
   "metadata": {},
   "source": [
    "## Validation diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Validation diagnostics\")\n",
    "winner = heads[0][0]\n",
    "if winner == \"threshold\":\n",
    "    yhat_val = (p_ens >= best_thr).astype(int)\n",
    "elif winner == \"prevalence\":\n",
    "    yhat_val = yhat_topk\n",
    "else:\n",
    "    yhat_val = (\n",
    "        val.select(\n",
    "            pl.any_horizontal(\n",
    "                [\n",
    "                    pl.col(\"ends_with_letter\"),\n",
    "                    pl.col(\"has_5gram_repetition\"),\n",
    "                    pl.col(\"max_uncommon_binary\"),\n",
    "                ]\n",
    "            ).alias(\"rule\")\n",
    "        )\n",
    "        .to_numpy()\n",
    "        .ravel()\n",
    "        .astype(int)\n",
    "    )\n",
    "print(classification_report_np(y_val, yhat_val))\n",
    "residual_plot(_to_numpy(y_val), p_ens, \"Residuals: ensemble on val\")\n",
    "qq_plot(_to_numpy(y_val) - p_ens, \"QQ: residuals (val)\")\n",
    "plot_roc_pr(_to_numpy(y_val), p_ens, \"Validation ROC/PR (ensemble)\")\n",
    "plot_confusion(_to_numpy(y_val), yhat_val, \"Confusion (val, winner head)\")\n",
    "log_step_end(\"Validation diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55f5f2",
   "metadata": {},
   "source": [
    "## Predict test and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Predict test and save submission\")\n",
    "scores_lr = predict_proba_chunks(\n",
    "    lr_tuned, X_test, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "scores_nb = predict_proba_chunks(\n",
    "    nb_tuned, X_test, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "p_lr_te = _to_numpy(cal_lr.predict_proba(scores_lr))\n",
    "p_nb_te = _to_numpy(cal_nb.predict_proba(scores_nb))\n",
    "p_ens_te = best_w * p_nb_te + (1.0 - best_w) * p_lr_te\n",
    "if winner == \"threshold\":\n",
    "    yhat_te = (p_ens_te >= best_thr).astype(int)\n",
    "elif winner == \"prevalence\":\n",
    "    yhat_te = decode_prevalence(p_ens_te, val_pos_rate)\n",
    "else:\n",
    "    yhat_te = (\n",
    "        test.select(\n",
    "            pl.any_horizontal(\n",
    "                [\n",
    "                    pl.col(\"ends_with_letter\"),\n",
    "                    pl.col(\"has_5gram_repetition\"),\n",
    "                    pl.col(\"max_uncommon_binary\"),\n",
    "                ]\n",
    "            ).alias(\"rule\")\n",
    "        )\n",
    "        .to_numpy()\n",
    "        .ravel()\n",
    "        .astype(int)\n",
    "    )\n",
    "submission = pl.DataFrame({\"id\": test[\"id\"], \"label\": yhat_te})\n",
    "outputs_dir = \"outputs\"\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "submission_path = os.path.join(outputs_dir, \"submission_hvsm_prod_c.csv\")\n",
    "submission.write_csv(submission_path)\n",
    "print(\"Saved\", submission_path, \"with\", submission.height, \"rows\")\n",
    "_gc()\n",
    "log_step_end(\"Predict test and save submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
