{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16df9532",
   "metadata": {},
   "source": [
    "# Kaggle Baseline: TF\u2013IDF + XGBoost + Logistic Regression\n\nThis notebook implements the exact baseline pipeline you provided, with documentation, type hints, assertions, and diagnostic plots. It expects `data/train.csv`, `data/val.csv`, and `data/test.csv` to reside in `data/`. The outputs include validation reports and a `outputs/submission_hvsm_prod.csv` file for Kaggle.\n\nThe workflow:\n1. Load CSVs and engineer basic text features.\n2. Compute TF\u2013IDF up to trigrams.\n3. Train XGBoost and Logistic Regression models.\n4. Calibrate via Platt scaling.\n5. Ensemble (weighted average) and threshold tune on validation.\n6. Generate predictions on `data/test.csv`.\n\nAdditional diagnostics: QQ plot, residual plot, violin plot, and a brief sanity audit of the inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n\nfrom typing import List, Tuple\nimport os\nimport re\nimport warnings\nimport gc\nimport time\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (\n    accuracy_score, classification_report, f1_score\n)\nfrom scipy.sparse import hstack, csr_matrix\nfrom xgboost import XGBClassifier\n\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ntry:\n    import seaborn as sns  # optional, for violin plots\nexcept Exception:  # pragma: no cover\n    sns = None\n\ntry:\n    from textblob import TextBlob\nexcept Exception as e:  # pragma: no cover\n    TextBlob = None\n    warnings.warn(\n        'TextBlob not available; sentiment features will be zeros.'\n    )\nfrom datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a7035",
   "metadata": {},
   "source": [
    "## Utilities and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc() -> None:\n    gc.collect()\n\n\n_STEP_STARTS = {}\n\ndef log_step(msg: str) -> None:\n    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    print(f\"[{ts}] {msg}\", flush=True)\n\ndef log_step_start(name: str) -> None:\n    _STEP_STARTS[name] = time.perf_counter()\n    log_step(f\"START: {name}\")\n\ndef log_step_end(name: str) -> None:\n    start = _STEP_STARTS.pop(name, None)\n    if start is None:\n        log_step(f\"END: {name}\")\n    else:\n        elapsed = time.perf_counter() - start\n        log_step(f\"END: {name} (elapsed {elapsed:.1f}s)\")\n\n\ndef predict_proba_chunks(model, X, chunk_size: int = 50000) -> np.ndarray:\n    n = X.shape[0]\n    out = np.empty(n, dtype=np.float32)\n    for start in range(0, n, chunk_size):\n        end = min(start + chunk_size, n)\n        out[start:end] = model.predict_proba(X[start:end])[:, 1]\n    return out\n\n\ndef qq_plot(residuals: np.ndarray, title: str) -> None:\n    \"\"\"Draw a QQ plot of residuals.\n\n    Args:\n        residuals: Array of residuals.\n        title: Plot title.\n    \"\"\"\n    plt.figure(figsize=(5, 4))\n    stats.probplot(residuals, dist='norm', plot=plt)\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\ndef residual_plot(y_true: np.ndarray, y_prob: np.ndarray,\n                  title: str) -> None:\n    \"\"\"Scatter residuals vs predicted probabilities.\n\n    Args:\n        y_true: True binary labels.\n        y_prob: Predicted probabilities for the positive class.\n        title: Plot title.\n    \"\"\"\n    resid = y_true - y_prob\n    plt.figure(figsize=(5, 4))\n    plt.scatter(y_prob, resid, s=8)\n    plt.axhline(0.0, linestyle='--')\n    plt.xlabel('p(y=1)')\n    plt.ylabel('residual')\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\ndef violin_by_label(df: pd.DataFrame, label_col: str,\n                    feat_col: str, title: str) -> None:\n    \"\"\"Violin plot of a numeric feature split by label.\n\n    Args:\n        df: DataFrame containing labels and the feature.\n        label_col: Name of the label column.\n        feat_col: Name of the numeric feature column.\n        title: Plot title.\n    \"\"\"\n    if sns is None:  # fallback to simple boxplot if seaborn missing\n        plt.figure(figsize=(5, 4))\n        df.boxplot(column=feat_col, by=label_col)\n        plt.title(title)\n        plt.suptitle('')\n        plt.tight_layout()\n        plt.show()\n        return\n    plt.figure(figsize=(5, 4))\n    sns.violinplot(data=df, x=label_col, y=feat_col)\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e300b63",
   "metadata": {},
   "source": [
    "## Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Data loading and processing')\n",
    "def process_text_file(filename: str) -> pd.DataFrame:\n    \"\"\"Load CSV and compute simple text-derived features.\n\n    The file is expected to contain at least a `text` column, and, for\n    training/validation, a `label` column.\n\n    Args:\n        filename: CSV path relative to the notebook directory.\n\n    Returns:\n        DataFrame with additional feature columns.\n\n    Raises:\n        AssertionError: If required columns are missing.\n    \"\"\"\n    df = pd.read_csv(os.path.join(filename))\n    assert 'text' in df.columns, 'CSV must contain a text column.'\n    df['text'] = df['text'].astype(str)\n\n    df['text_length'] = df['text'].str.len()\n    df['word_count'] = df['text'].str.split().str.len()\n    df['sentence_count'] = (\n        df['text'].str.count(r'[.!?]+').replace(0, 1)\n    )\n    df['avg_sentence_length'] = (\n        (df['word_count'] / df['sentence_count']).clip(upper=100)\n    )\n    df['punct_count'] = df['text'].str.count(r'[^\\w\\s]')\n    df['punct_ratio'] = (\n        (df['punct_count'] / df['text_length']).clip(0, 0.3)\n    )\n\n    def ttr(text: str) -> float:\n        words = re.findall(r'\\S+', text.lower())\n        return len(set(words)) / len(words) if len(words) > 0 else 0.0\n\n    tqdm.pandas()\n    df['ttr'] = df['text'].progress_apply(ttr)\n    return df\n",
    "log_step_end('Data loading and processing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44474c",
   "metadata": {},
   "source": [
    "## TF\u2013IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('TF\u2013IDF features')\n",
    "def add_ngram_tfidf(\n    train_texts: pd.Series,\n    valid_texts: pd.Series,\n    test_texts: pd.Series,\n    n: int = 3,\n    max_features: int = 5000,\n) -> Tuple[csr_matrix, csr_matrix, csr_matrix]:\n    \"\"\"Build an n-gram TF\u2013IDF representation.\n\n    Args:\n        train_texts: Training texts.\n        valid_texts: Validation texts.\n        test_texts: Test texts.\n        n: Maximum n-gram size.\n        max_features: Vocabulary size cap.\n\n    Returns:\n        Tuple of sparse matrices (train, valid, test).\n    \"\"\"\n    vectorizer = TfidfVectorizer(\n        ngram_range=(1, n), max_features=max_features,\n        stop_words='english', dtype=np.float32\n    )\n    X_train_ng = vectorizer.fit_transform(train_texts)\n    X_valid_ng = vectorizer.transform(valid_texts)\n    X_test_ng = vectorizer.transform(test_texts)\n    return X_train_ng, X_valid_ng, X_test_ng\n",
    "log_step_end('TF\u2013IDF features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518732dd",
   "metadata": {},
   "source": [
    "## Sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9898a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Sentiment features')\n",
    "def add_sentiment_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Attach TextBlob sentiment features.\n\n    If TextBlob is unavailable, the features are set to zeros with a\n    warning.\n\n    Args:\n        df: DataFrame with `text` column.\n\n    Returns:\n        DataFrame with `sentiment_polarity` and `sentiment_subjectivity`.\n    \"\"\"\n    tqdm.pandas()\n    if TextBlob is None:\n        df['sentiment_polarity'] = 0.0\n        df['sentiment_subjectivity'] = 0.0\n        return df\n\n    def _pol(x: str) -> float:\n        return float(TextBlob(x).sentiment.polarity)\n\n    def _subj(x: str) -> float:\n        return float(TextBlob(x).sentiment.subjectivity)\n\n    df['sentiment_polarity'] = df['text'].progress_apply(_pol)\n    df['sentiment_subjectivity'] = df['text'].progress_apply(_subj)\n    return df\n",
    "log_step_end('Sentiment features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7beafe4",
   "metadata": {},
   "source": [
    "## Load data (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Load data (train/val/test)')\n# Strict file names in the `data/` folder\ntrain = process_text_file('data/train.csv')\nvalidation = process_text_file('data/val.csv')\ntest = process_text_file('data/test.csv')\n\n# Basic schema checks\nfor name, df in [('train', train), ('val', validation), ('test', test)]:\n    assert 'text' in df.columns, f\"{name} missing 'text' column\"\nassert 'label' in train.columns, 'train must have label'\nassert 'label' in validation.columns, 'val must have label'\nassert 'label' not in test.columns, 'test must NOT have label'\n\nprint('Rows: train', len(train), ' val', len(validation), ' test', len(test))\nlog_step_end('Load data (train/val/test)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be7880",
   "metadata": {},
   "source": [
    "## Add sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Add sentiment features')\n",
    "train = add_sentiment_features(train)\nvalidation = add_sentiment_features(validation)\ntest = add_sentiment_features(test)\n",
    "log_step_end('Add sentiment features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a210fd8",
   "metadata": {},
   "source": [
    "## Assemble features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Assemble features')\n",
    "feature_cols: List[str] = [\n    'text_length', 'word_count', 'ttr', 'sentence_count',\n    'avg_sentence_length', 'punct_ratio',\n    'sentiment_polarity', 'sentiment_subjectivity',\n]\n\nX_train_basic = train[feature_cols]\nX_valid_basic = validation[feature_cols]\nX_test_basic = test[feature_cols]\n\nX_train_ngram, X_valid_ngram, X_test_ngram = add_ngram_tfidf(\n    train['text'], validation['text'], test['text'],\n    n=3, max_features=5000\n)\n\nX_train = hstack([csr_matrix(X_train_basic.to_numpy(dtype=np.float32)), X_train_ngram])\nX_valid = hstack([csr_matrix(X_valid_basic.to_numpy(dtype=np.float32)), X_valid_ngram])\nX_test = hstack([csr_matrix(X_test_basic.to_numpy(dtype=np.float32)), X_test_ngram])\n\ny_train = train['label']\ny_valid = validation['label']\n\nprint('Shapes:')\nprint('  X_train:', X_train.shape)\nprint('  X_valid:', X_valid.shape)\nprint('  X_test :', X_test.shape)\n_gc()\n",
    "log_step_end('Assemble features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fc490",
   "metadata": {},
   "source": [
    "## Class balance and scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Class balance and scale_pos_weight')\n",
    "counter = Counter(y_train)\nassert 0 in counter and 1 in counter, 'labels must be binary {0,1}'\nscale_pos_weight = counter[0] / counter[1]\nprint('Class counts:', counter)\nprint('scale_pos_weight:', scale_pos_weight)",
    "\n",
    "log_step_end('Class balance and scale_pos_weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76837d",
   "metadata": {},
   "source": [
    "## Train XGBoost and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Train XGBoost and Logistic Regression')\nxgb = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    n_jobs=1,\n    scale_pos_weight=scale_pos_weight,\n)\nlog_step_start('Fold 1/1 (single split)')\nlog_step_start('XGB training epochs')\nxgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=True)\nlog_step_end('XGB training epochs')\n\nlr = LogisticRegression(max_iter=1000, random_state=42)\nlog_step_start('LR fit')\nlr.fit(X_train, y_train)\nlog_step_end('LR fit')\nlog_step_end('Fold 1/1 (single split)')\nprint('Models trained.')\n_gc()\nlog_step_end('Train XGBoost and Logistic Regression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2efc4",
   "metadata": {},
   "source": [
    "## Calibrate with Platt scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Calibrate with Platt scaling')\n",
    "calibrated_xgb = CalibratedClassifierCV(xgb, method='sigmoid',\n                                        cv='prefit')\ncalibrated_xgb.fit(X_valid, y_valid)\n\ncalibrated_lr = CalibratedClassifierCV(lr, method='sigmoid',\n                                       cv='prefit')\ncalibrated_lr.fit(X_valid, y_valid)\nprint('Models calibrated.')\n_gc()\n",
    "log_step_end('Calibrate with Platt scaling')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbe065",
   "metadata": {},
   "source": [
    "## Ensemble and threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Ensemble and threshold tuning')\n",
    "val_pred_proba_xgb = calibrated_xgb.predict_proba(X_valid)[:, 1]\nval_pred_proba_lr = calibrated_lr.predict_proba(X_valid)[:, 1]\nval_pred_proba_ensemble = (\n    0.6 * val_pred_proba_xgb + 0.4 * val_pred_proba_lr\n)\n\nthresholds = np.arange(0.1, 0.9, 0.01)\nbest_threshold = 0.5\nbest_f1 = -1.0\n\nfor thr in thresholds:\n    val_pred_thr = (val_pred_proba_ensemble >= thr).astype(int)\n    f1 = f1_score(y_valid, val_pred_thr)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_threshold = float(thr)\n\nprint(\n    f'Best threshold: {best_threshold:.2f} with F1: {best_f1:.4f}'\n)",
    "\n",
    "log_step_end('Ensemble and threshold tuning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91516453",
   "metadata": {},
   "source": [
    "## Validation report and diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Validation report and diagnostics')\n",
    "val_pred_final = (val_pred_proba_ensemble >= best_threshold).astype(int)\nprint(classification_report(y_valid, val_pred_final))\n\n# Diagnostics\nresidual_plot(y_valid.to_numpy(), val_pred_proba_ensemble,\n              'Residuals: validation ensemble')\nqq_plot(y_valid.to_numpy() - val_pred_proba_ensemble,\n        'QQ plot: residuals (validation)')\ntry:\n    # Violin on a basic feature (train) to visualize label differences\n    violin_by_label(train, 'label', 'text_length',\n                    'Text length by label (train)')\nexcept Exception as e:\n    warnings.warn(f'Violin plot skipped: {e}')\n",
    "log_step_end('Validation report and diagnostics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3f803",
   "metadata": {},
   "source": [
    "## Predict on test and write submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Predict on test and write submission')\n",
    "p_xgb_te = predict_proba_chunks(calibrated_xgb, X_test)\np_lr_te = predict_proba_chunks(calibrated_lr, X_test)\np_ens_te = 0.6 * p_xgb_te + 0.4 * p_lr_te\nyhat_te = (p_ens_te >= best_threshold).astype(int)\nsubmission = pd.DataFrame({'id': test['id'], 'label': yhat_te})\noutputs_dir = 'outputs'\nos.makedirs(outputs_dir, exist_ok=True)\nsubmission_path = os.path.join(outputs_dir, 'submission_hvsm_prod.csv')\nsubmission.to_csv(submission_path, index=False)\nprint('Saved', submission_path, 'with', len(submission), 'rows')\n_gc()\n",
    "log_step_end('Predict on test and write submission')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}