{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2838c9b9d5845b28782e1046008c7c3",
   "metadata": {},
   "source": [
    "# HVSM Notebook: hvsm_prod_a.ipynb\n",
    "\n",
    "- Runs with: slurm_scripts/hvsm_job_a.sh\n",
    "- Purpose: GPU models (cuML) with CPU TF-IDF and LR/NB ensemble.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8a10021ab43c7a642458776675912",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (papermill)\n",
    "DATA_DIR = \"data\"\n",
    "TRAIN_CSV = \"data/train.csv\"\n",
    "VAL_CSV = \"data/val.csv\"\n",
    "TEST_CSV = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df9532",
   "metadata": {},
   "source": [
    "# HVSM \u2014 GPU TF-IDF + cuML LR/NB\n\nThis notebook mirrors the baseline pipeline but runs GPU-first with Polars + cuML. It expects `data/train.csv`, `data/val.csv`, and `data/test.csv` to reside in `data/`. The outputs include validation reports and `outputs/submission_hvsm_prod_a.csv`.\n\nThe workflow:\n1. Load CSVs and engineer basic text features.\n2. Compute TF-IDF up to 7-grams.\n3. Train cuML Logistic Regression and MultinomialNB.\n4. Calibrate via Platt scaling.\n5. Ensemble (weighted average) and threshold tune on validation.\n6. Generate predictions on `data/test.csv`.\n\nAdditional diagnostics: QQ plot, residual plot, violin plot, and a brief sanity audit of the inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import List\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer as SkTfidfVectorizer,\n",
    ")\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cupyx.scipy.sparse as cpx_sparse\n",
    "    import cuml\n",
    "    from cuml.linear_model import LogisticRegression\n",
    "    from cuml.naive_bayes import MultinomialNB\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"cuML + CUDA (cupy/cudf) required for GPU-first run.\"\n",
    "    ) from e\n",
    "\n",
    "try:\n",
    "    import seaborn as sns  # optional, for violin plots\n",
    "except Exception:  # pragma: no cover\n",
    "    sns = None\n",
    "\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except Exception as e:  # pragma: no cover\n",
    "    TextBlob = None\n",
    "    warnings.warn(\"TextBlob not available; sentiment features will be zeros.\")\n",
    "\n",
    "cuml.set_global_output_type(\"cupy\")\n",
    "RANDOM_SEED = 42\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a7035",
   "metadata": {},
   "source": [
    "## Utilities and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc() -> None:\n",
    "    gc.collect()\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        try:\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "_STEP_STARTS = {}\n",
    "\n",
    "\n",
    "def log_step(msg: str) -> None:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{ts}] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "def log_step_start(name: str) -> None:\n",
    "    _STEP_STARTS[name] = time.perf_counter()\n",
    "    log_step(f\"START: {name}\")\n",
    "\n",
    "\n",
    "def log_step_end(name: str) -> None:\n",
    "    start = _STEP_STARTS.pop(name, None)\n",
    "    if start is None:\n",
    "        log_step(f\"END: {name}\")\n",
    "    else:\n",
    "        elapsed = time.perf_counter() - start\n",
    "        log_step(f\"END: {name} (elapsed {elapsed:.1f}s)\")\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        try:\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def predict_proba_chunks(model, X, chunk_size: int = 50000):\n",
    "    n = X.shape[0]\n",
    "    out = cp.empty(n, dtype=cp.float32)\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        out[start:end] = model.predict_proba(X[start:end])[:, 1]\n",
    "        _gc()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _to_numpy(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    if hasattr(x, \"get\"):\n",
    "        return x.get()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def f1_score_np(y_true, y_pred) -> float:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    return float(2 * precision * recall / (precision + recall + 1e-12))\n",
    "\n",
    "\n",
    "def classification_report_np(y_true, y_pred) -> str:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "\n",
    "    def _prf(label):\n",
    "        tp = int(((y_true == label) & (y_pred == label)).sum())\n",
    "        fp = int(((y_true != label) & (y_pred == label)).sum())\n",
    "        fn = int(((y_true == label) & (y_pred != label)).sum())\n",
    "        precision = tp / (tp + fp + 1e-12)\n",
    "        recall = tp / (tp + fn + 1e-12)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "        support = int((y_true == label).sum())\n",
    "        return precision, recall, f1, support\n",
    "\n",
    "    p0, r0, f0, s0 = _prf(0)\n",
    "    p1, r1, f1, s1 = _prf(1)\n",
    "    acc = float((y_true == y_pred).mean())\n",
    "    macro_p = (p0 + p1) / 2\n",
    "    macro_r = (r0 + r1) / 2\n",
    "    macro_f = (f0 + f1) / 2\n",
    "    total = s0 + s1\n",
    "    w_p = (p0 * s0 + p1 * s1) / max(total, 1)\n",
    "    w_r = (r0 * s0 + r1 * s1) / max(total, 1)\n",
    "    w_f = (f0 * s0 + f1 * s1) / max(total, 1)\n",
    "    lines = [\n",
    "        \"              precision    recall  f1-score   support\",\n",
    "        f\"           0       {p0:0.3f}      {r0:0.3f}      {f0:0.3f}      {s0:5d}\",\n",
    "        f\"           1       {p1:0.3f}      {r1:0.3f}      {f1:0.3f}      {s1:5d}\",\n",
    "        \"\",\n",
    "        f\"    accuracy                           {acc:0.3f}      {total:5d}\",\n",
    "        f\"   macro avg       {macro_p:0.3f}      {macro_r:0.3f}      {macro_f:0.3f}      {total:5d}\",\n",
    "        f\"weighted avg       {w_p:0.3f}      {w_r:0.3f}      {w_f:0.3f}      {total:5d}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def qq_plot(residuals: np.ndarray, title: str) -> None:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def residual_plot(y_true: np.ndarray, y_prob: np.ndarray, title: str) -> None:\n",
    "    resid = y_true - y_prob\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(y_prob, resid, s=8)\n",
    "    plt.axhline(0.0, linestyle=\"--\")\n",
    "    plt.xlabel(\"p(y=1)\")\n",
    "    plt.ylabel(\"residual\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def violin_by_label(\n",
    "    df: pl.DataFrame, label_col: str, feat_col: str, title: str\n",
    ") -> None:\n",
    "    y = df.select(label_col).to_numpy().ravel()\n",
    "    x = df.select(feat_col).to_numpy().ravel()\n",
    "    if sns is None:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.boxplot([x[y == 0], x[y == 1]], labels=[\"0\", \"1\"])\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.violinplot(x=y, y=x)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e300b63",
   "metadata": {},
   "source": [
    "## Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Data loading and processing\")\n",
    "\n",
    "\n",
    "def _ttr(text: str) -> float:\n",
    "    words = re.findall(r\"\\S+\", text.lower())\n",
    "    return float(len(set(words)) / len(words)) if words else 0.0\n",
    "\n",
    "\n",
    "def process_text_file(filename: str) -> pl.DataFrame:\n",
    "    df = pl.read_csv(os.path.join(filename))\n",
    "    assert \"text\" in df.columns, \"CSV must contain a text column.\"\n",
    "    df = df.with_columns(pl.col(\"text\").cast(pl.Utf8))\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"text\").str.len_chars().alias(\"text_length\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"\\S+\").alias(\"word_count\"),\n",
    "            pl.col(\"text\")\n",
    "            .str.count_matches(r\"[.!?]+\")\n",
    "            .alias(\"sentence_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"[^\\w\\s]\").alias(\"punct_count\"),\n",
    "        ]\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"sentence_count\") == 0)\n",
    "            .then(1)\n",
    "            .otherwise(pl.col(\"sentence_count\"))\n",
    "            .alias(\"sentence_count\"),\n",
    "            pl.when(pl.col(\"text_length\") == 0)\n",
    "            .then(1)\n",
    "            .otherwise(pl.col(\"text_length\"))\n",
    "            .alias(\"text_length_safe\"),\n",
    "        ]\n",
    "    )\n",
    "    avg_sentence_expr = pl.col(\"word_count\") / pl.col(\"sentence_count\")\n",
    "    punct_expr = pl.col(\"punct_count\") / pl.col(\"text_length_safe\")\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(avg_sentence_expr > 100)\n",
    "            .then(100)\n",
    "            .otherwise(avg_sentence_expr)\n",
    "            .alias(\"avg_sentence_length\"),\n",
    "            pl.when(punct_expr > 0.3)\n",
    "            .then(0.3)\n",
    "            .otherwise(punct_expr)\n",
    "            .alias(\"punct_ratio\"),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_ttr, return_dtype=pl.Float64)\n",
    "            .alias(\"ttr\"),\n",
    "        ]\n",
    "    )\n",
    "    df = df.drop([\"text_length_safe\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "log_step_end(\"Data loading and processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44474c",
   "metadata": {},
   "source": [
    "## TF\u2013IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"TF\u2013IDF features\")\n",
    "TFIDF_NGRAM_MAX = 7\n",
    "TFIDF_MAX_FEATURES = 5000\n",
    "TFIDF_CHUNK_SIZE = 5000\n",
    "PROBA_CHUNK_SIZE = 20000\n",
    "\n",
    "\n",
    "def _transform_in_chunks_cpu(vectorizer, texts, chunk_size: int):\n",
    "    n = len(texts)\n",
    "    chunks = []\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        X_chunk = vectorizer.transform(texts[start:end])\n",
    "        chunks.append(X_chunk)\n",
    "        _gc()\n",
    "    if not chunks:\n",
    "        return sp.csr_matrix((0, 0))\n",
    "    if len(chunks) == 1:\n",
    "        return chunks[0].tocsr()\n",
    "    return sp.vstack(chunks).tocsr()\n",
    "\n",
    "\n",
    "def add_ngram_tfidf(\n",
    "    train_texts: list[str],\n",
    "    valid_texts: list[str],\n",
    "    test_texts: list[str],\n",
    "    n: int = TFIDF_NGRAM_MAX,\n",
    "    max_features: int = TFIDF_MAX_FEATURES,\n",
    "    chunk_size: int = TFIDF_CHUNK_SIZE,\n",
    ") -> tuple[\n",
    "    cpx_sparse.csr_matrix, cpx_sparse.csr_matrix, cpx_sparse.csr_matrix\n",
    "]:\n",
    "    vectorizer = SkTfidfVectorizer(\n",
    "        ngram_range=(1, n),\n",
    "        max_features=max_features,\n",
    "        stop_words=\"english\",\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    vectorizer.fit(train_texts)\n",
    "    _gc()\n",
    "    X_train_cpu = _transform_in_chunks_cpu(vectorizer, train_texts, chunk_size)\n",
    "    X_valid_cpu = _transform_in_chunks_cpu(vectorizer, valid_texts, chunk_size)\n",
    "    X_test_cpu = _transform_in_chunks_cpu(vectorizer, test_texts, chunk_size)\n",
    "    _gc()\n",
    "    X_train_ng = cpx_sparse.csr_matrix(X_train_cpu)\n",
    "    X_valid_ng = cpx_sparse.csr_matrix(X_valid_cpu)\n",
    "    X_test_ng = cpx_sparse.csr_matrix(X_test_cpu)\n",
    "    del X_train_cpu, X_valid_cpu, X_test_cpu\n",
    "    _gc()\n",
    "    return X_train_ng, X_valid_ng, X_test_ng\n",
    "\n",
    "\n",
    "log_step_end(\"TF\u2013IDF features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518732dd",
   "metadata": {},
   "source": [
    "## Sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9898a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Sentiment features\")\n",
    "\n",
    "\n",
    "def add_sentiment_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if TextBlob is None:\n",
    "        return df.with_columns(\n",
    "            [\n",
    "                pl.lit(0.0).alias(\"sentiment_polarity\"),\n",
    "                pl.lit(0.0).alias(\"sentiment_subjectivity\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _pol(x: str) -> float:\n",
    "        return float(TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    def _subj(x: str) -> float:\n",
    "        return float(TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_pol, return_dtype=pl.Float64)\n",
    "            .alias(\"sentiment_polarity\"),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_subj, return_dtype=pl.Float64)\n",
    "            .alias(\"sentiment_subjectivity\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "log_step_end(\"Sentiment features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7beafe4",
   "metadata": {},
   "source": [
    "## Load data (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Load data (train/val/test)\")\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / DATA_DIR).exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / DATA_DIR).exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "\n",
    "\n",
    "def resolve_path(path_str: str) -> str:\n",
    "    p = Path(path_str)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    if p.parent == Path(\".\"):\n",
    "        data_dir = Path(DATA_DIR)\n",
    "        if not data_dir.is_absolute():\n",
    "            data_dir = PROJECT_ROOT / data_dir\n",
    "        candidate = data_dir / p.name\n",
    "        if candidate.exists():\n",
    "            return str(candidate)\n",
    "    return str((PROJECT_ROOT / p).resolve())\n",
    "\n",
    "\n",
    "# Reassemble chunked CSVs if needed\n",
    "def ensure_chunked_csv(path: Path) -> None:\n",
    "    if path.exists():\n",
    "        return\n",
    "    parts = sorted(path.parent.glob(path.name + \".part*\"))\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(f\"Missing {path} and no chunk files found.\")\n",
    "    tmp_path = path.with_suffix(path.suffix + \".tmp\")\n",
    "    if tmp_path.exists():\n",
    "        tmp_path.unlink()\n",
    "    hasher = hashlib.sha256()\n",
    "    with tmp_path.open(\"wb\") as out:\n",
    "        for part in parts:\n",
    "            with part.open(\"rb\") as f:\n",
    "                while True:\n",
    "                    chunk = f.read(1024 * 1024)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    out.write(chunk)\n",
    "                    hasher.update(chunk)\n",
    "    sha_path = path.with_suffix(path.suffix + \".sha256\")\n",
    "    if sha_path.exists():\n",
    "        expected = sha_path.read_text().split()[0]\n",
    "        actual = hasher.hexdigest()\n",
    "        if expected != actual:\n",
    "            tmp_path.unlink(missing_ok=True)\n",
    "            raise ValueError(\n",
    "                f\"SHA256 mismatch for {path}: expected {expected} got {actual}\"\n",
    "            )\n",
    "    tmp_path.replace(path)\n",
    "    log_step(f\"Reassembled {path} from {len(parts)} chunks.\")\n",
    "\n",
    "\n",
    "# Strict file names in the `data/` folder\n",
    "train_path = Path(resolve_path(TRAIN_CSV))\n",
    "val_path = Path(resolve_path(VAL_CSV))\n",
    "test_path = Path(resolve_path(TEST_CSV))\n",
    "ensure_chunked_csv(train_path)\n",
    "ensure_chunked_csv(val_path)\n",
    "ensure_chunked_csv(test_path)\n",
    "\n",
    "train = process_text_file(str(train_path))\n",
    "validation = process_text_file(str(val_path))\n",
    "test = process_text_file(str(test_path))\n",
    "\n",
    "# Basic schema checks\n",
    "for name, df in [(\"train\", train), (\"val\", validation), (\"test\", test)]:\n",
    "    assert \"text\" in df.columns, f\"{name} missing 'text' column\"\n",
    "assert \"label\" in train.columns, \"train must have label\"\n",
    "assert \"label\" in validation.columns, \"val must have label\"\n",
    "assert \"label\" not in test.columns, \"test must NOT have label\"\n",
    "assert \"id\" in test.columns, \"test must have id\"\n",
    "\n",
    "print(\n",
    "    \"Rows: train\",\n",
    "    train.height,\n",
    "    \" val\",\n",
    "    validation.height,\n",
    "    \" test\",\n",
    "    test.height,\n",
    ")\n",
    "log_step_end(\"Load data (train/val/test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be7880",
   "metadata": {},
   "source": [
    "## Add sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Add sentiment features\")\n",
    "train = add_sentiment_features(train)\n",
    "validation = add_sentiment_features(validation)\n",
    "test = add_sentiment_features(test)\n",
    "log_step_end(\"Add sentiment features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a210fd8",
   "metadata": {},
   "source": [
    "## Assemble features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Assemble features\")\n",
    "feature_cols: List[str] = [\n",
    "    \"text_length\",\n",
    "    \"word_count\",\n",
    "    \"ttr\",\n",
    "    \"sentence_count\",\n",
    "    \"avg_sentence_length\",\n",
    "    \"punct_ratio\",\n",
    "    \"sentiment_polarity\",\n",
    "    \"sentiment_subjectivity\",\n",
    "]\n",
    "\n",
    "X_train_basic = cpx_sparse.csr_matrix(\n",
    "    cp.asarray(train.select(feature_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "X_valid_basic = cpx_sparse.csr_matrix(\n",
    "    cp.asarray(validation.select(feature_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "X_test_basic = cpx_sparse.csr_matrix(\n",
    "    cp.asarray(test.select(feature_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "\n",
    "train_text = train[\"text\"].to_list()\n",
    "valid_text = validation[\"text\"].to_list()\n",
    "test_text = test[\"text\"].to_list()\n",
    "\n",
    "X_train_ngram, X_valid_ngram, X_test_ngram = add_ngram_tfidf(\n",
    "    train_text,\n",
    "    valid_text,\n",
    "    test_text,\n",
    "    n=TFIDF_NGRAM_MAX,\n",
    "    max_features=TFIDF_MAX_FEATURES,\n",
    "    chunk_size=TFIDF_CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "X_train = cpx_sparse.hstack([X_train_basic, X_train_ngram]).tocsr()\n",
    "X_valid = cpx_sparse.hstack([X_valid_basic, X_valid_ngram]).tocsr()\n",
    "X_test = cpx_sparse.hstack([X_test_basic, X_test_ngram]).tocsr()\n",
    "\n",
    "del (\n",
    "    X_train_basic,\n",
    "    X_valid_basic,\n",
    "    X_test_basic,\n",
    "    X_train_ngram,\n",
    "    X_valid_ngram,\n",
    "    X_test_ngram,\n",
    ")\n",
    "_gc()\n",
    "\n",
    "y_train = cp.asarray(train[\"label\"].to_numpy()).astype(cp.int32)\n",
    "y_valid = cp.asarray(validation[\"label\"].to_numpy()).astype(cp.int32)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  X_valid:\", X_valid.shape)\n",
    "print(\"  X_test :\", X_test.shape)\n",
    "del train_text, valid_text, test_text\n",
    "_gc()\n",
    "log_step_end(\"Assemble features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fc490",
   "metadata": {},
   "source": [
    "## Class balance and scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Class balance and scale_pos_weight\")\n",
    "counter = Counter(_to_numpy(y_train))\n",
    "assert 0 in counter and 1 in counter, \"labels must be binary {0,1}\"\n",
    "scale_pos_weight = counter[0] / max(counter[1], 1)\n",
    "print(\"Class counts:\", counter)\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "log_step_end(\"Class balance and scale_pos_weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76837d",
   "metadata": {},
   "source": [
    "## Train cuML Logistic Regression and MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Train cuML Logistic Regression and MultinomialNB\")\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_root = Path(PROJECT_ROOT) if \"PROJECT_ROOT\" in globals() else Path.cwd()\n",
    "ckpt_dir = ckpt_root / \"checkpoints\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _model_signature(name, params, X_shape):\n",
    "    payload = {\n",
    "        \"name\": name,\n",
    "        \"params\": params,\n",
    "        \"X_shape\": list(X_shape),\n",
    "    }\n",
    "    raw = json.dumps(payload, sort_keys=True, default=str)\n",
    "    return hashlib.md5(raw.encode()).hexdigest()[:10]\n",
    "\n",
    "\n",
    "def _save_model(model, path: Path) -> None:\n",
    "    if hasattr(model, \"save\"):\n",
    "        model.save(str(path))\n",
    "        return\n",
    "    joblib.dump(model, path)\n",
    "\n",
    "\n",
    "def _load_model(cls, path: Path):\n",
    "    if hasattr(cls, \"load\"):\n",
    "        try:\n",
    "            return cls.load(str(path))\n",
    "        except Exception:\n",
    "            pass\n",
    "    model = cls()\n",
    "    if hasattr(model, \"load\"):\n",
    "        try:\n",
    "            model.load(str(path))\n",
    "            return model\n",
    "        except Exception:\n",
    "            pass\n",
    "    return joblib.load(path)\n",
    "\n",
    "\n",
    "lr_params = {\"max_iter\": 1000, \"random_state\": RANDOM_SEED}\n",
    "nb_params = {}\n",
    "\n",
    "lr_sig = _model_signature(\"lr\", lr_params, X_train.shape)\n",
    "nb_sig = _model_signature(\"nb\", nb_params, X_train.shape)\n",
    "\n",
    "lr_ckpt = ckpt_dir / f\"hvsm_prod_a_lr_{lr_sig}.bin\"\n",
    "nb_ckpt = ckpt_dir / f\"hvsm_prod_a_nb_{nb_sig}.bin\"\n",
    "\n",
    "log_step_start(\"Fold 1/1 (single split)\")\n",
    "log_step_start(\"LR fit\")\n",
    "if lr_ckpt.exists():\n",
    "    lr = _load_model(LogisticRegression, lr_ckpt)\n",
    "    log_step(f\"Loaded LR checkpoint: {lr_ckpt}\")\n",
    "else:\n",
    "    lr = LogisticRegression(**lr_params)\n",
    "    lr.fit(X_train, y_train)\n",
    "    _save_model(lr, lr_ckpt)\n",
    "    log_step(f\"Saved LR checkpoint: {lr_ckpt}\")\n",
    "log_step_end(\"LR fit\")\n",
    "\n",
    "log_step_start(\"NB fit\")\n",
    "if nb_ckpt.exists():\n",
    "    nb = _load_model(MultinomialNB, nb_ckpt)\n",
    "    log_step(f\"Loaded NB checkpoint: {nb_ckpt}\")\n",
    "else:\n",
    "    nb = MultinomialNB(**nb_params)\n",
    "    nb.fit(X_train, y_train)\n",
    "    _save_model(nb, nb_ckpt)\n",
    "    log_step(f\"Saved NB checkpoint: {nb_ckpt}\")\n",
    "log_step_end(\"NB fit\")\n",
    "log_step_end(\"Fold 1/1 (single split)\")\n",
    "print(\"Models trained.\")\n",
    "_gc()\n",
    "log_step_end(\"Train cuML Logistic Regression and MultinomialNB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2efc4",
   "metadata": {},
   "source": [
    "## Calibrate with Platt scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Calibrate with Platt scaling\")\n",
    "\n",
    "\n",
    "class PlattCalibrator:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    def fit(self, scores, y):\n",
    "        scores = cp.asarray(scores).reshape(-1, 1)\n",
    "        self.model.fit(scores, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, scores):\n",
    "        scores = cp.asarray(scores).reshape(-1, 1)\n",
    "        return self.model.predict_proba(scores)[:, 1]\n",
    "\n",
    "\n",
    "calibrated_nb = PlattCalibrator().fit(nb.predict_proba(X_valid)[:, 1], y_valid)\n",
    "calibrated_lr = PlattCalibrator().fit(lr.predict_proba(X_valid)[:, 1], y_valid)\n",
    "print(\"Models calibrated.\")\n",
    "_gc()\n",
    "log_step_end(\"Calibrate with Platt scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbe065",
   "metadata": {},
   "source": [
    "## Ensemble and threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Ensemble and threshold tuning\")\n",
    "scores_nb = predict_proba_chunks(nb, X_valid, chunk_size=PROBA_CHUNK_SIZE)\n",
    "scores_lr = predict_proba_chunks(lr, X_valid, chunk_size=PROBA_CHUNK_SIZE)\n",
    "val_pred_proba_nb = _to_numpy(calibrated_nb.predict_proba(scores_nb))\n",
    "val_pred_proba_lr = _to_numpy(calibrated_lr.predict_proba(scores_lr))\n",
    "val_pred_proba_ensemble = 0.6 * val_pred_proba_nb + 0.4 * val_pred_proba_lr\n",
    "\n",
    "y_valid_np = _to_numpy(y_valid)\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "best_threshold = 0.5\n",
    "best_f1 = -1.0\n",
    "\n",
    "for thr in thresholds:\n",
    "    val_pred_thr = (val_pred_proba_ensemble >= thr).astype(int)\n",
    "    f1 = f1_score_np(y_valid_np, val_pred_thr)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = float(thr)\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f} with F1: {best_f1:.4f}\")\n",
    "_gc()\n",
    "log_step_end(\"Ensemble and threshold tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91516453",
   "metadata": {},
   "source": [
    "## Validation report and diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Validation report and diagnostics\")\n",
    "val_pred_final = (val_pred_proba_ensemble >= best_threshold).astype(int)\n",
    "print(classification_report_np(y_valid_np, val_pred_final))\n",
    "\n",
    "residual_plot(\n",
    "    y_valid_np, val_pred_proba_ensemble, \"Residuals: validation ensemble\"\n",
    ")\n",
    "qq_plot(\n",
    "    y_valid_np - val_pred_proba_ensemble, \"QQ plot: residuals (validation)\"\n",
    ")\n",
    "try:\n",
    "    violin_by_label(\n",
    "        train, \"label\", \"text_length\", \"Text length by label (train)\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Violin plot skipped: {e}\")\n",
    "log_step_end(\"Validation report and diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3f803",
   "metadata": {},
   "source": [
    "## Predict on test and write submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Predict on test and write submission\")\n",
    "scores_nb = predict_proba_chunks(nb, X_test, chunk_size=PROBA_CHUNK_SIZE)\n",
    "scores_lr = predict_proba_chunks(lr, X_test, chunk_size=PROBA_CHUNK_SIZE)\n",
    "p_nb_te = _to_numpy(calibrated_nb.predict_proba(scores_nb))\n",
    "p_lr_te = _to_numpy(calibrated_lr.predict_proba(scores_lr))\n",
    "p_ens_te = 0.6 * p_nb_te + 0.4 * p_lr_te\n",
    "yhat_te = (p_ens_te >= best_threshold).astype(int)\n",
    "\n",
    "submission = pl.DataFrame({\"id\": test[\"id\"], \"label\": yhat_te})\n",
    "outputs_dir = \"outputs\"\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "submission_path = os.path.join(outputs_dir, \"submission_hvsm_prod_a.csv\")\n",
    "submission.write_csv(submission_path)\n",
    "print(\"Saved\", submission_path, \"with\", submission.height, \"rows\")\n",
    "_gc()\n",
    "log_step_end(\"Predict on test and write submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
