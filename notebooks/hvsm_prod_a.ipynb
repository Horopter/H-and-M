{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16df9532",
   "metadata": {},
   "source": [
    "# HVSM \u2014 GPU TF-IDF + cuML LR/NB\n\nThis notebook mirrors the baseline pipeline but runs GPU-first with Polars + cuML. It expects `data/train.csv`, `data/val.csv`, and `data/test.csv` to reside in `data/`. The outputs include validation reports and `outputs/submission_hvsm_prod_a.csv`.\n\nThe workflow:\n1. Load CSVs and engineer basic text features.\n2. Compute TF-IDF up to trigrams.\n3. Train cuML Logistic Regression and MultinomialNB.\n4. Calibrate via Platt scaling.\n5. Ensemble (weighted average) and threshold tune on validation.\n6. Generate predictions on `data/test.csv`.\n\nAdditional diagnostics: QQ plot, residual plot, violin plot, and a brief sanity audit of the inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n\nfrom typing import List, Tuple\nimport os\nimport re\nimport warnings\nimport gc\nimport time\nfrom collections import Counter\n\nimport numpy as np\nimport polars as pl\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ntry:\n    import cupy as cp\n    import cudf\n    import cupyx.scipy.sparse as cpx_sparse\n    import cuml\n    from cuml.feature_extraction.text import TfidfVectorizer\n    from cuml.linear_model import LogisticRegression\n    from cuml.naive_bayes import MultinomialNB\nexcept Exception as e:\n    raise RuntimeError('cuML + CUDA (cupy/cudf) required for GPU-first run.') from e\n\ntry:\n    import seaborn as sns  # optional, for violin plots\nexcept Exception:  # pragma: no cover\n    sns = None\n\ntry:\n    from textblob import TextBlob\nexcept Exception as e:  # pragma: no cover\n    TextBlob = None\n    warnings.warn(\n        'TextBlob not available; sentiment features will be zeros.'\n    )\n\ncuml.set_global_output_type('cupy')\nRANDOM_SEED = 42\nfrom datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a7035",
   "metadata": {},
   "source": [
    "## Utilities and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc() -> None:\n    gc.collect()\n\n\n_STEP_STARTS = {}\n\ndef log_step(msg: str) -> None:\n    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    print(f\"[{ts}] {msg}\", flush=True)\n\ndef log_step_start(name: str) -> None:\n    _STEP_STARTS[name] = time.perf_counter()\n    log_step(f\"START: {name}\")\n\ndef log_step_end(name: str) -> None:\n    start = _STEP_STARTS.pop(name, None)\n    if start is None:\n        log_step(f\"END: {name}\")\n    else:\n        elapsed = time.perf_counter() - start\n        log_step(f\"END: {name} (elapsed {elapsed:.1f}s)\")\n    try:\n        cp.get_default_memory_pool().free_all_blocks()\n    except Exception:\n        pass\n\n\ndef predict_proba_chunks(model, X, chunk_size: int = 50000):\n    n = X.shape[0]\n    out = cp.empty(n, dtype=cp.float32)\n    for start in range(0, n, chunk_size):\n        end = min(start + chunk_size, n)\n        out[start:end] = model.predict_proba(X[start:end])[:, 1]\n        _gc()\n    return out\n\n\ndef _to_numpy(x):\n    if isinstance(x, np.ndarray):\n        return x\n    if hasattr(x, 'get'):\n        return x.get()\n    return np.asarray(x)\n\n\ndef f1_score_np(y_true, y_pred) -> float:\n    y_true = _to_numpy(y_true).astype(int)\n    y_pred = _to_numpy(y_pred).astype(int)\n    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n    precision = tp / (tp + fp + 1e-12)\n    recall = tp / (tp + fn + 1e-12)\n    return float(2 * precision * recall / (precision + recall + 1e-12))\n\n\ndef classification_report_np(y_true, y_pred) -> str:\n    y_true = _to_numpy(y_true).astype(int)\n    y_pred = _to_numpy(y_pred).astype(int)\n    def _prf(label):\n        tp = int(((y_true == label) & (y_pred == label)).sum())\n        fp = int(((y_true != label) & (y_pred == label)).sum())\n        fn = int(((y_true == label) & (y_pred != label)).sum())\n        precision = tp / (tp + fp + 1e-12)\n        recall = tp / (tp + fn + 1e-12)\n        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n        support = int((y_true == label).sum())\n        return precision, recall, f1, support\n    p0, r0, f0, s0 = _prf(0)\n    p1, r1, f1, s1 = _prf(1)\n    acc = float((y_true == y_pred).mean())\n    macro_p = (p0 + p1) / 2\n    macro_r = (r0 + r1) / 2\n    macro_f = (f0 + f1) / 2\n    total = s0 + s1\n    w_p = (p0 * s0 + p1 * s1) / max(total, 1)\n    w_r = (r0 * s0 + r1 * s1) / max(total, 1)\n    w_f = (f0 * s0 + f1 * s1) / max(total, 1)\n    lines = [\n        '              precision    recall  f1-score   support',\n        f'           0       {p0:0.3f}      {r0:0.3f}      {f0:0.3f}      {s0:5d}',\n        f'           1       {p1:0.3f}      {r1:0.3f}      {f1:0.3f}      {s1:5d}',\n        '',\n        f'    accuracy                           {acc:0.3f}      {total:5d}',\n        f'   macro avg       {macro_p:0.3f}      {macro_r:0.3f}      {macro_f:0.3f}      {total:5d}',\n        f'weighted avg       {w_p:0.3f}      {w_r:0.3f}      {w_f:0.3f}      {total:5d}',\n    ]\n    return '\n'.join(lines)\n\n\ndef qq_plot(residuals: np.ndarray, title: str) -> None:\n    plt.figure(figsize=(5, 4))\n    stats.probplot(residuals, dist='norm', plot=plt)\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\n\ndef residual_plot(y_true: np.ndarray, y_prob: np.ndarray,\n                  title: str) -> None:\n    resid = y_true - y_prob\n    plt.figure(figsize=(5, 4))\n    plt.scatter(y_prob, resid, s=8)\n    plt.axhline(0.0, linestyle='--')\n    plt.xlabel('p(y=1)')\n    plt.ylabel('residual')\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\n\ndef violin_by_label(df: pl.DataFrame, label_col: str,\n                    feat_col: str, title: str) -> None:\n    y = df.select(label_col).to_numpy().ravel()\n    x = df.select(feat_col).to_numpy().ravel()\n    if sns is None:\n        plt.figure(figsize=(5, 4))\n        plt.boxplot([x[y == 0], x[y == 1]], labels=['0', '1'])\n        plt.title(title)\n        plt.tight_layout()\n        plt.show()\n        return\n    plt.figure(figsize=(5, 4))\n    sns.violinplot(x=y, y=x)\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e300b63",
   "metadata": {},
   "source": [
    "## Data loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Data loading and processing')\n",
    "def _ttr(text: str) -> float:\n    words = re.findall(r'\\S+', text.lower())\n    return float(len(set(words)) / len(words)) if words else 0.0\n\n\ndef process_text_file(filename: str) -> pl.DataFrame:\n    df = pl.read_csv(os.path.join(filename))\n    assert 'text' in df.columns, 'CSV must contain a text column.'\n    df = df.with_columns(pl.col('text').cast(pl.Utf8))\n\n    df = df.with_columns([\n        pl.col('text').str.len_chars().alias('text_length'),\n        pl.col('text').str.count_matches(r'\\S+').alias('word_count'),\n        pl.col('text').str.count_matches(r'[.!?]+').alias('sentence_count'),\n        pl.col('text').str.count_matches(r'[^\\w\\s]').alias('punct_count'),\n    ])\n    df = df.with_columns([\n        pl.when(pl.col('sentence_count') == 0).then(1)\n            .otherwise(pl.col('sentence_count')).alias('sentence_count'),\n        pl.when(pl.col('text_length') == 0).then(1)\n            .otherwise(pl.col('text_length')).alias('text_length_safe'),\n    ])\n    avg_sentence_expr = pl.col('word_count') / pl.col('sentence_count')\n    punct_expr = pl.col('punct_count') / pl.col('text_length_safe')\n    df = df.with_columns([\n        pl.when(avg_sentence_expr > 100).then(100)\n            .otherwise(avg_sentence_expr).alias('avg_sentence_length'),\n        pl.when(punct_expr > 0.3).then(0.3)\n            .otherwise(punct_expr).alias('punct_ratio'),\n        pl.col('text').map_elements(_ttr, return_dtype=pl.Float64)\n            .alias('ttr'),\n    ])\n    df = df.drop(['text_length_safe'])\n    return df\n",
    "log_step_end('Data loading and processing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44474c",
   "metadata": {},
   "source": [
    "## TF\u2013IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('TF\u2013IDF features')\n",
    "def add_ngram_tfidf(\n    train_texts: cudf.Series,\n    valid_texts: cudf.Series,\n    test_texts: cudf.Series,\n    n: int = 3,\n    max_features: int = 5000,\n) -> Tuple[cpx_sparse.csr_matrix, cpx_sparse.csr_matrix, cpx_sparse.csr_matrix]:\n    vectorizer = TfidfVectorizer(\n        ngram_range=(1, n), max_features=max_features,\n        stop_words='english'\n    )\n    X_train_ng = vectorizer.fit_transform(train_texts)\n    X_valid_ng = vectorizer.transform(valid_texts)\n    X_test_ng = vectorizer.transform(test_texts)\n    return X_train_ng, X_valid_ng, X_test_ng\n",
    "log_step_end('TF\u2013IDF features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518732dd",
   "metadata": {},
   "source": [
    "## Sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9898a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Sentiment features')\n",
    "def add_sentiment_features(df: pl.DataFrame) -> pl.DataFrame:\n    if TextBlob is None:\n        return df.with_columns([\n            pl.lit(0.0).alias('sentiment_polarity'),\n            pl.lit(0.0).alias('sentiment_subjectivity'),\n        ])\n\n    def _pol(x: str) -> float:\n        return float(TextBlob(x).sentiment.polarity)\n\n    def _subj(x: str) -> float:\n        return float(TextBlob(x).sentiment.subjectivity)\n\n    return df.with_columns([\n        pl.col('text').map_elements(_pol, return_dtype=pl.Float64)\n            .alias('sentiment_polarity'),\n        pl.col('text').map_elements(_subj, return_dtype=pl.Float64)\n            .alias('sentiment_subjectivity'),\n    ])\n",
    "log_step_end('Sentiment features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7beafe4",
   "metadata": {},
   "source": [
    "## Load data (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Load data (train/val/test)')\n# Strict file names in the `data/` folder\ntrain = process_text_file('data/train.csv')\nvalidation = process_text_file('data/val.csv')\ntest = process_text_file('data/test.csv')\n\n# Basic schema checks\nfor name, df in [('train', train), ('val', validation), ('test', test)]:\n    assert 'text' in df.columns, f\"{name} missing 'text' column\"\nassert 'label' in train.columns, 'train must have label'\nassert 'label' in validation.columns, 'val must have label'\nassert 'label' not in test.columns, 'test must NOT have label'\nassert 'id' in test.columns, 'test must have id'\n\nprint('Rows: train', train.height, ' val', validation.height,\n      ' test', test.height)\nlog_step_end('Load data (train/val/test)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be7880",
   "metadata": {},
   "source": [
    "## Add sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Add sentiment features')\n",
    "train = add_sentiment_features(train)\nvalidation = add_sentiment_features(validation)\ntest = add_sentiment_features(test)\n",
    "log_step_end('Add sentiment features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a210fd8",
   "metadata": {},
   "source": [
    "## Assemble features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Assemble features')\n",
    "feature_cols: List[str] = [\n    'text_length', 'word_count', 'ttr', 'sentence_count',\n    'avg_sentence_length', 'punct_ratio',\n    'sentiment_polarity', 'sentiment_subjectivity',\n]\n\nX_train_basic = cpx_sparse.csr_matrix(\n    cp.asarray(train.select(feature_cols).to_numpy().astype(np.float32))\n)\nX_valid_basic = cpx_sparse.csr_matrix(\n    cp.asarray(validation.select(feature_cols).to_numpy().astype(np.float32))\n)\nX_test_basic = cpx_sparse.csr_matrix(\n    cp.asarray(test.select(feature_cols).to_numpy().astype(np.float32))\n)\n\ntrain_text = cudf.Series(train['text'].to_list())\nvalid_text = cudf.Series(validation['text'].to_list())\ntest_text = cudf.Series(test['text'].to_list())\n\nX_train_ngram, X_valid_ngram, X_test_ngram = add_ngram_tfidf(\n    train_text, valid_text, test_text, n=3, max_features=5000\n)\n\nX_train = cpx_sparse.hstack([X_train_basic, X_train_ngram]).tocsr()\nX_valid = cpx_sparse.hstack([X_valid_basic, X_valid_ngram]).tocsr()\nX_test = cpx_sparse.hstack([X_test_basic, X_test_ngram]).tocsr()\n\ny_train = cp.asarray(train['label'].to_numpy()).astype(cp.int32)\ny_valid = cp.asarray(validation['label'].to_numpy()).astype(cp.int32)\n\nprint('Shapes:')\nprint('  X_train:', X_train.shape)\nprint('  X_valid:', X_valid.shape)\nprint('  X_test :', X_test.shape)\n_gc()\n",
    "log_step_end('Assemble features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fc490",
   "metadata": {},
   "source": [
    "## Class balance and scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Class balance and scale_pos_weight')\n",
    "counter = Counter(_to_numpy(y_train))\nassert 0 in counter and 1 in counter, 'labels must be binary {0,1}'\nscale_pos_weight = counter[0] / max(counter[1], 1)\nprint('Class counts:', counter)\nprint('scale_pos_weight:', scale_pos_weight)\n",
    "log_step_end('Class balance and scale_pos_weight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76837d",
   "metadata": {},
   "source": [
    "## Train cuML Logistic Regression and MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Train cuML Logistic Regression and MultinomialNB')\nlr = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\nnb = MultinomialNB()\n\nlog_step_start('Fold 1/1 (single split)')\nlog_step_start('LR fit')\nlr.fit(X_train, y_train)\nlog_step_end('LR fit')\nlog_step_start('NB fit')\nnb.fit(X_train, y_train)\nlog_step_end('NB fit')\nlog_step_end('Fold 1/1 (single split)')\nprint('Models trained.')\n_gc()\nlog_step_end('Train cuML Logistic Regression and MultinomialNB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2efc4",
   "metadata": {},
   "source": [
    "## Calibrate with Platt scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Calibrate with Platt scaling')\n",
    "class PlattCalibrator:\n    def __init__(self):\n        self.model = LogisticRegression(max_iter=1000)\n    def fit(self, scores, y):\n        scores = cp.asarray(scores).reshape(-1, 1)\n        self.model.fit(scores, y)\n        return self\n    def predict_proba(self, scores):\n        scores = cp.asarray(scores).reshape(-1, 1)\n        return self.model.predict_proba(scores)[:, 1]\n\ncalibrated_nb = PlattCalibrator().fit(nb.predict_proba(X_valid)[:, 1], y_valid)\ncalibrated_lr = PlattCalibrator().fit(lr.predict_proba(X_valid)[:, 1], y_valid)\nprint('Models calibrated.')\n_gc()\n",
    "log_step_end('Calibrate with Platt scaling')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbe065",
   "metadata": {},
   "source": [
    "## Ensemble and threshold tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Ensemble and threshold tuning')\n",
    "scores_nb = predict_proba_chunks(nb, X_valid)\nscores_lr = predict_proba_chunks(lr, X_valid)\nval_pred_proba_nb = _to_numpy(calibrated_nb.predict_proba(scores_nb))\nval_pred_proba_lr = _to_numpy(calibrated_lr.predict_proba(scores_lr))\nval_pred_proba_ensemble = (\n    0.6 * val_pred_proba_nb + 0.4 * val_pred_proba_lr\n)\n\ny_valid_np = _to_numpy(y_valid)\nthresholds = np.arange(0.1, 0.9, 0.01)\nbest_threshold = 0.5\nbest_f1 = -1.0\n\nfor thr in thresholds:\n    val_pred_thr = (val_pred_proba_ensemble >= thr).astype(int)\n    f1 = f1_score_np(y_valid_np, val_pred_thr)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_threshold = float(thr)\n\nprint(\n    f'Best threshold: {best_threshold:.2f} with F1: {best_f1:.4f}'\n)\n_gc()\n",
    "log_step_end('Ensemble and threshold tuning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91516453",
   "metadata": {},
   "source": [
    "## Validation report and diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Validation report and diagnostics')\n",
    "val_pred_final = (val_pred_proba_ensemble >= best_threshold).astype(int)\nprint(classification_report_np(y_valid_np, val_pred_final))\n\nresidual_plot(y_valid_np, val_pred_proba_ensemble,\n              'Residuals: validation ensemble')\nqq_plot(y_valid_np - val_pred_proba_ensemble,\n        'QQ plot: residuals (validation)')\ntry:\n    violin_by_label(train, 'label', 'text_length',\n                    'Text length by label (train)')\nexcept Exception as e:\n    warnings.warn(f'Violin plot skipped: {e}')\n",
    "log_step_end('Validation report and diagnostics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3f803",
   "metadata": {},
   "source": [
    "## Predict on test and write submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Predict on test and write submission')\n",
    "scores_nb = predict_proba_chunks(nb, X_test)\nscores_lr = predict_proba_chunks(lr, X_test)\np_nb_te = _to_numpy(calibrated_nb.predict_proba(scores_nb))\np_lr_te = _to_numpy(calibrated_lr.predict_proba(scores_lr))\np_ens_te = 0.6 * p_nb_te + 0.4 * p_lr_te\nyhat_te = (p_ens_te >= best_threshold).astype(int)\n\nsubmission = pl.DataFrame({'id': test['id'], 'label': yhat_te})\noutputs_dir = 'outputs'\nos.makedirs(outputs_dir, exist_ok=True)\nsubmission_path = os.path.join(outputs_dir, 'submission_hvsm_prod_a.csv')\nsubmission.write_csv(submission_path)\nprint('Saved', submission_path, 'with', submission.height, 'rows')\n_gc()\n",
    "log_step_end('Predict on test and write submission')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}