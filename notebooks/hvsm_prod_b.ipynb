{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a407fa8b",
   "metadata": {},
   "source": [
    "# HVSM: GPU TF-IDF + cuML LR/NB (tuning)\n\nThis notebook is a GPU-first rewrite using Polars + cuML, with random-search tuning on the validation split and expanded diagnostics.\n\n**Inputs (strict):** `data/train.csv`, `data/val.csv`, `data/test.csv` in the `data/` folder. `data/test.csv` must have `id` and no `label`. The notebook creates `outputs/submission_hvsm_prod_b.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532c8b5",
   "metadata": {},
   "source": [
    "## Imports and guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\nimport os, re, warnings\nimport gc\nimport time\nfrom typing import List, Tuple, Dict, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom collections import Counter\nimport numpy as np\nimport polars as pl\nfrom tqdm import tqdm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ntry:\n    import cupy as cp\n    import cudf\n    import cupyx.scipy.sparse as cpx_sparse\n    import cuml\n    from cuml.feature_extraction.text import TfidfVectorizer\n    from cuml.linear_model import LogisticRegression\n    from cuml.naive_bayes import MultinomialNB\nexcept Exception as e:\n    raise RuntimeError('cuML + CUDA (cupy/cudf) required for GPU-first run.') from e\ntry:\n    import seaborn as sns\nexcept Exception:\n    sns = None\ntry:\n    from textblob import TextBlob\nexcept Exception:\n    TextBlob = None\n    warnings.warn('TextBlob missing; sentiment features set to zeros.')\nnp.set_printoptions(linewidth=79)\ncuml.set_global_output_type('cupy')\nRANDOM_SEED = 42\nrng = np.random.default_rng(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2dab59",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c19b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    tfidf_max_features: int = 50000\n",
    "    tfidf_ngram_max: int = 3\n",
    "    use_char_ngrams: bool = False\n",
    "    min_df: int = 2\n",
    "    kfolds: int = 5\n",
    "    lr_iter: int = 25\n",
    "    nb_iter: int = 15\n",
    "    plot_level: str = 'full'\n",
    "CFG = Config()\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e303a",
   "metadata": {},
   "source": [
    "## Plotting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c919db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc() -> None:\n    gc.collect()\n\n\n_STEP_STARTS = {}\n\ndef log_step(msg: str) -> None:\n    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    print(f\"[{ts}] {msg}\", flush=True)\n\ndef log_step_start(name: str) -> None:\n    _STEP_STARTS[name] = time.perf_counter()\n    log_step(f\"START: {name}\")\n\ndef log_step_end(name: str) -> None:\n    start = _STEP_STARTS.pop(name, None)\n    if start is None:\n        log_step(f\"END: {name}\")\n    else:\n        elapsed = time.perf_counter() - start\n        log_step(f\"END: {name} (elapsed {elapsed:.1f}s)\")\n    try:\n        cp.get_default_memory_pool().free_all_blocks()\n    except Exception:\n        pass\n\n\ndef predict_proba_chunks(model, X, chunk_size: int = 50000):\n    n = X.shape[0]\n    out = cp.empty(n, dtype=cp.float32)\n    for start in range(0, n, chunk_size):\n        end = min(start + chunk_size, n)\n        out[start:end] = model.predict_proba(X[start:end])[:, 1]\n        _gc()\n    return out\n\n\ndef _to_numpy(x):\n    if isinstance(x, np.ndarray):\n        return x\n    if hasattr(x, 'get'):\n        return x.get()\n    return np.asarray(x)\n\n\ndef f1_score_np(y_true, y_pred) -> float:\n    y_true = _to_numpy(y_true).astype(int)\n    y_pred = _to_numpy(y_pred).astype(int)\n    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n    precision = tp / (tp + fp + 1e-12)\n    recall = tp / (tp + fn + 1e-12)\n    return float(2 * precision * recall / (precision + recall + 1e-12))\n\n\ndef confusion_matrix_np(y_true, y_pred) -> np.ndarray:\n    y_true = _to_numpy(y_true).astype(int)\n    y_pred = _to_numpy(y_pred).astype(int)\n    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n    return np.array([[tn, fp], [fn, tp]])\n\n\ndef classification_report_np(y_true, y_pred) -> str:\n    y_true = _to_numpy(y_true).astype(int)\n    y_pred = _to_numpy(y_pred).astype(int)\n    def _prf(label):\n        tp = int(((y_true == label) & (y_pred == label)).sum())\n        fp = int(((y_true != label) & (y_pred == label)).sum())\n        fn = int(((y_true == label) & (y_pred != label)).sum())\n        precision = tp / (tp + fp + 1e-12)\n        recall = tp / (tp + fn + 1e-12)\n        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n        support = int((y_true == label).sum())\n        return precision, recall, f1, support\n    p0, r0, f0, s0 = _prf(0)\n    p1, r1, f1, s1 = _prf(1)\n    acc = float((y_true == y_pred).mean())\n    macro_p = (p0 + p1) / 2\n    macro_r = (r0 + r1) / 2\n    macro_f = (f0 + f1) / 2\n    total = s0 + s1\n    w_p = (p0 * s0 + p1 * s1) / max(total, 1)\n    w_r = (r0 * s0 + r1 * s1) / max(total, 1)\n    w_f = (f0 * s0 + f1 * s1) / max(total, 1)\n    lines = [\n        '              precision    recall  f1-score   support',\n        f'           0       {p0:0.3f}      {r0:0.3f}      {f0:0.3f}      {s0:5d}',\n        f'           1       {p1:0.3f}      {r1:0.3f}      {f1:0.3f}      {s1:5d}',\n        '',\n        f'    accuracy                           {acc:0.3f}      {total:5d}',\n        f'   macro avg       {macro_p:0.3f}      {macro_r:0.3f}      {macro_f:0.3f}      {total:5d}',\n        f'weighted avg       {w_p:0.3f}      {w_r:0.3f}      {w_f:0.3f}      {total:5d}',\n    ]\n    return '\n'.join(lines)\n\n\ndef roc_curve_np(y_true, y_score):\n    y_true = _to_numpy(y_true).astype(int)\n    y_score = _to_numpy(y_score).astype(float)\n    order = np.argsort(-y_score)\n    y_true = y_true[order]\n    y_score = y_score[order]\n    tps = np.cumsum(y_true == 1)\n    fps = np.cumsum(y_true == 0)\n    tpr = tps / max(tps[-1], 1)\n    fpr = fps / max(fps[-1], 1)\n    thresholds = y_score\n    return fpr, tpr, thresholds\n\n\ndef precision_recall_curve_np(y_true, y_score):\n    y_true = _to_numpy(y_true).astype(int)\n    y_score = _to_numpy(y_score).astype(float)\n    order = np.argsort(-y_score)\n    y_true = y_true[order]\n    y_score = y_score[order]\n    tps = np.cumsum(y_true == 1)\n    fps = np.cumsum(y_true == 0)\n    precision = tps / np.maximum(tps + fps, 1)\n    recall = tps / max(tps[-1], 1)\n    return precision, recall, y_score\n\n\ndef roc_auc_score_np(y_true, y_score) -> float:\n    fpr, tpr, _ = roc_curve_np(y_true, y_score)\n    return float(np.trapz(tpr, fpr))\n\n\ndef _tight() -> None:\n    plt.tight_layout()\n\n\ndef qq_plot(residuals: np.ndarray, title: str) -> None:\n    plt.figure(figsize=(5, 4)); stats.probplot(residuals, dist='norm',\n                                               plot=plt)\n    plt.title(title); _tight(); plt.show()\n\n\ndef residual_plot(y_true: np.ndarray, y_prob: np.ndarray,\n                  title: str) -> None:\n    resid = y_true - y_prob\n    plt.figure(figsize=(5, 4)); plt.scatter(y_prob, resid, s=8)\n    plt.axhline(0.0, linestyle='--'); plt.xlabel('p(y=1)');\n    plt.ylabel('residual'); plt.title(title); _tight(); plt.show()\n\n\ndef violin_by_label(df: pl.DataFrame, label_col: str, feat_col: str,\n                    title: str) -> None:\n    y = df.select(label_col).to_numpy().ravel()\n    x = df.select(feat_col).to_numpy().ravel()\n    if sns is None:\n        plt.figure(figsize=(5, 4))\n        plt.boxplot([x[y == 0], x[y == 1]], labels=['0', '1'])\n        plt.title(title); _tight(); plt.show(); return\n    plt.figure(figsize=(5, 4))\n    sns.violinplot(x=y, y=x)\n    plt.title(title); _tight(); plt.show()\n\n\ndef plot_roc_pr(y_true: np.ndarray, y_prob: np.ndarray, title: str)->None:\n    fpr, tpr, _ = roc_curve_np(y_true, y_prob)\n    prec, rec, _ = precision_recall_curve_np(y_true, y_prob)\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n    ax[0].plot(fpr, tpr)\n    ax[0].set_title(f'ROC AUC={roc_auc_score_np(y_true, y_prob):.3f}')\n    ax[0].set_xlabel('FPR'); ax[0].set_ylabel('TPR')\n    ax[1].plot(rec, prec); ax[1].set_title('Precision-Recall')\n    ax[1].set_xlabel('Recall'); ax[1].set_ylabel('Precision')\n    _tight(); plt.show()\n\n\ndef plot_confusion(y_true: np.ndarray, y_hat: np.ndarray, title: str)->None:\n    cm = confusion_matrix_np(y_true, y_hat)\n    plt.figure(figsize=(4, 3)); plt.imshow(cm, cmap='Blues')\n    plt.title(title); plt.colorbar()\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, int(cm[i, j]), ha='center', va='center')\n    plt.xlabel('Pred'); plt.ylabel('True'); _tight(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f43999",
   "metadata": {},
   "source": [
    "## Processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73aa79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Processing and feature engineering')\n",
    "def _ttr(text: str) -> float:\n    words = re.findall(r'\\S+', text.lower())\n    return float(len(set(words)) / len(words)) if words else 0.0\n\ndef _sentiment(df: pl.DataFrame) -> pl.DataFrame:\n    if TextBlob is None:\n        return df.with_columns([\n            pl.lit(0.0).alias('sentiment_polarity'),\n            pl.lit(0.0).alias('sentiment_subjectivity'),\n        ])\n    def _polarity(x: str) -> float:\n        return float(TextBlob(x).sentiment.polarity)\n    def _subjectivity(x: str) -> float:\n        return float(TextBlob(x).sentiment.subjectivity)\n    return df.with_columns([\n        pl.col('text').map_elements(_polarity, return_dtype=pl.Float64)\n            .alias('sentiment_polarity'),\n        pl.col('text').map_elements(_subjectivity, return_dtype=pl.Float64)\n            .alias('sentiment_subjectivity'),\n    ])\n\ndef process_text_file(filename: str) -> pl.DataFrame:\n    df = pl.read_csv(os.path.join(filename))\n    assert 'text' in df.columns, 'CSV must contain a text column.'\n    df = df.with_columns(pl.col('text').cast(pl.Utf8))\n    df = df.with_columns([\n        pl.col('text').str.len_chars().alias('text_length'),\n        pl.col('text').str.count_matches(r'\\S+').alias('word_count'),\n        pl.col('text').str.count_matches(r'[.!?]+').alias('sentence_count'),\n        pl.col('text').str.count_matches(r'[^\\w\\s]').alias('punct_count'),\n        pl.col('text').str.count_matches(r'\\d').alias('digit_count'),\n        pl.col('text').str.count_matches(r'[A-Z]').alias('upper_count'),\n        pl.col('text').str.count_matches(r'!').alias('bangs'),\n        pl.col('text').str.count_matches(r'\\?').alias('questions'),\n    ])\n    df = df.with_columns([\n        pl.when(pl.col('sentence_count') == 0).then(1)\n            .otherwise(pl.col('sentence_count')).alias('sentence_count'),\n        pl.when(pl.col('text_length') == 0).then(1)\n            .otherwise(pl.col('text_length')).alias('text_length_safe'),\n    ])\n    avg_sentence_expr = pl.col('word_count') / pl.col('sentence_count')\n    punct_expr = pl.col('punct_count') / pl.col('text_length_safe')\n    df = df.with_columns([\n        pl.when(avg_sentence_expr > 100).then(100)\n            .otherwise(avg_sentence_expr).alias('avg_sentence_length'),\n        pl.when(punct_expr > 0.3).then(0.3)\n            .otherwise(punct_expr).alias('punct_ratio'),\n        (pl.col('digit_count') / pl.col('text_length_safe')).alias('digit_ratio'),\n        (pl.col('upper_count') / pl.col('text_length_safe')).alias('upper_ratio'),\n        pl.col('text').map_elements(_ttr, return_dtype=pl.Float64).alias('ttr'),\n    ])\n    df = df.drop(['digit_count', 'upper_count', 'text_length_safe'])\n    return df\n",
    "log_step_end('Processing and feature engineering')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f0aa5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Load data')\ntrain = process_text_file('data/train.csv')\nval = process_text_file('data/val.csv')\ntest = process_text_file('data/test.csv')\nassert 'label' in train.columns and 'label' in val.columns\nassert 'label' not in test.columns\nassert 'id' in test.columns\nprint('Rows:', train.height, val.height, test.height)\nlog_step_end('Load data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affdb891",
   "metadata": {},
   "source": [
    "## Sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba07a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Sentiment features')\n",
    "train = _sentiment(train); val = _sentiment(val); test = _sentiment(test)\n",
    "log_step_end('Sentiment features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8e010",
   "metadata": {},
   "source": [
    "## Numeric and TF\u2013IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Numeric and TF\u2013IDF features')\n",
    "feature_cols: List[str] = [\n    'text_length','word_count','ttr','sentence_count','avg_sentence_length',\n    'punct_ratio','sentiment_polarity','sentiment_subjectivity',\n    'digit_ratio','upper_ratio','bangs','questions'\n]\nXtr_basic = cpx_sparse.csr_matrix(\n    cp.asarray(train.select(feature_cols).to_numpy().astype(np.float32))\n)\nXva_basic = cpx_sparse.csr_matrix(\n    cp.asarray(val.select(feature_cols).to_numpy().astype(np.float32))\n)\nXte_basic = cpx_sparse.csr_matrix(\n    cp.asarray(test.select(feature_cols).to_numpy().astype(np.float32))\n)\ntrain_text = cudf.Series(train['text'].to_list())\nval_text = cudf.Series(val['text'].to_list())\ntest_text = cudf.Series(test['text'].to_list())\nvec_word = TfidfVectorizer(ngram_range=(1, 3), max_features=50000,\n                           min_df=2, stop_words='english')\nXtr_w = vec_word.fit_transform(train_text)\nXva_w = vec_word.transform(val_text)\nXte_w = vec_word.transform(test_text)\nX_train = cpx_sparse.hstack([Xtr_basic, Xtr_w]).tocsr()\nX_val = cpx_sparse.hstack([Xva_basic, Xva_w]).tocsr()\nX_test = cpx_sparse.hstack([Xte_basic, Xte_w]).tocsr()\ny_train = cp.asarray(train['label'].to_numpy()).astype(cp.int32)\ny_val = cp.asarray(val['label'].to_numpy()).astype(cp.int32)\nprint('Shapes:', X_train.shape, X_val.shape, X_test.shape)\n_gc()\n",
    "log_step_end('Numeric and TF\u2013IDF features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d868d0f",
   "metadata": {},
   "source": [
    "## Random-search tuning (GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2901045",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Random-search tuning (GPU)')\ndef _lr_space():\n    return {'C':[0.5,1.0,2.0,4.0]}\n\ndef _nb_space():\n    return {'alpha':[0.01,0.05,0.1,0.5,1.0]}\n\ndef _sample(space):\n    return {k: rng.choice(v) for k, v in space.items()}\n\ndef _f1_from_proba(y_true, y_prob, thr: float = 0.5) -> float:\n    y_hat = (y_prob >= thr).astype(cp.int32)\n    return f1_score_np(y_true, y_hat)\n\ndef tune_model(name: str, build_fn, space, X_tr, y_tr, X_va, y_va,\n               n_iter: int):\n    best_params, best_f1 = None, -1.0\n    log_step(f\"{name} fold 1/1 (single split)\")\n    for i in range(n_iter):\n        iter_name = f\"{name} iter {i+1}/{n_iter}\"\n        log_step_start(iter_name)\n        params = _sample(space)\n        model = build_fn(**params)\n        model.fit(X_tr, y_tr)\n        p = model.predict_proba(X_va)[:, 1]\n        f1 = float(_f1_from_proba(y_va, p))\n        if f1 > best_f1:\n            best_f1, best_params = float(f1), params\n        log_step(f\"{iter_name} f1={f1:.4f} best={best_f1:.4f}\")\n        log_step_end(iter_name)\n        del model\n        _gc()\n    print(f'Best {name}: {best_params} | F1={best_f1:.4f}')\n    return best_params\n\nlr_params = tune_model(\n    'LR', lambda **p: LogisticRegression(max_iter=2000, **p),\n    _lr_space(), X_train, y_train, X_val, y_val, CFG.lr_iter,\n)\nnb_params = tune_model(\n    'NB', lambda **p: MultinomialNB(**p),\n    _nb_space(), X_train, y_train, X_val, y_val, CFG.nb_iter,\n)\nlog_step_end('Random-search tuning (GPU)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb30f46",
   "metadata": {},
   "source": [
    "## Fit final models and calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Fit final models and calibrate')\nclass PlattCalibrator:\n    def __init__(self):\n        self.model = LogisticRegression(max_iter=1000)\n    def fit(self, scores, y):\n        scores = cp.asarray(scores).reshape(-1, 1)\n        self.model.fit(scores, y)\n        return self\n    def predict_proba(self, scores):\n        scores = cp.asarray(scores).reshape(-1, 1)\n        return self.model.predict_proba(scores)[:, 1]\n\nX_trval = cpx_sparse.vstack([X_train, X_val]).tocsr()\ny_trval = cp.concatenate([y_train, y_val])\nlr_tuned = LogisticRegression(max_iter=2000, **lr_params)\nnb_tuned = MultinomialNB(**nb_params)\nlog_step_start('Fold 1/1 (single split)')\nlog_step_start('LR fit')\nlr_tuned.fit(X_trval, y_trval)\nlog_step_end('LR fit')\nlog_step_start('NB fit')\nnb_tuned.fit(X_trval, y_trval)\nlog_step_end('NB fit')\nlog_step_end('Fold 1/1 (single split)')\n\ncal_lr = PlattCalibrator().fit(lr_tuned.predict_proba(X_val)[:, 1], y_val)\ncal_nb = PlattCalibrator().fit(nb_tuned.predict_proba(X_val)[:, 1], y_val)\n_gc()\nlog_step_end('Fit final models and calibrate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe7e8f",
   "metadata": {},
   "source": [
    "## Ensemble and threshold tuning on val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2176f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Ensemble and threshold tuning on val')\n",
    "scores_lr = predict_proba_chunks(lr_tuned, X_val)\nscores_nb = predict_proba_chunks(nb_tuned, X_val)\np_lr = _to_numpy(cal_lr.predict_proba(scores_lr))\np_nb = _to_numpy(cal_nb.predict_proba(scores_nb))\nbest_w, best_f1, best_thr = 0.5, -1.0, 0.5\nfor w in np.linspace(0.0, 1.0, 21):\n    p = w * p_nb + (1.0 - w) * p_lr\n    for thr in np.arange(0.1, 0.91, 0.01):\n        yhat = (p >= thr).astype(int)\n        f1 = f1_score_np(y_val, yhat)\n        if f1 > best_f1:\n            best_w, best_f1, best_thr = float(w), float(f1), float(thr)\nprint(f'Ensemble w={best_w:.2f} thr={best_thr:.2f} F1={best_f1:.4f}')\n_gc()\n",
    "log_step_end('Ensemble and threshold tuning on val')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4aacd3",
   "metadata": {},
   "source": [
    "## Validation diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Validation diagnostics')\n",
    "p_ens = best_w * p_nb + (1.0 - best_w) * p_lr\nyhat_val = (p_ens >= best_thr).astype(int)\nprint(classification_report_np(y_val, yhat_val))\nresidual_plot(_to_numpy(y_val), p_ens, 'Residuals: validation ensemble')\nqq_plot(_to_numpy(y_val) - p_ens, 'QQ: residuals (validation)')\nplot_roc_pr(_to_numpy(y_val), p_ens, 'Validation ROC/PR (ensemble)')\nplot_confusion(_to_numpy(y_val), yhat_val, 'Confusion (validation)')\nviolin_by_label(train, 'label', 'text_length', 'Text length by label')\n",
    "log_step_end('Validation diagnostics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e00c4f",
   "metadata": {},
   "source": [
    "## Coefficient snapshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c26e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Coefficient snapshots')\n",
    "try:\n    if hasattr(lr_tuned, 'coef_'):\n        coef = _to_numpy(lr_tuned.coef_).ravel()\n        idx = np.argsort(np.abs(coef))[::-1][:25]\n        print('Top 25 |coef| for LR: indices and values:')\n        for i in idx:\n            print(i, float(coef[i]))\nexcept Exception as e:\n    warnings.warn(f'LR coef summary failed: {e}')\n",
    "log_step_end('Coefficient snapshots')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d4521",
   "metadata": {},
   "source": [
    "## Predict on test and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Predict on test and save submission')\n",
    "scores_lr = predict_proba_chunks(lr_tuned, X_test)\nscores_nb = predict_proba_chunks(nb_tuned, X_test)\np_lr_te = _to_numpy(cal_lr.predict_proba(scores_lr))\np_nb_te = _to_numpy(cal_nb.predict_proba(scores_nb))\np_ens_te = best_w * p_nb_te + (1.0 - best_w) * p_lr_te\nyhat_te = (p_ens_te >= best_thr).astype(int)\nsubmission = pl.DataFrame({'id': test['id'], 'label': yhat_te})\noutputs_dir = 'outputs'\nos.makedirs(outputs_dir, exist_ok=True)\nsubmission_path = os.path.join(outputs_dir, 'submission_hvsm_prod_b.csv')\nsubmission.write_csv(submission_path)\nprint('Saved', submission_path, 'with', submission.height, 'rows')\n_gc()\n",
    "log_step_end('Predict on test and save submission')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f3b67",
   "metadata": {},
   "source": [
    "## Final checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Final checks')\n",
    "assert set(submission['label'].to_list()).issubset({0, 1})\nprint('Done. All checks passed.')\n",
    "log_step_end('Final checks')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}