{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b66b50e1fd04e8d92d448c5b2f8c163",
   "metadata": {},
   "source": [
    "# HVSM Notebook: hvsm_prod_b.ipynb\n",
    "\n",
    "- Runs with: slurm_scripts/hvsm_job_b.sh\n",
    "- Purpose: GPU models (cuML) with CPU TF-IDF and tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7e4854f954cfb88d8a89e893dd54c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (papermill)\n",
    "DATA_DIR = \"data\"\n",
    "TRAIN_CSV = \"data/train.csv\"\n",
    "VAL_CSV = \"data/val.csv\"\n",
    "TEST_CSV = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407fa8b",
   "metadata": {},
   "source": [
    "# HVSM: GPU TF-IDF + cuML LR/NB (tuning)\n\nThis notebook is a GPU-first rewrite using Polars + cuML, with random-search tuning on the validation split and expanded diagnostics.\n\n**Inputs (strict):** `data/train.csv`, `data/val.csv`, `data/test.csv` in the `data/` folder. `data/test.csv` must have `id` and no `label`. The notebook creates `outputs/submission_hvsm_prod_b.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532c8b5",
   "metadata": {},
   "source": [
    "## Imports and guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, warnings, json, hashlib\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer as SkTfidfVectorizer,\n",
    ")\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cupyx.scipy.sparse as cpx_sparse\n",
    "    import cuml\n",
    "    from cuml.linear_model import LogisticRegression\n",
    "    from cuml.naive_bayes import MultinomialNB\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"cuML + CUDA (cupy/cudf) required for GPU-first run.\"\n",
    "    ) from e\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except Exception:\n",
    "    TextBlob = None\n",
    "    warnings.warn(\"TextBlob missing; sentiment features set to zeros.\")\n",
    "np.set_printoptions(linewidth=79)\n",
    "cuml.set_global_output_type(\"cupy\")\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2dab59",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c19b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    tfidf_max_features: int = 5000\n",
    "    tfidf_chunk_size: int = 5000\n",
    "    proba_chunk_size: int = 20000\n",
    "    tfidf_ngram_max: int = 7\n",
    "    use_char_ngrams: bool = False\n",
    "    min_df: int = 2\n",
    "    kfolds: int = 3\n",
    "    lr_iter: int = 8\n",
    "    nb_iter: int = 6\n",
    "    plot_level: str = \"full\"\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e303a",
   "metadata": {},
   "source": [
    "## Plotting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c919db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc() -> None:\n",
    "    gc.collect()\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        try:\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "_STEP_STARTS = {}\n",
    "\n",
    "\n",
    "def log_step(msg: str) -> None:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{ts}] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "def log_step_start(name: str) -> None:\n",
    "    _STEP_STARTS[name] = time.perf_counter()\n",
    "    log_step(f\"START: {name}\")\n",
    "\n",
    "\n",
    "def log_step_end(name: str) -> None:\n",
    "    start = _STEP_STARTS.pop(name, None)\n",
    "    if start is None:\n",
    "        log_step(f\"END: {name}\")\n",
    "    else:\n",
    "        elapsed = time.perf_counter() - start\n",
    "        log_step(f\"END: {name} (elapsed {elapsed:.1f}s)\")\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        try:\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _cp_asarray_with_backoff(\n",
    "    arr,\n",
    "    *,\n",
    "    max_attempts: int = 16,\n",
    "    base_sleep: float = 0.5,\n",
    "    max_sleep: float = 60.0,\n",
    "):\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return cp.asarray(arr)\n",
    "        except Exception as exc:\n",
    "            msg = str(exc).lower()\n",
    "            if \"devicesunavailable\" in msg or \"busy or unavailable\" in msg:\n",
    "                if attempt >= max_attempts - 1:\n",
    "                    raise\n",
    "                sleep = min(max_sleep, base_sleep * (2 ** attempt))\n",
    "                jitter = random.uniform(0, sleep * 0.3)\n",
    "                wait = sleep + jitter\n",
    "                log_step(\n",
    "                    \"CUDA busy/unavailable; retrying in \"\n",
    "                    f\"{wait:.1f}s (attempt {attempt + 1}/{max_attempts})\"\n",
    "                )\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "def predict_proba_chunks(model, X, chunk_size: int = 50000):\n",
    "    n = X.shape[0]\n",
    "    out = cp.empty(n, dtype=cp.float32)\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        out[start:end] = model.predict_proba(X[start:end])[:, 1]\n",
    "        _gc()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _to_numpy(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    if hasattr(x, \"get\"):\n",
    "        return x.get()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def f1_score_np(y_true, y_pred) -> float:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    return float(2 * precision * recall / (precision + recall + 1e-12))\n",
    "\n",
    "\n",
    "def confusion_matrix_np(y_true, y_pred) -> np.ndarray:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "\n",
    "def classification_report_np(y_true, y_pred) -> str:\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_pred = _to_numpy(y_pred).astype(int)\n",
    "\n",
    "    def _prf(label):\n",
    "        tp = int(((y_true == label) & (y_pred == label)).sum())\n",
    "        fp = int(((y_true != label) & (y_pred == label)).sum())\n",
    "        fn = int(((y_true == label) & (y_pred != label)).sum())\n",
    "        precision = tp / (tp + fp + 1e-12)\n",
    "        recall = tp / (tp + fn + 1e-12)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "        support = int((y_true == label).sum())\n",
    "        return precision, recall, f1, support\n",
    "\n",
    "    p0, r0, f0, s0 = _prf(0)\n",
    "    p1, r1, f1, s1 = _prf(1)\n",
    "    acc = float((y_true == y_pred).mean())\n",
    "    macro_p = (p0 + p1) / 2\n",
    "    macro_r = (r0 + r1) / 2\n",
    "    macro_f = (f0 + f1) / 2\n",
    "    total = s0 + s1\n",
    "    w_p = (p0 * s0 + p1 * s1) / max(total, 1)\n",
    "    w_r = (r0 * s0 + r1 * s1) / max(total, 1)\n",
    "    w_f = (f0 * s0 + f1 * s1) / max(total, 1)\n",
    "    lines = [\n",
    "        \"              precision    recall  f1-score   support\",\n",
    "        f\"           0       {p0:0.3f}      {r0:0.3f}      {f0:0.3f}      {s0:5d}\",\n",
    "        f\"           1       {p1:0.3f}      {r1:0.3f}      {f1:0.3f}      {s1:5d}\",\n",
    "        \"\",\n",
    "        f\"    accuracy                           {acc:0.3f}      {total:5d}\",\n",
    "        f\"   macro avg       {macro_p:0.3f}      {macro_r:0.3f}      {macro_f:0.3f}      {total:5d}\",\n",
    "        f\"weighted avg       {w_p:0.3f}      {w_r:0.3f}      {w_f:0.3f}      {total:5d}\",\n",
    "    ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def roc_curve_np(y_true, y_score):\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_score = _to_numpy(y_score).astype(float)\n",
    "    order = np.argsort(-y_score)\n",
    "    y_true = y_true[order]\n",
    "    y_score = y_score[order]\n",
    "    tps = np.cumsum(y_true == 1)\n",
    "    fps = np.cumsum(y_true == 0)\n",
    "    tpr = tps / max(tps[-1], 1)\n",
    "    fpr = fps / max(fps[-1], 1)\n",
    "    thresholds = y_score\n",
    "    return fpr, tpr, thresholds\n",
    "\n",
    "\n",
    "def precision_recall_curve_np(y_true, y_score):\n",
    "    y_true = _to_numpy(y_true).astype(int)\n",
    "    y_score = _to_numpy(y_score).astype(float)\n",
    "    order = np.argsort(-y_score)\n",
    "    y_true = y_true[order]\n",
    "    y_score = y_score[order]\n",
    "    tps = np.cumsum(y_true == 1)\n",
    "    fps = np.cumsum(y_true == 0)\n",
    "    precision = tps / np.maximum(tps + fps, 1)\n",
    "    recall = tps / max(tps[-1], 1)\n",
    "    return precision, recall, y_score\n",
    "\n",
    "\n",
    "def roc_auc_score_np(y_true, y_score) -> float:\n",
    "    fpr, tpr, _ = roc_curve_np(y_true, y_score)\n",
    "    return float(np.trapz(tpr, fpr))\n",
    "\n",
    "\n",
    "def _tight() -> None:\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def qq_plot(residuals: np.ndarray, title: str) -> None:\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(title)\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def residual_plot(y_true: np.ndarray, y_prob: np.ndarray, title: str) -> None:\n",
    "    resid = y_true - y_prob\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(y_prob, resid, s=8)\n",
    "    plt.axhline(0.0, linestyle=\"--\")\n",
    "    plt.xlabel(\"p(y=1)\")\n",
    "    plt.ylabel(\"residual\")\n",
    "    plt.title(title)\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def violin_by_label(\n",
    "    df: pl.DataFrame, label_col: str, feat_col: str, title: str\n",
    ") -> None:\n",
    "    y = df.select(label_col).to_numpy().ravel()\n",
    "    x = df.select(feat_col).to_numpy().ravel()\n",
    "    if sns is None:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.boxplot([x[y == 0], x[y == 1]], labels=[\"0\", \"1\"])\n",
    "        plt.title(title)\n",
    "        _tight()\n",
    "        plt.show()\n",
    "        return\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.violinplot(x=y, y=x)\n",
    "    plt.title(title)\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_pr(y_true: np.ndarray, y_prob: np.ndarray, title: str) -> None:\n",
    "    fpr, tpr, _ = roc_curve_np(y_true, y_prob)\n",
    "    prec, rec, _ = precision_recall_curve_np(y_true, y_prob)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax[0].plot(fpr, tpr)\n",
    "    ax[0].set_title(f\"ROC AUC={roc_auc_score_np(y_true, y_prob):.3f}\")\n",
    "    ax[0].set_xlabel(\"FPR\")\n",
    "    ax[0].set_ylabel(\"TPR\")\n",
    "    ax[1].plot(rec, prec)\n",
    "    ax[1].set_title(\"Precision-Recall\")\n",
    "    ax[1].set_xlabel(\"Recall\")\n",
    "    ax[1].set_ylabel(\"Precision\")\n",
    "    _tight()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion(y_true: np.ndarray, y_hat: np.ndarray, title: str) -> None:\n",
    "    cm = confusion_matrix_np(y_true, y_hat)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "    plt.xlabel(\"Pred\")\n",
    "    plt.ylabel(\"True\")\n",
    "    _tight()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f43999",
   "metadata": {},
   "source": [
    "## Processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73aa79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Processing and feature engineering\")\n",
    "\n",
    "\n",
    "def _ttr(text: str) -> float:\n",
    "    words = re.findall(r\"\\S+\", text.lower())\n",
    "    return float(len(set(words)) / len(words)) if words else 0.0\n",
    "\n",
    "\n",
    "def _sentiment(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if TextBlob is None:\n",
    "        return df.with_columns(\n",
    "            [\n",
    "                pl.lit(0.0).alias(\"sentiment_polarity\"),\n",
    "                pl.lit(0.0).alias(\"sentiment_subjectivity\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _polarity(x: str) -> float:\n",
    "        return float(TextBlob(x).sentiment.polarity)\n",
    "\n",
    "    def _subjectivity(x: str) -> float:\n",
    "        return float(TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    return df.with_columns(\n",
    "        [\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_polarity, return_dtype=pl.Float64)\n",
    "            .alias(\"sentiment_polarity\"),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_subjectivity, return_dtype=pl.Float64)\n",
    "            .alias(\"sentiment_subjectivity\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def process_text_file(filename: str) -> pl.DataFrame:\n",
    "    df = pl.read_csv(os.path.join(filename))\n",
    "    assert \"text\" in df.columns, \"CSV must contain a text column.\"\n",
    "    df = df.with_columns(pl.col(\"text\").cast(pl.Utf8))\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"text\").str.len_chars().alias(\"text_length\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"\\S+\").alias(\"word_count\"),\n",
    "            pl.col(\"text\")\n",
    "            .str.count_matches(r\"[.!?]+\")\n",
    "            .alias(\"sentence_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"[^\\w\\s]\").alias(\"punct_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"\\d\").alias(\"digit_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"[A-Z]\").alias(\"upper_count\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"!\").alias(\"bangs\"),\n",
    "            pl.col(\"text\").str.count_matches(r\"\\?\").alias(\"questions\"),\n",
    "        ]\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"sentence_count\") == 0)\n",
    "            .then(1)\n",
    "            .otherwise(pl.col(\"sentence_count\"))\n",
    "            .alias(\"sentence_count\"),\n",
    "            pl.when(pl.col(\"text_length\") == 0)\n",
    "            .then(1)\n",
    "            .otherwise(pl.col(\"text_length\"))\n",
    "            .alias(\"text_length_safe\"),\n",
    "        ]\n",
    "    )\n",
    "    avg_sentence_expr = pl.col(\"word_count\") / pl.col(\"sentence_count\")\n",
    "    punct_expr = pl.col(\"punct_count\") / pl.col(\"text_length_safe\")\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.when(avg_sentence_expr > 100)\n",
    "            .then(100)\n",
    "            .otherwise(avg_sentence_expr)\n",
    "            .alias(\"avg_sentence_length\"),\n",
    "            pl.when(punct_expr > 0.3)\n",
    "            .then(0.3)\n",
    "            .otherwise(punct_expr)\n",
    "            .alias(\"punct_ratio\"),\n",
    "            (pl.col(\"digit_count\") / pl.col(\"text_length_safe\")).alias(\n",
    "                \"digit_ratio\"\n",
    "            ),\n",
    "            (pl.col(\"upper_count\") / pl.col(\"text_length_safe\")).alias(\n",
    "                \"upper_ratio\"\n",
    "            ),\n",
    "            pl.col(\"text\")\n",
    "            .map_elements(_ttr, return_dtype=pl.Float64)\n",
    "            .alias(\"ttr\"),\n",
    "        ]\n",
    "    )\n",
    "    df = df.drop([\"digit_count\", \"upper_count\", \"text_length_safe\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "log_step_end(\"Processing and feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f0aa5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Load data\")\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / DATA_DIR).exists():\n",
    "    for parent in PROJECT_ROOT.parents:\n",
    "        if (parent / DATA_DIR).exists():\n",
    "            PROJECT_ROOT = parent\n",
    "            break\n",
    "\n",
    "\n",
    "def resolve_path(path_str: str) -> str:\n",
    "    p = Path(path_str)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    if p.parent == Path(\".\"):\n",
    "        data_dir = Path(DATA_DIR)\n",
    "        if not data_dir.is_absolute():\n",
    "            data_dir = PROJECT_ROOT / data_dir\n",
    "        candidate = data_dir / p.name\n",
    "        if candidate.exists():\n",
    "            return str(candidate)\n",
    "    return str((PROJECT_ROOT / p).resolve())\n",
    "\n",
    "\n",
    "# Reassemble chunked CSVs if needed\n",
    "def ensure_chunked_csv(path: Path) -> None:\n",
    "    if path.exists():\n",
    "        return\n",
    "    parts = sorted(path.parent.glob(path.name + \".part*\"))\n",
    "    if not parts:\n",
    "        raise FileNotFoundError(f\"Missing {path} and no chunk files found.\")\n",
    "    tmp_path = path.with_suffix(path.suffix + \".tmp\")\n",
    "    if tmp_path.exists():\n",
    "        tmp_path.unlink()\n",
    "    hasher = hashlib.sha256()\n",
    "    with tmp_path.open(\"wb\") as out:\n",
    "        for part in parts:\n",
    "            with part.open(\"rb\") as f:\n",
    "                while True:\n",
    "                    chunk = f.read(1024 * 1024)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    out.write(chunk)\n",
    "                    hasher.update(chunk)\n",
    "    sha_path = path.with_suffix(path.suffix + \".sha256\")\n",
    "    if sha_path.exists():\n",
    "        expected = sha_path.read_text().split()[0]\n",
    "        actual = hasher.hexdigest()\n",
    "        if expected != actual:\n",
    "            tmp_path.unlink(missing_ok=True)\n",
    "            raise ValueError(\n",
    "                f\"SHA256 mismatch for {path}: expected {expected} got {actual}\"\n",
    "            )\n",
    "    tmp_path.replace(path)\n",
    "    log_step(f\"Reassembled {path} from {len(parts)} chunks.\")\n",
    "\n",
    "\n",
    "train_path = Path(resolve_path(TRAIN_CSV))\n",
    "val_path = Path(resolve_path(VAL_CSV))\n",
    "test_path = Path(resolve_path(TEST_CSV))\n",
    "ensure_chunked_csv(train_path)\n",
    "ensure_chunked_csv(val_path)\n",
    "ensure_chunked_csv(test_path)\n",
    "\n",
    "train = process_text_file(str(train_path))\n",
    "val = process_text_file(str(val_path))\n",
    "test = process_text_file(str(test_path))\n",
    "assert \"label\" in train.columns and \"label\" in val.columns\n",
    "assert \"label\" not in test.columns and \"id\" in test.columns\n",
    "print(\"Rows:\", train.height, val.height, test.height)\n",
    "log_step_end(\"Load data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affdb891",
   "metadata": {},
   "source": [
    "## Sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba07a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Sentiment features\")\n",
    "train = _sentiment(train)\n",
    "val = _sentiment(val)\n",
    "test = _sentiment(test)\n",
    "log_step_end(\"Sentiment features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8e010",
   "metadata": {},
   "source": [
    "## Numeric and TF\u2013IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Numeric and TF\u2013IDF features\")\n",
    "feature_cols: List[str] = [\n",
    "    \"text_length\",\n",
    "    \"word_count\",\n",
    "    \"ttr\",\n",
    "    \"sentence_count\",\n",
    "    \"avg_sentence_length\",\n",
    "    \"punct_ratio\",\n",
    "    \"sentiment_polarity\",\n",
    "    \"sentiment_subjectivity\",\n",
    "    \"digit_ratio\",\n",
    "    \"upper_ratio\",\n",
    "    \"bangs\",\n",
    "    \"questions\",\n",
    "]\n",
    "Xtr_basic = cpx_sparse.csr_matrix(\n",
    "    _cp_asarray_with_backoff(train.select(feature_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "Xva_basic = cpx_sparse.csr_matrix(\n",
    "    _cp_asarray_with_backoff(val.select(feature_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "Xte_basic = cpx_sparse.csr_matrix(\n",
    "    _cp_asarray_with_backoff(test.select(feature_cols).to_numpy().astype(np.float32))\n",
    ")\n",
    "train_text = train[\"text\"].to_list()\n",
    "val_text = val[\"text\"].to_list()\n",
    "test_text = test[\"text\"].to_list()\n",
    "vec_word = SkTfidfVectorizer(\n",
    "    ngram_range=(1, CFG.tfidf_ngram_max),\n",
    "    max_features=CFG.tfidf_max_features,\n",
    "    min_df=CFG.min_df,\n",
    "    stop_words=\"english\",\n",
    "    dtype=np.float32,\n",
    ")\n",
    "vec_word.fit(train_text)\n",
    "_gc()\n",
    "\n",
    "\n",
    "def _transform_in_chunks_cpu(texts, chunk_size: int):\n",
    "    n = len(texts)\n",
    "    chunks = []\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(start + chunk_size, n)\n",
    "        X_chunk = vec_word.transform(texts[start:end])\n",
    "        chunks.append(X_chunk)\n",
    "        _gc()\n",
    "    if not chunks:\n",
    "        return sp.csr_matrix((0, 0))\n",
    "    if len(chunks) == 1:\n",
    "        return chunks[0].tocsr()\n",
    "    return sp.vstack(chunks).tocsr()\n",
    "\n",
    "\n",
    "Xtr_w_cpu = _transform_in_chunks_cpu(train_text, CFG.tfidf_chunk_size)\n",
    "Xva_w_cpu = _transform_in_chunks_cpu(val_text, CFG.tfidf_chunk_size)\n",
    "Xte_w_cpu = _transform_in_chunks_cpu(test_text, CFG.tfidf_chunk_size)\n",
    "\n",
    "del train_text, val_text, test_text\n",
    "_gc()\n",
    "\n",
    "Xtr_w = cpx_sparse.csr_matrix(Xtr_w_cpu)\n",
    "Xva_w = cpx_sparse.csr_matrix(Xva_w_cpu)\n",
    "Xte_w = cpx_sparse.csr_matrix(Xte_w_cpu)\n",
    "\n",
    "del Xtr_w_cpu, Xva_w_cpu, Xte_w_cpu\n",
    "_gc()\n",
    "\n",
    "X_train = cpx_sparse.hstack([Xtr_basic, Xtr_w]).tocsr()\n",
    "X_val = cpx_sparse.hstack([Xva_basic, Xva_w]).tocsr()\n",
    "X_test = cpx_sparse.hstack([Xte_basic, Xte_w]).tocsr()\n",
    "\n",
    "del Xtr_basic, Xva_basic, Xte_basic, Xtr_w, Xva_w, Xte_w\n",
    "_gc()\n",
    "\n",
    "y_train = _cp_asarray_with_backoff(train[\"label\"].to_numpy()).astype(cp.int32)\n",
    "y_val = _cp_asarray_with_backoff(val[\"label\"].to_numpy()).astype(cp.int32)\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "_gc()\n",
    "log_step_end(\"Numeric and TF\u2013IDF features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d868d0f",
   "metadata": {},
   "source": [
    "## Random-search tuning (GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2901045",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Random-search tuning (GPU)\")\n",
    "\n",
    "\n",
    "def _lr_space():\n",
    "    return {\"C\": [0.5, 1.0, 2.0, 4.0]}\n",
    "\n",
    "\n",
    "def _nb_space():\n",
    "    return {\"alpha\": [0.01, 0.05, 0.1, 0.5, 1.0]}\n",
    "\n",
    "\n",
    "def _search_signature(name, space, n_iter, random_state, X_shape):\n",
    "    payload = {\n",
    "        \"name\": name,\n",
    "        \"space\": space,\n",
    "        \"n_iter\": n_iter,\n",
    "        \"random_state\": random_state,\n",
    "        \"X_shape\": list(X_shape),\n",
    "    }\n",
    "    raw = json.dumps(payload, sort_keys=True, default=str)\n",
    "    return hashlib.md5(raw.encode()).hexdigest()[:10]\n",
    "\n",
    "\n",
    "def _load_results(results_path: Path):\n",
    "    rows = []\n",
    "    if not results_path.exists():\n",
    "        return rows\n",
    "    with results_path.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _to_jsonable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _to_jsonable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [_to_jsonable(v) for v in obj]\n",
    "    if isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    return obj\n",
    "\n",
    "\n",
    "def tune_model(\n",
    "    name: str,\n",
    "    build_fn,\n",
    "    space,\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    X_va,\n",
    "    y_va,\n",
    "    n_iter: int,\n",
    "    random_state: int,\n",
    "):\n",
    "    ckpt_root = Path(PROJECT_ROOT) if \"PROJECT_ROOT\" in globals() else Path.cwd()\n",
    "    ckpt_dir = ckpt_root / \"checkpoints\"\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sig = _search_signature(name, space, n_iter, random_state, X_tr.shape)\n",
    "    base = f\"{name.lower()}_search_{sig}\"\n",
    "    candidates_path = ckpt_dir / f\"{base}_candidates.json\"\n",
    "    results_path = ckpt_dir / f\"{base}_results.jsonl\"\n",
    "    best_path = ckpt_dir / f\"{base}_best.json\"\n",
    "    meta_path = ckpt_dir / f\"{base}_meta.json\"\n",
    "\n",
    "    if candidates_path.exists():\n",
    "        candidates = json.loads(candidates_path.read_text())\n",
    "    else:\n",
    "        candidates = list(\n",
    "            ParameterSampler(space, n_iter=n_iter, random_state=random_state)\n",
    "        )\n",
    "        candidates_path.write_text(json.dumps(_to_jsonable(candidates), indent=2))\n",
    "\n",
    "    meta_path.write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"signature\": sig,\n",
    "                \"name\": name,\n",
    "                \"n_iter\": n_iter,\n",
    "                \"random_state\": random_state,\n",
    "                \"X_shape\": list(X_tr.shape),\n",
    "                \"n_candidates\": len(candidates),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rows = _load_results(results_path)\n",
    "    done = set()\n",
    "    best_params, best_f1 = None, -1.0\n",
    "    for row in rows:\n",
    "        idx = row.get(\"iter_idx\")\n",
    "        if idx is not None:\n",
    "            done.add(int(idx))\n",
    "        if row.get(\"status\") == \"ok\":\n",
    "            f1 = float(row.get(\"f1\", -1.0))\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_params = row.get(\"params\")\n",
    "\n",
    "    if rows:\n",
    "        log_step(\n",
    "            f\"{name} resume: {len(done)}/{len(candidates)} candidates done\"\n",
    "        )\n",
    "\n",
    "    with results_path.open(\"a\") as f:\n",
    "        for i, params in enumerate(candidates):\n",
    "            if i in done:\n",
    "                continue\n",
    "            iter_name = f\"{name} iter {i + 1}/{len(candidates)}\"\n",
    "            log_step_start(iter_name)\n",
    "            status = \"ok\"\n",
    "            f1 = None\n",
    "            error = None\n",
    "            model = build_fn(**params)\n",
    "            try:\n",
    "                model.fit(X_tr, y_tr)\n",
    "                p = model.predict_proba(X_va)[:, 1]\n",
    "                f1 = float(_f1_from_proba(y_va, p))\n",
    "            except Exception as exc:\n",
    "                status = \"fail\"\n",
    "                error = f\"{type(exc).__name__}: {exc}\"\n",
    "            record = {\n",
    "                \"iter_idx\": i,\n",
    "                \"status\": status,\n",
    "                \"f1\": f1,\n",
    "                \"params\": _to_jsonable(params),\n",
    "                \"error\": error,\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\n",
    "\")\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "            if status == \"ok\":\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_params = _to_jsonable(params)\n",
    "                    best_path.write_text(\n",
    "                        json.dumps(\n",
    "                            {\n",
    "                                \"best_f1\": best_f1,\n",
    "                                \"best_params\": best_params,\n",
    "                            },\n",
    "                            indent=2,\n",
    "                        )\n",
    "                    )\n",
    "                log_step(f\"{iter_name} f1={f1:.4f} best={best_f1:.4f}\")\n",
    "            else:\n",
    "                log_step(f\"{iter_name} failed: {error}\")\n",
    "            log_step_end(iter_name)\n",
    "            del model\n",
    "            _gc()\n",
    "\n",
    "    print(f\"Best {name}: {best_params} | F1={best_f1:.4f}\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "lr_params = tune_model(\n",
    "    \"LR\",\n",
    "    lambda **p: LogisticRegression(max_iter=2000, **p),\n",
    "    _lr_space(),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    CFG.lr_iter,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "nb_params = tune_model(\n",
    "    \"NB\",\n",
    "    lambda **p: MultinomialNB(**p),\n",
    "    _nb_space(),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    CFG.nb_iter,\n",
    "    random_state=RANDOM_SEED + 17,\n",
    ")\n",
    "log_step_end(\"Random-search tuning (GPU)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb30f46",
   "metadata": {},
   "source": [
    "## Fit final models and calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Fit final models and calibrate\")\n",
    "\n",
    "\n",
    "class PlattCalibrator:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    def fit(self, scores, y):\n",
    "        scores = _cp_asarray_with_backoff(scores).reshape(-1, 1)\n",
    "        self.model.fit(scores, y)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, scores):\n",
    "        scores = _cp_asarray_with_backoff(scores).reshape(-1, 1)\n",
    "        return self.model.predict_proba(scores)[:, 1]\n",
    "\n",
    "\n",
    "X_trval = cpx_sparse.vstack([X_train, X_val]).tocsr()\n",
    "y_trval = cp.concatenate([y_train, y_val])\n",
    "lr_tuned = LogisticRegression(max_iter=2000, **lr_params)\n",
    "nb_tuned = MultinomialNB(**nb_params)\n",
    "log_step_start(\"Fold 1/1 (single split)\")\n",
    "log_step_start(\"LR fit\")\n",
    "lr_tuned.fit(X_trval, y_trval)\n",
    "log_step_end(\"LR fit\")\n",
    "log_step_start(\"NB fit\")\n",
    "nb_tuned.fit(X_trval, y_trval)\n",
    "log_step_end(\"NB fit\")\n",
    "log_step_end(\"Fold 1/1 (single split)\")\n",
    "\n",
    "cal_lr = PlattCalibrator().fit(lr_tuned.predict_proba(X_val)[:, 1], y_val)\n",
    "cal_nb = PlattCalibrator().fit(nb_tuned.predict_proba(X_val)[:, 1], y_val)\n",
    "_gc()\n",
    "log_step_end(\"Fit final models and calibrate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe7e8f",
   "metadata": {},
   "source": [
    "## Ensemble and threshold tuning on val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2176f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Ensemble and threshold tuning on val\")\n",
    "scores_lr = predict_proba_chunks(\n",
    "    lr_tuned, X_val, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "scores_nb = predict_proba_chunks(\n",
    "    nb_tuned, X_val, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "p_lr = _to_numpy(cal_lr.predict_proba(scores_lr))\n",
    "p_nb = _to_numpy(cal_nb.predict_proba(scores_nb))\n",
    "best_w, best_f1, best_thr = 0.5, -1.0, 0.5\n",
    "for w in np.linspace(0.0, 1.0, 21):\n",
    "    p = w * p_nb + (1.0 - w) * p_lr\n",
    "    for thr in np.arange(0.1, 0.91, 0.01):\n",
    "        yhat = (p >= thr).astype(int)\n",
    "        f1 = f1_score_np(y_val, yhat)\n",
    "        if f1 > best_f1:\n",
    "            best_w, best_f1, best_thr = float(w), float(f1), float(thr)\n",
    "print(f\"Ensemble w={best_w:.2f} thr={best_thr:.2f} F1={best_f1:.4f}\")\n",
    "_gc()\n",
    "log_step_end(\"Ensemble and threshold tuning on val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4aacd3",
   "metadata": {},
   "source": [
    "## Validation diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Validation diagnostics\")\n",
    "p_ens = best_w * p_nb + (1.0 - best_w) * p_lr\n",
    "yhat_val = (p_ens >= best_thr).astype(int)\n",
    "print(classification_report_np(y_val, yhat_val))\n",
    "residual_plot(_to_numpy(y_val), p_ens, \"Residuals: validation ensemble\")\n",
    "qq_plot(_to_numpy(y_val) - p_ens, \"QQ: residuals (validation)\")\n",
    "plot_roc_pr(_to_numpy(y_val), p_ens, \"Validation ROC/PR (ensemble)\")\n",
    "plot_confusion(_to_numpy(y_val), yhat_val, \"Confusion (validation)\")\n",
    "violin_by_label(train, \"label\", \"text_length\", \"Text length by label\")\n",
    "log_step_end(\"Validation diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e00c4f",
   "metadata": {},
   "source": [
    "## Coefficient snapshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c26e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Coefficient snapshots\")\n",
    "try:\n",
    "    if hasattr(lr_tuned, \"coef_\"):\n",
    "        coef = _to_numpy(lr_tuned.coef_).ravel()\n",
    "        idx = np.argsort(np.abs(coef))[::-1][:25]\n",
    "        print(\"Top 25 |coef| for LR: indices and values:\")\n",
    "        for i in idx:\n",
    "            print(i, float(coef[i]))\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"LR coef summary failed: {e}\")\n",
    "log_step_end(\"Coefficient snapshots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d4521",
   "metadata": {},
   "source": [
    "## Predict on test and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Predict on test and save submission\")\n",
    "scores_lr = predict_proba_chunks(\n",
    "    lr_tuned, X_test, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "scores_nb = predict_proba_chunks(\n",
    "    nb_tuned, X_test, chunk_size=CFG.proba_chunk_size\n",
    ")\n",
    "p_lr_te = _to_numpy(cal_lr.predict_proba(scores_lr))\n",
    "p_nb_te = _to_numpy(cal_nb.predict_proba(scores_nb))\n",
    "p_ens_te = best_w * p_nb_te + (1.0 - best_w) * p_lr_te\n",
    "yhat_te = (p_ens_te >= best_thr).astype(int)\n",
    "submission = pl.DataFrame({\"id\": test[\"id\"], \"label\": yhat_te})\n",
    "outputs_dir = \"outputs\"\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "submission_path = os.path.join(outputs_dir, \"submission_hvsm_prod_b.csv\")\n",
    "submission.write_csv(submission_path)\n",
    "print(\"Saved\", submission_path, \"with\", submission.height, \"rows\")\n",
    "_gc()\n",
    "log_step_end(\"Predict on test and save submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f3b67",
   "metadata": {},
   "source": [
    "## Final checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start(\"Final checks\")\n",
    "assert set(submission[\"label\"].to_list()).issubset({0, 1})\n",
    "print(\"Done. All checks passed.\")\n",
    "log_step_end(\"Final checks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
