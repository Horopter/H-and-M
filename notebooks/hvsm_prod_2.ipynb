{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2cf675",
   "metadata": {},
   "source": [
    "# HVSM \u2014 TF\u2013IDF + LR/XGB with Binary Rules, CV, and Prevalence Match\n\nInputs: `data/train.csv`, `data/val.csv`, `data/test.csv` in `data/`. `data/test.csv` must have `id` and no `label`. Output: `outputs/submission_hvsm_prod_2.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d71cc",
   "metadata": {},
   "source": [
    "## Imports and guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468392e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\nimport os, re, string, warnings\nimport gc\nimport time\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np, pandas as pd\nfrom tqdm import tqdm\nfrom scipy import stats\nfrom scipy.sparse import csr_matrix, hstack, vstack\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (classification_report, confusion_matrix,\n    f1_score, roc_auc_score, roc_curve, precision_recall_curve)\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\ntry:\n    import seaborn as sns\nexcept Exception:\n    sns = None\ntry:\n    from textblob import TextBlob\nexcept Exception:\n    TextBlob = None\n    warnings.warn('TextBlob missing; sentiment features set to zeros.')\nnp.set_printoptions(linewidth=79)\npd.set_option('display.width', 79)\npd.set_option('display.max_columns', 60)\nRANDOM_SEED = 42\nrng = np.random.default_rng(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d366fbf",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42190c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    tfidf_max_features: int = 50000\n",
    "    tfidf_ngram_max: int = 3\n",
    "    use_char_ngrams: bool = False\n",
    "    char_tfidf_max_features: int = 20000\n",
    "    min_df: int = 2\n",
    "    kfolds: int = 5\n",
    "    xgb_iter: int = 25\n",
    "    lr_iter: int = 25\n",
    "    plot_level: str = 'full'\n",
    "CFG = Config(); print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44654139",
   "metadata": {},
   "source": [
    "## Plotting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gc() -> None:\n    gc.collect()\n\n\n_STEP_STARTS = {}\n\ndef log_step(msg: str) -> None:\n    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    print(f\"[{ts}] {msg}\", flush=True)\n\ndef log_step_start(name: str) -> None:\n    _STEP_STARTS[name] = time.perf_counter()\n    log_step(f\"START: {name}\")\n\ndef log_step_end(name: str) -> None:\n    start = _STEP_STARTS.pop(name, None)\n    if start is None:\n        log_step(f\"END: {name}\")\n    else:\n        elapsed = time.perf_counter() - start\n        log_step(f\"END: {name} (elapsed {elapsed:.1f}s)\")\n\n\ndef predict_proba_chunks(model, X, chunk_size: int = 50000) -> np.ndarray:\n    n = X.shape[0]\n    out = np.empty(n, dtype=np.float32)\n    for start in range(0, n, chunk_size):\n        end = min(start + chunk_size, n)\n        out[start:end] = model.predict_proba(X[start:end])[:, 1]\n    return out\n\n\ndef _tight() -> None:\n    plt.tight_layout()\ndef qq_plot(residuals: np.ndarray, title: str) -> None:\n    plt.figure(figsize=(5, 4)); stats.probplot(residuals, dist='norm',\n                                               plot=plt)\n    plt.title(title); _tight(); plt.show()\ndef residual_plot(y_true: np.ndarray, y_prob: np.ndarray,\n                  title: str) -> None:\n    resid = y_true - y_prob\n    plt.figure(figsize=(5, 4)); plt.scatter(y_prob, resid, s=8)\n    plt.axhline(0.0, linestyle='--'); plt.xlabel('p(y=1)');\n    plt.ylabel('residual'); plt.title(title); _tight(); plt.show()\ndef violin_by_label(df: pd.DataFrame, label_col: str, feat_col: str,\n                    title: str) -> None:\n    if sns is None:\n        df.boxplot(column=feat_col, by=label_col, figsize=(5, 4))\n        plt.title(title); plt.suptitle(''); _tight(); plt.show(); return\n    plt.figure(figsize=(5, 4));\n    sns.violinplot(data=df, x=label_col, y=feat_col)\n    plt.title(title); _tight(); plt.show()\ndef plot_roc_pr(y_true: np.ndarray, y_prob: np.ndarray, title: str)->None:\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    prec, rec, _ = precision_recall_curve(y_true, y_prob)\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n    ax[0].plot(fpr, tpr)\n    ax[0].set_title(f'ROC AUC={roc_auc_score(y_true, y_prob):.3f}')\n    ax[0].set_xlabel('FPR'); ax[0].set_ylabel('TPR')\n    ax[1].plot(rec, prec); ax[1].set_title('Precision\u2013Recall')\n    ax[1].set_xlabel('Recall'); ax[1].set_ylabel('Precision')\n    _tight(); plt.show()\ndef plot_confusion(y_true: np.ndarray, y_hat: np.ndarray, title: str)->None:\n    cm = confusion_matrix(y_true, y_hat)\n    plt.figure(figsize=(4, 3)); plt.imshow(cm, cmap='Blues')\n    plt.title(title); plt.colorbar()\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, int(cm[i, j]), ha='center', va='center')\n    plt.xlabel('Pred'); plt.ylabel('True'); _tight(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca448e3f",
   "metadata": {},
   "source": [
    "## Processing and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Processing and features')\n",
    "def _ttr(text: str) -> float:\n    toks = re.findall(r'\\S+', text.lower())\n    return float(len(set(toks)) / len(toks)) if toks else 0.0\ndef _sentiment(df: pd.DataFrame) -> pd.DataFrame:\n    if TextBlob is None:\n        df['sentiment_polarity'] = 0.0\n        df['sentiment_subjectivity'] = 0.0\n        return df\n    tqdm.pandas();\n    df['sentiment_polarity'] = df['text'].progress_apply(\n        lambda x: float(TextBlob(x).sentiment.polarity))\n    df['sentiment_subjectivity'] = df['text'].progress_apply(\n        lambda x: float(TextBlob(x).sentiment.subjectivity))\n    return df\ndef process_text_file(filename: str) -> pd.DataFrame:\n    df = pd.read_csv(os.path.join(filename))\n    assert 'text' in df.columns\n    df['text'] = df['text'].astype(str)\n    df['text_length'] = df['text'].str.len()\n    df['word_count'] = df['text'].str.split().str.len()\n    df['sentence_count'] = df['text'].str.count(r'[.!?]+').replace(0, 1)\n    df['avg_sentence_length'] = (\n        (df['word_count']/df['sentence_count']).clip(upper=100))\n    df['punct_count'] = df['text'].str.count(r'[^\\w\\s]')\n    df['punct_ratio'] = (\n        (df['punct_count']/df['text_length']).clip(0, 0.3))\n    df['ttr'] = df['text'].apply(_ttr)\n    df['digit_ratio'] = df['text'].str.count(r'\\d') / (\n        df['text_length'].replace(0, 1))\n    df['upper_ratio'] = df['text'].str.count(r'[A-Z]') / (\n        df['text_length'].replace(0, 1))\n    df['bangs'] = df['text'].str.count(r'!')\n    df['questions'] = df['text'].str.count(r'\\?')\n    return df\n",
    "log_step_end('Processing and features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b3d62",
   "metadata": {},
   "source": [
    "## Binary features and 2^3 sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17276e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Binary features and 2^3 sweep')\n",
    "def ends_with_letter(text: str) -> int:\n    s = text.rstrip();\n    return int(len(s) > 0 and s[-1] in string.ascii_letters)\ndef has_5gram_repetition(text: str) -> int:\n    toks = re.findall(r\"\\S+\", text)\n    if len(toks) < 10: return 0\n    seen = {}; w = 5\n    for i in range(len(toks) - w + 1):\n        key = tuple(toks[i:i+w])\n        if key in seen: return 1\n        seen[key] = 1\n    return 0\nCOMMON_SMALL = set(['the','be','to','of','and','a','in','that','have',\n    'i','it','for','not','on','with','he','as','you','do','at','this',\n    'but','his'])\ndef max_uncommon_binary(text: str, thr_rep: int = 3,\n                        thr_count: int = 5) -> int:\n    toks = [t.lower() for t in re.findall(r\"\\w+\", text)]\n    if not toks: return 0\n    freqs = {}; uncommon = 0\n    for t in toks:\n        if t not in COMMON_SMALL:\n            uncommon += 1\n            freqs[t] = freqs.get(t, 0) + 1\n    if uncommon < thr_count: return 0\n    return int(any(v >= thr_rep for v in freqs.values()))\ndef add_binary_feats(df: pd.DataFrame) -> pd.DataFrame:\n    out = df.copy(); tqdm.pandas()\n    out['ends_with_letter'] = out['text'].progress_apply(ends_with_letter)\n    out['has_5gram_repetition'] = out['text'].progress_apply(\n        has_5gram_repetition)\n    out['max_uncommon_binary'] = out['text'].progress_apply(\n        max_uncommon_binary)\n    return out\ndef sweep_binary_subsets(y_true: np.ndarray, fe_df: pd.DataFrame):\n    cols = ['ends_with_letter','has_5gram_repetition','max_uncommon_binary']\n    best_f1, best_key = -1.0, 'none'\n    for mask in range(1, 1 << len(cols)):\n        sel = [cols[i] for i in range(len(cols)) if (mask >> i) & 1]\n        rule = fe_df[sel].any(axis=1).astype(int).values\n        f1 = f1_score(y_true, rule)\n        key = '|'.join(sel)\n        if f1 > best_f1: best_f1, best_key = f1, key\n    return best_key, float(best_f1)\n",
    "log_step_end('Binary features and 2^3 sweep')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cc586",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Load data')\ntrain = process_text_file('data/train.csv')\nval = process_text_file('data/val.csv')\ntest = process_text_file('data/test.csv')\nassert 'label' in train.columns and 'label' in val.columns\nassert 'label' not in test.columns and 'id' in test.columns\nprint('Rows:', len(train), len(val), len(test))\nlog_step_end('Load data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542007f6",
   "metadata": {},
   "source": [
    "## Sentiment + binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Sentiment + binaries')\n",
    "train = _sentiment(train); val = _sentiment(val); test = _sentiment(test)\ntrain = add_binary_feats(train); val = add_binary_feats(val);\ntest = add_binary_feats(test)\nrk, rf1 = sweep_binary_subsets(val['label'].astype(int).values, val)\nprint(f'Best binary subset (val): {rk} | F1={rf1:.4f}')\n",
    "log_step_end('Sentiment + binaries')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb7f0d",
   "metadata": {},
   "source": [
    "## Numeric + TF\u2013IDF design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038226de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Numeric + TF\u2013IDF design')\n",
    "num_cols = ['text_length','word_count','ttr','sentence_count',\n            'avg_sentence_length','punct_ratio','sentiment_polarity',\n            'sentiment_subjectivity','digit_ratio','upper_ratio','bangs',\n            'questions','ends_with_letter','has_5gram_repetition',\n            'max_uncommon_binary']\nXtr_num = csr_matrix(train[num_cols].to_numpy(dtype=np.float32))\nXva_num = csr_matrix(val[num_cols].to_numpy(dtype=np.float32))\nXte_num = csr_matrix(test[num_cols].to_numpy(dtype=np.float32))\nvec_word = TfidfVectorizer(ngram_range=(1, 3), max_features=50000,\n                           min_df=2, stop_words='english', dtype=np.float32)\nXtr_w = vec_word.fit_transform(train['text'])\nXva_w = vec_word.transform(val['text'])\nXte_w = vec_word.transform(test['text'])\nX_train = hstack([Xtr_num, Xtr_w])\nX_val = hstack([Xva_num, Xva_w])\nX_test = hstack([Xte_num, Xte_w])\ny_train = train['label'].astype(int).values\ny_val = val['label'].astype(int).values\nprint('Shapes:', X_train.shape, X_val.shape, X_test.shape)\n_gc()\n",
    "log_step_end('Numeric + TF\u2013IDF design')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb16023",
   "metadata": {},
   "source": [
    "## Tuning and calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Tuning and calibration')\ndef _xgb_space():\n    return {'n_estimators':[300,500,700],'max_depth':[4,6,8],\n            'learning_rate':[0.05,0.1],'min_child_weight':[1,3],\n            'subsample':[0.7,1.0],'colsample_bytree':[0.7,1.0],\n            'reg_alpha':[0.0,0.1,0.5],'reg_lambda':[0.5,1.0,1.5]}\ndef _lr_space():\n    return {'C':[0.5,1.0,2.0,4.0],'penalty':['l2'],\n            'solver':['liblinear','lbfgs'],'class_weight':[None,'balanced']}\ndef tune_xgb(X, y):\n    base = XGBClassifier(random_state=42, eval_metric='logloss',\n                         tree_method='hist', n_jobs=1)\n    rs = RandomizedSearchCV(base, _xgb_space(), n_iter=25, scoring='f1',\n                            n_jobs=1, cv=5, verbose=3, random_state=42,\n                            pre_dispatch=1,\n                            refit=True)\n    log_step_start('XGB randomized search')\n    rs.fit(X, y)\n    log_step_end('XGB randomized search')\n    print('Best XGB:', rs.best_params_); return rs.best_estimator_\ndef tune_lr(X, y):\n    base = LogisticRegression(max_iter=2000, random_state=42)\n    rs = RandomizedSearchCV(base, _lr_space(), n_iter=25, scoring='f1',\n                            n_jobs=1, cv=5, verbose=3, random_state=42,\n                            pre_dispatch=1,\n                            refit=True)\n    log_step_start('LR randomized search')\n    rs.fit(X, y)\n    log_step_end('LR randomized search')\n    print('Best LR:', rs.best_params_); return rs.best_estimator_\nxgb_tuned = tune_xgb(X_train, y_train)\nlr_tuned = tune_lr(X_train, y_train)\nX_trval = vstack([X_train, X_val]); y_trval = np.concatenate([y_train, y_val])\nlog_step_start('Fold 1/1 (single split)')\nlog_step_start('XGB training epochs')\nxgb_tuned.fit(X_trval, y_trval, eval_set=[(X_val, y_val)], verbose=True)\nlog_step_end('XGB training epochs')\nlog_step_start('LR fit')\nlr_tuned.fit(X_trval, y_trval)\nlog_step_end('LR fit')\nlog_step_end('Fold 1/1 (single split)')\n\ncal_xgb = CalibratedClassifierCV(xgb_tuned, method='sigmoid', cv='prefit')\ncal_xgb.fit(X_val, y_val)\ncal_lr = CalibratedClassifierCV(lr_tuned, method='sigmoid', cv='prefit')\ncal_lr.fit(X_val, y_val)\n_gc()\nlog_step_end('Tuning and calibration')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c0101",
   "metadata": {},
   "source": [
    "## Ensembling, thresholding, prevalence match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b81022",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Ensembling, thresholding, prevalence match')\n",
    "def decode_prevalence(y_prob: np.ndarray, pos_rate: float) -> np.ndarray:\n    n = len(y_prob); k = int(round(pos_rate * n))\n    idx = np.argsort(-y_prob); out = np.zeros(n, dtype=int); out[idx[:k]] = 1\n    return out\np_xgb = cal_xgb.predict_proba(X_val)[:, 1]\np_lr = cal_lr.predict_proba(X_val)[:, 1]\nbest_w, best_f1, best_thr = 0.5, -1.0, 0.5\nfor w in np.linspace(0.0, 1.0, 21):\n    p = w * p_xgb + (1.0 - w) * p_lr\n    for thr in np.arange(0.1, 0.91, 0.01):\n        f1 = f1_score(y_val, (p >= thr).astype(int))\n        if f1 > best_f1: best_w, best_f1, best_thr = float(w), float(f1), float(thr)\nprint(f'Threshold head: w={best_w:.2f} thr={best_thr:.2f} F1={best_f1:.4f}')\np_ens = best_w * p_xgb + (1.0 - best_w) * p_lr\nval_pos_rate = float(np.mean(y_val))\nyhat_topk = decode_prevalence(p_ens, val_pos_rate)\nf1_topk = f1_score(y_val, yhat_topk)\nprint(f'Prevalence head: rate={val_pos_rate:.3f} F1={f1_topk:.4f}')\nrk, rf1 = sweep_binary_subsets(y_val, val)\nprint(f'Rule head (best subset {rk}) F1={rf1:.4f}')\nheads = [('threshold', best_f1), ('prevalence', f1_topk), ('rule', rf1)]\nheads.sort(key=lambda x: x[1], reverse=True)\nprint('Head ranking:', heads)\n",
    "log_step_end('Ensembling, thresholding, prevalence match')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d5fd4",
   "metadata": {},
   "source": [
    "## Validation diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Validation diagnostics')\n",
    "winner = heads[0][0]\nif winner == 'threshold': yhat_val = (p_ens >= best_thr).astype(int)\nelif winner == 'prevalence': yhat_val = yhat_topk\nelse:\n    yhat_val = val[['ends_with_letter','has_5gram_repetition',\n                    'max_uncommon_binary']].any(axis=1).astype(int).values\nprint(classification_report(y_val, yhat_val))\nresidual_plot(y_val, p_ens, 'Residuals: ensemble on val')\nqq_plot(y_val - p_ens, 'QQ: residuals (val)')\nplot_roc_pr(y_val, p_ens, 'Validation ROC/PR (ensemble)')\nplot_confusion(y_val, yhat_val, 'Confusion (val, winner head)')\n",
    "log_step_end('Validation diagnostics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55f5f2",
   "metadata": {},
   "source": [
    "## Predict test and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_start('Predict test and save submission')\n",
    "p_xgb_te = predict_proba_chunks(cal_xgb, X_test)\np_lr_te = predict_proba_chunks(cal_lr, X_test)\np_ens_te = best_w * p_xgb_te + (1.0 - best_w) * p_lr_te\nif winner == 'threshold': yhat_te = (p_ens_te >= best_thr).astype(int)\nelif winner == 'prevalence': yhat_te = decode_prevalence(p_ens_te, val_pos_rate)\nelse:\n    yhat_te = test[['ends_with_letter','has_5gram_repetition',\n                    'max_uncommon_binary']].any(axis=1).astype(int).values\nsubmission = pd.DataFrame({'id': test['id'], 'label': yhat_te})\noutputs_dir = 'outputs'\nos.makedirs(outputs_dir, exist_ok=True)\nsubmission_path = os.path.join(outputs_dir, 'submission_hvsm_prod_2.csv')\nsubmission.to_csv(submission_path, index=False)\nprint('Saved', submission_path, 'with', len(submission), 'rows')\n_gc()\n",
    "log_step_end('Predict test and save submission')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}