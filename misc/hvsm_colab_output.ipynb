{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a0784c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e851c38",
   "metadata": {
    "papermill": {
     "duration": 0.006843,
     "end_time": "2025-10-21T13:56:23.076743",
     "exception": false,
     "start_time": "2025-10-21T13:56:23.069900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HVSM: Local CSVs, Time‑boxed Training, Rich Features + Models\n",
    "\n",
    "This notebook reads `./train.csv`, `./val.csv`, and `./test.csv` from the current folder. It implements non‑sklearn baselines and deep models in PyTorch, with strict time budgets per algorithm, robust early exits, and many seaborn visualizations. All model weights are saved under `./weights/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3107a5e",
   "metadata": {
    "papermill": {
     "duration": 0.006697,
     "end_time": "2025-10-21T13:56:23.089437",
     "exception": false,
     "start_time": "2025-10-21T13:56:23.082740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Environment and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6975de3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T13:56:23.103108Z",
     "iopub.status.busy": "2025-10-21T13:56:23.102904Z",
     "iopub.status.idle": "2025-10-21T13:56:28.282318Z",
     "shell.execute_reply": "2025-10-21T13:56:28.281795Z"
    },
    "papermill": {
     "duration": 5.187602,
     "end_time": "2025-10-21T13:56:28.283822",
     "exception": false,
     "start_time": "2025-10-21T13:56:23.096220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Cannot install datasets==2.21.0, requests==2.31.0 and transformers==4.44.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installs complete.\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip -q install \\\n",
    "  transformers==4.44.2 datasets==2.21.0 torchtext==0.18.0 \\\n",
    "  ftfy==6.2.3 emoji==2.14.0 ipywidgets==8.1.5 xgboost==2.1.1 \\\n",
    "  spacy==3.7.5 spacy-lookups-data==1.0.5 wordfreq==3.1.1 \\\n",
    "  textstat==0.7.4 nltk==3.9.1 scipy==1.11.4 \\\n",
    "  requests_mock==1.11.0 requests==2.31.0 clyent==1.2.1\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Installs complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72924fe6",
   "metadata": {
    "papermill": {
     "duration": 0.005864,
     "end_time": "2025-10-21T13:56:28.296040",
     "exception": false,
     "start_time": "2025-10-21T13:56:28.290176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Imports, Device, Seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21570a3d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23fc81c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T13:56:28.309220Z",
     "iopub.status.busy": "2025-10-21T13:56:28.309001Z",
     "iopub.status.idle": "2025-10-21T13:56:39.472164Z",
     "shell.execute_reply": "2025-10-21T13:56:39.470630Z"
    },
    "papermill": {
     "duration": 11.171057,
     "end_time": "2025-10-21T13:56:39.473118",
     "exception": true,
     "start_time": "2025-10-21T13:56:28.302061",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordfreq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zipf_frequency\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextstat\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy import sparse as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(context='notebook', style='whitegrid')\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from wordfreq import zipf_frequency\n",
    "import textstat\n",
    "import xgboost as xgb\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "RANDOM_SEED: int = 12345\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "DEVICE: torch.device = (\n",
    "    torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "try:\n",
    "    _ = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    from spacy.cli import download as _spacy_dl\n",
    "    _spacy_dl('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "os.makedirs('./weights', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe4a2f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Load Local CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb1c83",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH: str = './train.csv'\n",
    "VAL_PATH: str = './val.csv'\n",
    "TEST_PATH: str = './test.csv'\n",
    "assert os.path.exists(TRAIN_PATH)\n",
    "assert os.path.exists(VAL_PATH)\n",
    "assert os.path.exists(TEST_PATH)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.replace('\\r', '\\n')\n",
    "    text = ''.join(ch for ch in text if ch == '\\n' or ch >= ' ')\n",
    "    return text\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df = pd.read_csv(VAL_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "for df in (train_df, val_df, test_df):\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['text'] = df['text'].astype(str).map(clean_text)\n",
    "display(train_df.head())\n",
    "display(val_df.head())\n",
    "display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf55c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Metrics and Cross‑Validation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81bbba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_macro(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    f1s: List[float] = []\n",
    "    for c in classes:\n",
    "        tp = np.sum((y_true == c) & (y_pred == c))\n",
    "        fp = np.sum((y_true != c) & (y_pred == c))\n",
    "        fn = np.sum((y_true == c) & (y_pred != c))\n",
    "        prec = tp / (tp + fp + 1e-12)\n",
    "        rec = tp / (tp + fn + 1e-12)\n",
    "        f1 = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
    "        f1s.append(float(f1))\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "def kfold_indices(n: int, k: int, seed: int) -> List[Tuple[np.ndarray,\n",
    "                                                          np.ndarray]]:\n",
    "    idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idx)\n",
    "    folds: List[np.ndarray] = np.array_split(idx, k)\n",
    "    splits: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "    for i in range(k):\n",
    "        val_idx = folds[i]\n",
    "        tr_idx = np.concatenate([f for j, f in enumerate(folds) if j != i])\n",
    "        splits.append((tr_idx, val_idx))\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87169245",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. EDA: Distributions, Correlations, Stylometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ee15d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_basic_feats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp = df.copy()\n",
    "    tmp['n_chars'] = tmp['text'].str.len()\n",
    "    tmp['n_words'] = tmp['text'].str.split().map(len)\n",
    "    tmp['pct_punct'] = tmp['text'].str.count(r\"[\\.,;:!?]\") \\\n",
    "        / tmp['n_chars'].clip(lower=1)\n",
    "    tmp['pct_digits'] = tmp['text'].str.count(r\"\\d\") \\\n",
    "        / tmp['n_chars'].clip(lower=1)\n",
    "    tmp['pct_upper'] = tmp['text'].str.count(r\"[A-Z]\") \\\n",
    "        / tmp['n_chars'].clip(lower=1)\n",
    "    return tmp\n",
    "\n",
    "train_feats = add_basic_feats(train_df)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(13, 7))\n",
    "sns.countplot(x='label', data=train_df, ax=axes[0,0])\n",
    "axes[0,0].set_title('Label distribution')\n",
    "sns.kdeplot(data=train_feats, x='n_chars', hue='label', ax=axes[0,1])\n",
    "axes[0,1].set_title('Chars by class')\n",
    "sns.kdeplot(data=train_feats, x='n_words', hue='label', ax=axes[0,2])\n",
    "axes[0,2].set_title('Words by class')\n",
    "sns.kdeplot(data=train_feats, x='pct_punct', hue='label', ax=axes[1,0])\n",
    "axes[1,0].set_title('Punctuation ratio')\n",
    "sns.kdeplot(data=train_feats, x='pct_digits', hue='label', ax=axes[1,1])\n",
    "axes[1,1].set_title('Digits ratio')\n",
    "sns.kdeplot(data=train_feats, x='pct_upper', hue='label', ax=axes[1,2])\n",
    "axes[1,2].set_title('Uppercase ratio')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d2e03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Rich Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e225dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_ngrams(tokens: List[str], n_min: int, n_max: int) -> List[str]:\n",
    "    grams: List[str] = []\n",
    "    for n in range(n_min, n_max + 1):\n",
    "        for i in range(0, len(tokens) - n + 1):\n",
    "            grams.append('_'.join(tokens[i:i+n]))\n",
    "    return grams\n",
    "\n",
    "def char_ngrams(text: str, n_min: int, n_max: int) -> List[str]:\n",
    "    grams: List[str] = []\n",
    "    for n in range(n_min, n_max + 1):\n",
    "        for i in range(0, len(text) - n + 1):\n",
    "            grams.append(text[i:i+n])\n",
    "    return grams\n",
    "\n",
    "def hash_token(tok: str, dim: int, seed: int) -> int:\n",
    "    return abs(hash((tok, seed))) % dim\n",
    "\n",
    "def build_hashed_csr(texts: List[str], dim: int, seed: int,\n",
    "                     w_ng: Tuple[int, int], c_ng: Tuple[int, int]\n",
    "                     ) -> sp.csr_matrix:\n",
    "    data: List[float] = []\n",
    "    rows: List[int] = []\n",
    "    cols: List[int] = []\n",
    "    for r, t in enumerate(texts):\n",
    "        toks = t.split()\n",
    "        feats = toks + word_ngrams(toks, w_ng[0], w_ng[1]) \\\n",
    "            + char_ngrams(t, c_ng[0], c_ng[1])\n",
    "        counts: Dict[int, int] = {}\n",
    "        for f in feats:\n",
    "            j = hash_token(f, dim, seed)\n",
    "            counts[j] = counts.get(j, 0) + 1\n",
    "        for j, v in counts.items():\n",
    "            rows.append(r); cols.append(j); data.append(float(v))\n",
    "    mat = sp.csr_matrix((np.array(data), (np.array(rows), np.array(cols))),\n",
    "                        shape=(len(texts), dim), dtype=np.float32)\n",
    "    return mat\n",
    "\n",
    "def tfidf_transform(mat: sp.csr_matrix) -> sp.csr_matrix:\n",
    "    df = np.asarray((mat > 0).sum(axis=0)).ravel()\n",
    "    n_docs = mat.shape[0]\n",
    "    idf = np.log((1 + n_docs) / (1 + df)) + 1.0\n",
    "    mat = mat.tocoo(copy=True)\n",
    "    mat.data = np.log1p(mat.data)\n",
    "    mat = mat.tocsr(copy=True)\n",
    "    mat = mat.multiply(idf)\n",
    "    row_norms = np.sqrt(mat.multiply(mat).sum(axis=1)).A1\n",
    "    row_norms[row_norms == 0] = 1.0\n",
    "    inv = sp.diags(1.0 / row_norms)\n",
    "    return inv.dot(mat)\n",
    "\n",
    "def pos_dep_feats(text: str) -> Dict[str, float]:\n",
    "    doc = nlp(text)\n",
    "    pos_cnt = Counter(t.pos_ for t in doc)\n",
    "    dep_cnt = Counter(t.dep_ for t in doc)\n",
    "    total = max(1, len(doc))\n",
    "    pos_props = {f'pos_{k}': v / total for k, v in pos_cnt.items()}\n",
    "    dep_props = {f'dep_{k}': v / total for k, v in dep_cnt.items()}\n",
    "    return {**pos_props, **dep_props}\n",
    "\n",
    "def sentiment_feats(text: str) -> Dict[str, float]:\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return {f'sent_{k}': float(v) for k, v in scores.items()}\n",
    "\n",
    "def readability_feats(text: str) -> Dict[str, float]:\n",
    "    try:\n",
    "        fl = float(textstat.flesch_reading_ease(text))\n",
    "        smog = float(textstat.smog_index(text))\n",
    "        ari = float(textstat.automated_readability_index(text))\n",
    "        gf = float(textstat.gunning_fog(text))\n",
    "    except Exception:\n",
    "        fl, smog, ari, gf = 0.0, 0.0, 0.0, 0.0\n",
    "    return {\n",
    "        'read_flesch': fl, 'read_smog': smog,\n",
    "        'read_ari': ari, 'read_gunning': gf\n",
    "    }\n",
    "\n",
    "def lexical_richness(tokens: List[str]) -> Dict[str, float]:\n",
    "    n = len(tokens)\n",
    "    types = set(tokens)\n",
    "    ttr = len(types) / max(1, n)\n",
    "    cnt = Counter(tokens)\n",
    "    hapax = sum(1 for k, v in cnt.items() if v == 1) / max(1, len(cnt))\n",
    "    dis = sum(1 for k, v in cnt.items() if v == 2) / max(1, len(cnt))\n",
    "    m1 = sum(cnt.values())\n",
    "    m2 = sum(v*v for v in cnt.values())\n",
    "    yule_k = 1e4 * (m2 - m1) / (m1 * m1 + 1e-12)\n",
    "    return {\n",
    "        'ttr': float(ttr), 'hapax_ratio': float(hapax),\n",
    "        'dis_ratio': float(dis), 'yule_k': float(yule_k)\n",
    "    }\n",
    "\n",
    "def zipf_feats(tokens: List[str]) -> Dict[str, float]:\n",
    "    if not tokens:\n",
    "        return {'zipf_mean': 0.0, 'zipf_std': 0.0}\n",
    "    z = [zipf_frequency(t, 'en') for t in tokens]\n",
    "    return {'zipf_mean': float(np.mean(z)), 'zipf_std': float(np.std(z))}\n",
    "\n",
    "def build_dense_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows: List[Dict[str, float]] = []\n",
    "    for t in df['text'].tolist():\n",
    "        toks = t.split()\n",
    "        feats: Dict[str, float] = {}\n",
    "        feats.update(lexical_richness(toks))\n",
    "        feats.update(pos_dep_feats(t))\n",
    "        feats.update(sentiment_feats(t))\n",
    "        feats.update(readability_feats(t))\n",
    "        feats.update(zipf_feats(toks))\n",
    "        rows.append(feats)\n",
    "    dense = pd.DataFrame(rows).fillna(0.0)\n",
    "    return dense.reset_index(drop=True)\n",
    "\n",
    "W_DIM: int = 2**19\n",
    "SEED_HASH: int = 13\n",
    "\n",
    "def build_sparse_features(df: pd.DataFrame) -> sp.csr_matrix:\n",
    "    Xw = build_hashed_csr(df['text'].tolist(), W_DIM, SEED_HASH,\n",
    "                          w_ng=(1, 2), c_ng=(0, -1))\n",
    "    Xc = build_hashed_csr(df['text'].tolist(), W_DIM, SEED_HASH,\n",
    "                          w_ng=(0, -1), c_ng=(3, 5))\n",
    "    Xw = tfidf_transform(Xw)\n",
    "    Xc = tfidf_transform(Xc)\n",
    "    return sp.hstack([Xw, Xc], format='csr')\n",
    "\n",
    "def build_all_features(df: pd.DataFrame) -> Tuple[sp.csr_matrix, pd.DataFrame]:\n",
    "    dense = build_dense_features(df)\n",
    "    sparse = build_sparse_features(df)\n",
    "    return sparse, dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc7d92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9b6c9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dense = build_dense_features(train_df)\n",
    "corr = train_dense.corr(numeric_only=True)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.title('Dense feature correlations')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "keep_cols = [c for c in train_dense.columns if c.startswith('pos_')][:8]\n",
    "plt.figure(figsize=(10, 5))\n",
    "melted = pd.concat([train_dense[keep_cols], train_df['label']], axis=1) \\\n",
    "    .melt(id_vars='label', var_name='feat', value_name='val')\n",
    "sns.violinplot(data=melted, x='feat', y='val', hue='label', split=True)\n",
    "plt.xticks(rotation=30); plt.title('POS proportions by class')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8948939",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. Classical Models (no sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ad22f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, in_dim: int, n_classes: int = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, n_classes)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_linear_model(\n",
    "    Xs: sp.csr_matrix,\n",
    "    Xd: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    Xs_val: sp.csr_matrix,\n",
    "    Xd_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    loss: str = 'log',\n",
    "    epochs: int = 6,\n",
    "    bs: int = 256,\n",
    "    lr: float = 1e-2,\n",
    "    name: str = 'linear'\n",
    ") -> Tuple[LinearClassifier, float]:\n",
    "    start = time.time()\n",
    "    time_budget = 3 * 60 * 60  # 3 hours\n",
    "    early_check = 30 * 60  # 30 minutes\n",
    "\n",
    "    X = sp.hstack([Xs, sp.csr_matrix(Xd)], format='csr')\n",
    "    V = sp.hstack([Xs_val, sp.csr_matrix(Xd_val)], format='csr')\n",
    "    in_dim = X.shape[1]\n",
    "    model = LinearClassifier(in_dim).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def batcher(A: sp.csr_matrix, b: np.ndarray, batch: int):\n",
    "        n = A.shape[0]\n",
    "        idx = np.arange(n)\n",
    "        np.random.shuffle(idx)\n",
    "        for i in range(0, n, batch):\n",
    "            j = idx[i:i+batch]\n",
    "            Xt = torch.tensor(A[j].toarray(), dtype=torch.float32,\n",
    "                               device=DEVICE)\n",
    "            yt = torch.tensor(b[j], dtype=torch.long, device=DEVICE)\n",
    "            yield Xt, yt\n",
    "\n",
    "    def predict(A: sp.csr_matrix) -> np.ndarray:\n",
    "        outs: List[np.ndarray] = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, A.shape[0], bs):\n",
    "                sl = slice(i, min(i+bs, A.shape[0]))\n",
    "                Xt = torch.tensor(A[sl].toarray(), dtype=torch.float32,\n",
    "                                   device=DEVICE)\n",
    "                outs.append(model(Xt).cpu().numpy())\n",
    "        return np.vstack(outs)\n",
    "\n",
    "    best = -1.0\n",
    "    history: List[Tuple[int, float]] = []\n",
    "    for ep in range(1, epochs + 1):\n",
    "        if time.time() - start > time_budget:\n",
    "            print(f\"[STOP] {name} exceeded 3h.\")\n",
    "            break\n",
    "        model.train(); total = 0.0\n",
    "        for Xt, yt in batcher(X, y, bs):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(Xt)\n",
    "            if loss == 'hinge':\n",
    "                yt_oh = torch.nn.functional.one_hot(yt, num_classes=2)\n",
    "                margins = 1 - logits * (2 * yt_oh - 1)\n",
    "                l = torch.clamp(margins, min=0.0).mean()\n",
    "            else:\n",
    "                l = nn.CrossEntropyLoss()(logits, yt)\n",
    "            l.backward(); opt.step(); total += float(l.item())\n",
    "        val_logits = predict(V)\n",
    "        val_pred = val_logits.argmax(1)\n",
    "        f1 = f1_macro(y_val, val_pred)\n",
    "        history.append((ep, f1))\n",
    "        print(f\"Epoch {ep} | val F1: {f1:.4f} | loss: {total:.2f}\")\n",
    "        best = max(best, f1)\n",
    "        if time.time() - start > early_check and best < 0.65:\n",
    "            print(f\"[DROP] {name} <0.65 F1 after 30 min. Aborting.\")\n",
    "            break\n",
    "    # Save weights\n",
    "    torch.save(model.state_dict(), f'./weights/{name}.pt')\n",
    "    # Learning curve\n",
    "    if history:\n",
    "        epc = [e for e, _ in history]\n",
    "        f1s = [v for _, v in history]\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        sns.lineplot(x=epc, y=f1s)\n",
    "        plt.xlabel('epoch'); plt.ylabel('val F1')\n",
    "        plt.title(f'Learning curve: {name}')\n",
    "        plt.tight_layout(); plt.show()\n",
    "    return model, best\n",
    "\n",
    "def random_projection(X: sp.csr_matrix, out_dim: int, seed: int\n",
    "                     ) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    R = rng.standard_normal((X.shape[1], out_dim)).astype(np.float32)\n",
    "    return X.dot(R).astype(np.float32)\n",
    "\n",
    "def knn_predict(X_tr: np.ndarray, y_tr: np.ndarray, X_te: np.ndarray,\n",
    "                k: int = 11) -> np.ndarray:\n",
    "    def norm_rows(A: np.ndarray) -> np.ndarray:\n",
    "        n = np.linalg.norm(A, axis=1, keepdims=True)\n",
    "        n[n == 0] = 1.0\n",
    "        return A / n\n",
    "    A = norm_rows(X_tr)\n",
    "    B = norm_rows(X_te)\n",
    "    preds: List[int] = []\n",
    "    bs = 1024\n",
    "    for i in range(0, B.shape[0], bs):\n",
    "        Sl = slice(i, min(i+bs, B.shape[0]))\n",
    "        sims = B[Sl] @ A.T\n",
    "        idx = np.argpartition(sims, -k, axis=1)[:, -k:]\n",
    "        votes = y_tr[idx]\n",
    "        maj = (votes.mean(1) >= 0.5).astype(int)\n",
    "        preds.extend(list(maj))\n",
    "    return np.array(preds)\n",
    "\n",
    "def train_xgboost(Xs: sp.csr_matrix, Xd: np.ndarray, y: np.ndarray,\n",
    "                  Xs_v: sp.csr_matrix, Xd_v: np.ndarray, y_v: np.ndarray,\n",
    "                  name: str = 'xgboost', rounds: int = 200\n",
    "                  ) -> Tuple[xgb.Booster, float]:\n",
    "    start = time.time(); time_budget = 3 * 60 * 60\n",
    "    X = sp.hstack([Xs, sp.csr_matrix(Xd)], format='csr')\n",
    "    V = sp.hstack([Xs_v, sp.csr_matrix(Xd_v)], format='csr')\n",
    "    dtr = xgb.DMatrix(X, label=y)\n",
    "    dva = xgb.DMatrix(V, label=y_v)\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'tree_method': 'hist',\n",
    "        'eval_metric': 'logloss', 'max_depth': 7, 'eta': 0.12,\n",
    "        'subsample': 0.9, 'colsample_bytree': 0.9\n",
    "    }\n",
    "    bst = xgb.train(params, dtr, num_boost_round=rounds,\n",
    "                    evals=[(dtr, 'train'), (dva, 'val')],\n",
    "                    verbose_eval=50)\n",
    "    if time.time() - start > 30 * 60:\n",
    "        preds = (bst.predict(dva) >= 0.5).astype(int)\n",
    "        f1 = f1_macro(y_v, preds)\n",
    "        if f1 < 0.65:\n",
    "            print('[DROP] XGBoost <0.65 F1 after 30 min. Aborting.')\n",
    "            return bst, f1\n",
    "    preds = (bst.predict(dva) >= 0.5).astype(int)\n",
    "    f1 = f1_macro(y_v, preds)\n",
    "    bst.save_model('./weights/xgboost.json')\n",
    "    print(f\"XGB val F1: {f1:.4f}\")\n",
    "    return bst, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7e93e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. PyTorch Text Models with Time Guards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd7344",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HashBagModel(nn.Module):\n",
    "    def __init__(self, dim: int, n_classes: int = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.emb = nn.EmbeddingBag(dim, n_classes, mode='sum')\n",
    "    def forward(self, idxs: torch.Tensor, offs: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        return self.emb(idxs, offs)\n",
    "\n",
    "def build_indices(text: str, dim: int, seed: int,\n",
    "                  w_ng: Tuple[int, int], c_ng: Tuple[int, int]\n",
    "                  ) -> List[int]:\n",
    "    toks = text.split()\n",
    "    feats = toks + word_ngrams(toks, w_ng[0], w_ng[1]) \\\n",
    "        + char_ngrams(text, c_ng[0], c_ng[1])\n",
    "    return list({hash_token(f, dim, seed) for f in feats})\n",
    "\n",
    "class HashedDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, dim: int, seed: int,\n",
    "                 w_ng: Tuple[int, int], c_ng: Tuple[int, int]) -> None:\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.dim = int(dim)\n",
    "        self.seed = int(seed)\n",
    "        self.w_ng = w_ng\n",
    "        self.c_ng = c_ng\n",
    "    def __len__(self) -> int:\n",
    "        return int(len(self.df))\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, Optional[int]]:\n",
    "        row = self.df.iloc[i]\n",
    "        idxs = build_indices(row['text'], self.dim, self.seed,\n",
    "                             self.w_ng, self.c_ng)\n",
    "        x = torch.tensor(idxs, dtype=torch.long)\n",
    "        y: Optional[int] = None\n",
    "        if 'label' in row and not pd.isna(row['label']):\n",
    "            y = int(row['label'])\n",
    "        return x, y\n",
    "\n",
    "def collate_hashed(\n",
    "    batch: List[Tuple[torch.Tensor, Optional[int]]]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
    "    feats: List[torch.Tensor] = []\n",
    "    offsets: List[int] = [0]\n",
    "    labels: List[int] = []\n",
    "    total = 0\n",
    "    for x, y in batch:\n",
    "        feats.append(x)\n",
    "        total += int(x.numel())\n",
    "        offsets.append(total)\n",
    "        if y is not None:\n",
    "            labels.append(int(y))\n",
    "    feats_cat = torch.cat(feats) if feats else torch.tensor([], dtype=torch.long)\n",
    "    offs = torch.tensor(offsets[:-1], dtype=torch.long)\n",
    "    y_t: Optional[torch.Tensor] = None\n",
    "    if len(labels) == len(batch):\n",
    "        y_t = torch.tensor(labels, dtype=torch.long)\n",
    "    return feats_cat, offs, y_t\n",
    "\n",
    "def train_hash_model(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    dim: int = 2**19,\n",
    "    epochs: int = 3,\n",
    "    bs: int = 128,\n",
    "    lr: float = 5e-3,\n",
    "    seed: int = 13,\n",
    "    name: str = 'embbag'\n",
    ") -> Tuple[HashBagModel, float]:\n",
    "    start = time.time()\n",
    "    early_check = 30 * 60\n",
    "    time_budget = 3 * 60 * 60\n",
    "    tr_ds = HashedDataset(train_df, dim, seed, (1, 2), (3, 5))\n",
    "    va_ds = HashedDataset(val_df, dim, seed, (1, 2), (3, 5))\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True,\n",
    "                       collate_fn=collate_hashed)\n",
    "    va_dl = DataLoader(va_ds, batch_size=bs, shuffle=False,\n",
    "                       collate_fn=collate_hashed)\n",
    "    model = HashBagModel(dim).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    best = -1.0\n",
    "    hist: List[Tuple[int, float]] = []\n",
    "    for ep in range(1, epochs + 1):\n",
    "        if time.time() - start > time_budget:\n",
    "            print('[STOP] EmbBag exceeded 3h.')\n",
    "            break\n",
    "        model.train(); total = 0.0\n",
    "        for x, offs, y in tr_dl:\n",
    "            x = x.to(DEVICE); offs = offs.to(DEVICE)\n",
    "            assert y is not None; y = y.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(x, offs), y)\n",
    "            loss.backward(); opt.step(); total += float(loss.item())\n",
    "        model.eval(); y_t, y_p = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, offs, y in va_dl:\n",
    "                p = model(x.to(DEVICE), offs.to(DEVICE)).argmax(1)\n",
    "                if y is not None:\n",
    "                    y_t.extend(list(y.numpy()))\n",
    "                    y_p.extend(list(p.cpu().numpy()))\n",
    "        f1 = f1_macro(np.array(y_t), np.array(y_p))\n",
    "        hist.append((ep, f1))\n",
    "        print(f\"Epoch {ep} | val F1: {f1:.4f} | loss: {total:.2f}\")\n",
    "        best = max(best, f1)\n",
    "        if time.time() - start > early_check and best < 0.65:\n",
    "            print('[DROP] EmbBag <0.65 F1 after 30 min. Aborting.')\n",
    "            break\n",
    "    torch.save(model.state_dict(), './weights/embbag.pt')\n",
    "    if hist:\n",
    "        epc = [e for e, _ in hist]; f1s = [v for _, v in hist]\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        sns.lineplot(x=epc, y=f1s)\n",
    "        plt.title('Learning curve: EmbBag'); plt.tight_layout(); plt.show()\n",
    "    return model, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77811cc2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, vocab: str, emb_dim: int = 32, n_classes: int = 2\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.map = {c: i + 1 for i, c in enumerate(vocab)}\n",
    "        self.emb = nn.Embedding(len(vocab) + 1, emb_dim)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(emb_dim, 128, 7, padding=3), nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.Conv1d(128, 256, 5, padding=2), nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(256, n_classes)\n",
    "    def encode(self, text: str, max_len: int = 1024) -> List[int]:\n",
    "        ids = [self.map.get(c, 0) for c in text[:max_len]]\n",
    "        if len(ids) < max_len:\n",
    "            ids += [0] * (max_len - len(ids))\n",
    "        return ids\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        emb = self.emb(x).transpose(1, 2)\n",
    "        h = self.conv(emb).squeeze(-1)\n",
    "        return self.fc(h)\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, enc: Callable[[str], List[int]]) -> None:\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.enc = enc\n",
    "    def __len__(self) -> int: return int(len(self.df))\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, Optional[int]]:\n",
    "        row = self.df.iloc[i]\n",
    "        x = torch.tensor(self.enc(row['text']), dtype=torch.long)\n",
    "        y: Optional[int] = None\n",
    "        if 'label' in row and not pd.isna(row['label']):\n",
    "            y = int(row['label'])\n",
    "        return x, y\n",
    "\n",
    "def train_char_cnn(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    epochs: int = 3,\n",
    "    bs: int = 128,\n",
    "    lr: float = 2e-3,\n",
    "    name: str = 'charcnn'\n",
    ") -> Tuple[CharCNN, float]:\n",
    "    start = time.time(); early_check = 30 * 60; time_budget = 3 * 60 * 60\n",
    "    vocab = string.ascii_letters + string.digits + string.punctuation + ' \\n\\t'\n",
    "    model = CharCNN(vocab).to(DEVICE)\n",
    "    enc = lambda s: model.encode(s, 1024)\n",
    "    tr_dl = DataLoader(CharDataset(train_df, enc), batch_size=bs,\n",
    "                       shuffle=True)\n",
    "    va_dl = DataLoader(CharDataset(val_df, enc), batch_size=bs,\n",
    "                       shuffle=False)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(); best = -1.0\n",
    "    hist: List[Tuple[int, float]] = []\n",
    "    for ep in range(1, epochs + 1):\n",
    "        if time.time() - start > time_budget:\n",
    "            print('[STOP] CharCNN exceeded 3h.')\n",
    "            break\n",
    "        model.train(); tot = 0.0\n",
    "        for x, y in tr_dl:\n",
    "            x = x.to(DEVICE); assert y is not None; y = y.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward(); opt.step(); tot += float(loss.item())\n",
    "        model.eval(); y_t, y_p = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in va_dl:\n",
    "                p = model(x.to(DEVICE)).argmax(1).cpu().numpy()\n",
    "                if y is not None:\n",
    "                    y_t.extend(list(y.numpy()))\n",
    "                    y_p.extend(list(p))\n",
    "        f1 = f1_macro(np.array(y_t), np.array(y_p))\n",
    "        hist.append((ep, f1))\n",
    "        print(f\"Epoch {ep} | val F1: {f1:.4f} | loss: {tot:.2f}\")\n",
    "        best = max(best, f1)\n",
    "        if time.time() - start > early_check and best < 0.65:\n",
    "            print('[DROP] CharCNN <0.65 F1 after 30 min. Aborting.')\n",
    "            break\n",
    "    torch.save(model.state_dict(), './weights/charcnn.pt')\n",
    "    if hist:\n",
    "        epc = [e for e, _ in hist]; f1s = [v for _, v in hist]\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        sns.lineplot(x=epc, y=f1s)\n",
    "        plt.title('Learning curve: CharCNN')\n",
    "        plt.tight_layout(); plt.show()\n",
    "    return model, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80a533",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(tokenizer: Any, texts: List[str], max_len: int\n",
    "                  ) -> Dict[str, torch.Tensor]:\n",
    "    enc = tokenizer(texts, padding=True, truncation=True,\n",
    "                    max_length=max_len, return_tensors='pt')\n",
    "    return {k: v for k, v in enc.items()}\n",
    "\n",
    "class HFClsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self) -> int: return int(len(self.df))\n",
    "    def __getitem__(self, i: int) -> Tuple[str, Optional[int]]:\n",
    "        row = self.df.iloc[i]\n",
    "        y: Optional[int] = None\n",
    "        if 'label' in row and not pd.isna(row['label']):\n",
    "            y = int(row['label'])\n",
    "        return str(row['text']), y\n",
    "\n",
    "def collate_hf(\n",
    "    batch: List[Tuple[str, Optional[int]]],\n",
    "    tokenizer: Any,\n",
    "    max_len: int\n",
    ") -> Tuple[Dict[str, torch.Tensor], Optional[torch.Tensor]]:\n",
    "    texts = [b[0] for b in batch]\n",
    "    enc = tokenize_batch(tokenizer, texts, max_len)\n",
    "    labels: Optional[torch.Tensor] = None\n",
    "    if all(b[1] is not None for b in batch):\n",
    "        labels = torch.tensor([int(b[1]) for b in batch],\n",
    "                             dtype=torch.long)\n",
    "    return enc, labels\n",
    "\n",
    "def train_transformer(\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    model_name: str = 'roberta-base',\n",
    "    epochs: int = 2,\n",
    "    bs: int = 16,\n",
    "    lr: float = 2e-5,\n",
    "    max_len: int = 512,\n",
    "    name: str = 'roberta'\n",
    ") -> Tuple[nn.Module, Any, float]:\n",
    "    start = time.time(); early_check = 30 * 60; time_budget = 3 * 60 * 60\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    ).to(DEVICE)\n",
    "    tr_ds = HFClsDataset(train_df)\n",
    "    va_ds = HFClsDataset(val_df)\n",
    "    coll = lambda b: collate_hf(b, tok, max_len)\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True,\n",
    "                       collate_fn=coll)\n",
    "    va_dl = DataLoader(va_ds, batch_size=bs, shuffle=False,\n",
    "                       collate_fn=coll)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    steps = max(1, len(tr_dl) * epochs)\n",
    "    sch = get_linear_schedule_with_warmup(opt, int(0.1 * steps), steps)\n",
    "    loss_fn = nn.CrossEntropyLoss(); best = -1.0\n",
    "    hist: List[Tuple[int, float]] = []\n",
    "    for ep in range(1, epochs + 1):\n",
    "        if time.time() - start > time_budget:\n",
    "            print('[STOP] Transformer exceeded 3h.')\n",
    "            break\n",
    "        model.train(); tot = 0.0\n",
    "        for enc, y in tr_dl:\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            assert y is not None; y = y.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            out = model(**enc); loss = loss_fn(out.logits, y)\n",
    "            loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step(); sch.step(); tot += float(loss.item())\n",
    "        model.eval(); y_t, y_p = [], []\n",
    "        with torch.no_grad():\n",
    "            for enc, y in va_dl:\n",
    "                enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "                p = model(**enc).logits.argmax(1).cpu().numpy()\n",
    "                if y is not None:\n",
    "                    y_t.extend(list(y.numpy()))\n",
    "                    y_p.extend(list(p))\n",
    "        f1 = f1_macro(np.array(y_t), np.array(y_p))\n",
    "        hist.append((ep, f1))\n",
    "        print(f\"Epoch {ep} | val F1: {f1:.4f} | loss: {tot:.2f}\")\n",
    "        best = max(best, f1)\n",
    "        if time.time() - start > early_check and best < 0.65:\n",
    "            print('[DROP] Transformer <0.65 F1 after 30 min. Aborting.')\n",
    "            break\n",
    "    model.save_pretrained('./weights/roberta')\n",
    "    tok.save_pretrained('./weights/roberta')\n",
    "    if hist:\n",
    "        epc = [e for e, _ in hist]; f1s = [v for _, v in hist]\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        sns.lineplot(x=epc, y=f1s)\n",
    "        plt.title('Learning curve: RoBERTa')\n",
    "        plt.tight_layout(); plt.show()\n",
    "    return model, tok, best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f38379",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 10. Build Features, CV, and Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c2625",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_s, X_train_d = build_all_features(train_df)\n",
    "X_val_s, X_val_d = build_all_features(val_df)\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "def cv_linear(loss: str, tag: str) -> float:\n",
    "    splits = kfold_indices(X_train_s.shape[0], 5, RANDOM_SEED)\n",
    "    scores: List[float] = []\n",
    "    for i, (tr, va) in enumerate(splits):\n",
    "        m, f1 = train_linear_model(\n",
    "            X_train_s[tr], X_train_d.iloc[tr].to_numpy(), y_train[tr],\n",
    "            X_train_s[va], X_train_d.iloc[va].to_numpy(), y_train[va],\n",
    "            loss=loss, epochs=4, bs=256, lr=1e-2, name=f'{tag}_cv'\n",
    "        )\n",
    "        scores.append(f1)\n",
    "        print(f\"Fold {i+1}/5 F1: {f1:.4f}\")\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def cv_knn() -> float:\n",
    "    splits = kfold_indices(X_train_s.shape[0], 5, RANDOM_SEED)\n",
    "    scores: List[float] = []\n",
    "    for i, (tr, va) in enumerate(splits):\n",
    "        A = random_projection(sp.hstack([X_train_s[tr],\n",
    "                                         sp.csr_matrix(X_train_d.iloc[tr].to_numpy())],\n",
    "                                        format='csr'), 512, 13)\n",
    "        B = random_projection(sp.hstack([X_train_s[va],\n",
    "                                         sp.csr_matrix(X_train_d.iloc[va].to_numpy())],\n",
    "                                        format='csr'), 512, 13)\n",
    "        pred = knn_predict(A, y_train[tr], B, k=11)\n",
    "        f1 = f1_macro(y_train[va], pred)\n",
    "        scores.append(f1)\n",
    "        print(f\"Fold {i+1}/5 F1: {f1:.4f}\")\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "results: List[Dict[str, Any]] = []\n",
    "cv_lr = cv_linear('log', 'logreg')\n",
    "results.append({'model': 'LogReg', 'cv_f1': cv_lr})\n",
    "cv_svm = cv_linear('hinge', 'linsvm')\n",
    "results.append({'model': 'LinSVM', 'cv_f1': cv_svm})\n",
    "cv_kn = cv_knn()\n",
    "results.append({'model': 'KNN(Proj)', 'cv_f1': cv_kn})\n",
    "\n",
    "# Fit on full training and evaluate on held-out val\n",
    "m_lr, f1_lr = train_linear_model(\n",
    "    X_train_s, X_train_d.to_numpy(), y_train,\n",
    "    X_val_s, X_val_d.to_numpy(), y_val,\n",
    "    loss='log', epochs=6, bs=256, lr=1e-2, name='logreg'\n",
    ")\n",
    "m_svm, f1_svm = train_linear_model(\n",
    "    X_train_s, X_train_d.to_numpy(), y_train,\n",
    "    X_val_s, X_val_d.to_numpy(), y_val,\n",
    "    loss='hinge', epochs=6, bs=256, lr=1e-2, name='linsvm'\n",
    ")\n",
    "A_tr = random_projection(sp.hstack([X_train_s,\n",
    "                                    sp.csr_matrix(X_train_d.to_numpy())],\n",
    "                                   format='csr'), 512, 13)\n",
    "A_va = random_projection(sp.hstack([X_val_s,\n",
    "                                    sp.csr_matrix(X_val_d.to_numpy())],\n",
    "                                   format='csr'), 512, 13)\n",
    "pred_kn = knn_predict(A_tr, y_train, A_va, k=11)\n",
    "f1_kn = f1_macro(y_val, pred_kn)\n",
    "\n",
    "bst, f1_xg = train_xgboost(\n",
    "    X_train_s, X_train_d.to_numpy(), y_train,\n",
    "    X_val_s, X_val_d.to_numpy(), y_val,\n",
    "    name='xgboost', rounds=200\n",
    ")\n",
    "\n",
    "mh, f1_hash = train_hash_model(train_df, val_df)\n",
    "mc, f1_char = train_char_cnn(train_df, val_df)\n",
    "mt, tok, f1_tr = train_transformer(train_df, val_df)\n",
    "\n",
    "res = pd.DataFrame([\n",
    "    {'model': 'LogReg', 'cv_f1': cv_lr, 'val_f1': f1_lr},\n",
    "    {'model': 'LinSVM', 'cv_f1': cv_svm, 'val_f1': f1_svm},\n",
    "    {'model': 'KNN(Proj)', 'cv_f1': cv_kn, 'val_f1': f1_kn},\n",
    "    {'model': 'XGBoost', 'cv_f1': np.nan, 'val_f1': f1_xg},\n",
    "    {'model': 'EmbBag', 'cv_f1': np.nan, 'val_f1': f1_hash},\n",
    "    {'model': 'CharCNN', 'cv_f1': np.nan, 'val_f1': f1_char},\n",
    "    {'model': 'RoBERTa', 'cv_f1': np.nan, 'val_f1': f1_tr},\n",
    "])\n",
    "display(res)\n",
    "plt.figure(figsize=(9, 4))\n",
    "sns.barplot(data=res.melt(id_vars='model', value_vars=['cv_f1','val_f1'],\n",
    "                         var_name='split', value_name='f1'),\n",
    "            x='model', y='f1', hue='split')\n",
    "plt.xticks(rotation=20); plt.title('Model comparison (macro-F1)')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db85a22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 11. Error Analysis with More Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92ed14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_row = res.sort_values('val_f1', ascending=False).iloc[0]\n",
    "print('Best model:', best_row['model'])\n",
    "\n",
    "def confusion(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    cm = np.zeros((2, 2), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[int(t), int(p)] += 1\n",
    "    return cm\n",
    "\n",
    "def preds_from_best() -> np.ndarray:\n",
    "    if best_row['model'] == 'LogReg':\n",
    "        V = sp.hstack([X_val_s, sp.csr_matrix(X_val_d.to_numpy())],\n",
    "                      format='csr')\n",
    "        logits = m_lr(torch.tensor(V.toarray(), dtype=torch.float32,\n",
    "                                   device=DEVICE)).cpu().numpy()\n",
    "        return logits.argmax(1)\n",
    "    if best_row['model'] == 'LinSVM':\n",
    "        V = sp.hstack([X_val_s, sp.csr_matrix(X_val_d.to_numpy())],\n",
    "                      format='csr')\n",
    "        logits = m_svm(torch.tensor(V.toarray(), dtype=torch.float32,\n",
    "                                    device=DEVICE)).cpu().numpy()\n",
    "        return logits.argmax(1)\n",
    "    if best_row['model'] == 'KNN(Proj)':\n",
    "        return pred_kn\n",
    "    if best_row['model'] == 'XGBoost':\n",
    "        dva = xgb.DMatrix(sp.hstack([X_val_s, sp.csr_matrix(X_val_d.to_numpy())],\n",
    "                                    format='csr'))\n",
    "        return (bst.predict(dva) >= 0.5).astype(int)\n",
    "    if best_row['model'] == 'EmbBag':\n",
    "        ds = HashedDataset(val_df, 2**19, 13, (1, 2), (3, 5))\n",
    "        dl = DataLoader(ds, batch_size=128, shuffle=False,\n",
    "                        collate_fn=collate_hashed)\n",
    "        preds: List[int] = []\n",
    "        with torch.no_grad():\n",
    "            for x, offs, _ in dl:\n",
    "                p = mh(x.to(DEVICE), offs.to(DEVICE)).argmax(1).cpu().numpy()\n",
    "                preds.extend(list(p))\n",
    "        return np.array(preds)\n",
    "    if best_row['model'] == 'CharCNN':\n",
    "        enc = lambda s: mc.encode(s, 1024)\n",
    "        dl = DataLoader(CharDataset(val_df, enc), batch_size=128,\n",
    "                        shuffle=False)\n",
    "        preds: List[int] = []\n",
    "        with torch.no_grad():\n",
    "            for x, _ in dl:\n",
    "                p = mc(x.to(DEVICE)).argmax(1).cpu().numpy()\n",
    "                preds.extend(list(p))\n",
    "        return np.array(preds)\n",
    "    ds = HFClsDataset(val_df)\n",
    "    coll = lambda b: collate_hf(b, tok, 512)\n",
    "    dl = DataLoader(ds, batch_size=32, shuffle=False, collate_fn=coll)\n",
    "    preds: List[int] = []\n",
    "    with torch.no_grad():\n",
    "        for enc, _ in dl:\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            p = mt(**enc).logits.argmax(1).cpu().numpy()\n",
    "            preds.extend(list(p))\n",
    "    return np.array(preds)\n",
    "\n",
    "y_hat = preds_from_best()\n",
    "cm = confusion(y_val, y_hat)\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion (val)')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "val_dense = X_val_d.copy()\n",
    "val_dense = val_dense.assign(pred=y_hat, label=y_val)\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.kdeplot(data=val_dense, x='ttr', hue='label')\n",
    "plt.title('TTR by label'); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.violinplot(data=val_dense, x='pred', y='read_flesch')\n",
    "plt.title('Flesch vs prediction'); plt.tight_layout(); plt.show()\n",
    "\n",
    "err = val_df.assign(pred=y_hat)\n",
    "err = err[err['label'] != err['pred']].copy()\n",
    "display(err.head(12)[['id','label','pred','text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572307fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 12. Final Train on Train+Val and Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb0184",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df, val_df], axis=0).reset_index(drop=True)\n",
    "X_full_s, X_full_d = build_all_features(full_df)\n",
    "X_test_s, X_test_d = build_all_features(test_df)\n",
    "y_full = full_df['label'].values\n",
    "\n",
    "def fit_and_predict(best: str) -> pd.DataFrame:\n",
    "    if best == 'LogReg':\n",
    "        m, _ = train_linear_model(\n",
    "            X_full_s, X_full_d.to_numpy(), y_full,\n",
    "            X_val_s[:1], X_val_d[:1].to_numpy(), y_val[:1],\n",
    "            loss='log', epochs=6, bs=256, lr=1e-2, name='logreg_full'\n",
    "        )\n",
    "        V = sp.hstack([X_test_s, sp.csr_matrix(X_test_d.to_numpy())],\n",
    "                      format='csr')\n",
    "        p = m(torch.tensor(V.toarray(), dtype=torch.float32,\n",
    "                           device=DEVICE)).argmax(1).cpu().numpy()\n",
    "        return pd.DataFrame({'id': test_df['id'], 'label': p})\n",
    "    if best == 'LinSVM':\n",
    "        m, _ = train_linear_model(\n",
    "            X_full_s, X_full_d.to_numpy(), y_full,\n",
    "            X_val_s[:1], X_val_d[:1].to_numpy(), y_val[:1],\n",
    "            loss='hinge', epochs=6, bs=256, lr=1e-2,\n",
    "            name='linsvm_full'\n",
    "        )\n",
    "        V = sp.hstack([X_test_s, sp.csr_matrix(X_test_d.to_numpy())],\n",
    "                      format='csr')\n",
    "        p = m(torch.tensor(V.toarray(), dtype=torch.float32,\n",
    "                           device=DEVICE)).argmax(1).cpu().numpy()\n",
    "        return pd.DataFrame({'id': test_df['id'], 'label': p})\n",
    "    if best == 'KNN(Proj)':\n",
    "        A = random_projection(sp.hstack([X_full_s,\n",
    "                                         sp.csr_matrix(X_full_d.to_numpy())],\n",
    "                                        format='csr'), 512, 13)\n",
    "        B = random_projection(sp.hstack([X_test_s,\n",
    "                                         sp.csr_matrix(X_test_d.to_numpy())],\n",
    "                                        format='csr'), 512, 13)\n",
    "        p = knn_predict(A, y_full, B, k=11)\n",
    "        return pd.DataFrame({'id': test_df['id'], 'label': p})\n",
    "    if best == 'XGBoost':\n",
    "        bst, _ = train_xgboost(\n",
    "            X_full_s, X_full_d.to_numpy(), y_full,\n",
    "            X_val_s[:1], X_val_d[:1].to_numpy(), y_val[:1],\n",
    "            name='xgboost_full', rounds=250\n",
    "        )\n",
    "        dt = xgb.DMatrix(sp.hstack([X_test_s,\n",
    "                                    sp.csr_matrix(X_test_d.to_numpy())],\n",
    "                                   format='csr'))\n",
    "        p = (bst.predict(dt) >= 0.5).astype(int)\n",
    "        return pd.DataFrame({'id': test_df['id'], 'label': p})\n",
    "    if best == 'EmbBag':\n",
    "        mh_full, _ = train_hash_model(full_df, val_df.iloc[:0])\n",
    "        ds = HashedDataset(test_df, 2**19, 13, (1, 2), (3, 5))\n",
    "        dl = DataLoader(ds, batch_size=128, shuffle=False,\n",
    "                        collate_fn=collate_hashed)\n",
    "        preds: List[int] = []\n",
    "        with torch.no_grad():\n",
    "            for x, offs, _ in dl:\n",
    "                p = mh_full(x.to(DEVICE), offs.to(DEVICE)).argmax(1)\n",
    "                preds.extend(list(p.cpu().numpy()))\n",
    "        torch.save(mh_full.state_dict(), './weights/embbag_full.pt')\n",
    "        return pd.DataFrame({'id': test_df['id'], 'label': preds})\n",
    "    if best == 'CharCNN':\n",
    "        mc_full, _ = train_char_cnn(full_df, val_df.iloc[:0])\n",
    "        enc = lambda s: mc_full.encode(s, 1024)\n",
    "        dl = DataLoader(CharDataset(test_df, enc), batch_size=128,\n",
    "                        shuffle=False)\n",
    "        preds: List[int] = []\n",
    "        with torch.no_grad():\n",
    "            for x, _ in dl:\n",
    "                p = mc_full(x.to(DEVICE)).argmax(1).cpu().numpy()\n",
    "                preds.extend(list(p))\n",
    "        torch.save(mc_full.state_dict(), './weights/charcnn_full.pt')\n",
    "        return pd.DataFrame({'id': test_df['id'], 'label': preds})\n",
    "    # RoBERTa\n",
    "    mt_full, tok, _ = train_transformer(full_df, val_df.iloc[:0])\n",
    "    mt_full.save_pretrained('./weights/roberta_full')\n",
    "    tok.save_pretrained('./weights/roberta_full')\n",
    "    ds = HFClsDataset(test_df)\n",
    "    coll = lambda b: collate_hf(b, tok, 512)\n",
    "    dl = DataLoader(ds, batch_size=32, shuffle=False, collate_fn=coll)\n",
    "    preds: List[int] = []\n",
    "    with torch.no_grad():\n",
    "        for enc, _ in dl:\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            p = mt_full(**enc).logits.argmax(1).cpu().numpy()\n",
    "            preds.extend(list(p))\n",
    "    return pd.DataFrame({'id': test_df['id'], 'label': preds})\n",
    "\n",
    "best_name = str(res.sort_values('val_f1', ascending=False).iloc[0]['model'])\n",
    "sub_df = fit_and_predict(best_name)\n",
    "SUB_PATH = './submission.csv'\n",
    "sub_df.to_csv(SUB_PATH, index=False)\n",
    "print('Saved submission:', SUB_PATH)\n",
    "display(sub_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d97268",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0012a98",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "res.to_csv('./model_results.csv', index=False)\n",
    "print('Saved results to ./model_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.373509,
   "end_time": "2025-10-21T13:56:42.697870",
   "environment_variables": {},
   "exception": true,
   "input_path": "hvsm_colab.ipynb",
   "output_path": "hvsm_colab_output.ipynb",
   "parameters": {},
   "start_time": "2025-10-21T13:56:21.324361",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}